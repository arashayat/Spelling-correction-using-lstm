{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spelling correction using LSTMs\n",
    "\n",
    "This is my undergraduate final project.\n",
    "We'll try and train a RNN for spelling correction of search queries."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing relevant data and libraries.\n",
    "\n",
    "In this section we will import the necessary libraries and the datasets and preprocess them for the model to train on.\n",
    "\n",
    "### 1.1. Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import preprocess_data, string_to_int"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Importing and preprocessing the datasets\n",
    "\n",
    "#### 1.2.1. faspell\n",
    "FASpell dataset was developed for the evaluation of spell checking algorithms. It contains a set of pairs of misspelled Persian words and their corresponding corrected forms similar to the ASpell dataset used for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#misspelt</th>\n",
       "      <th>corrected</th>\n",
       "      <th>error-category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>آاهي</td>\n",
       "      <td>آگاهي</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>آبات</td>\n",
       "      <td>آیات</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>آبباشد</td>\n",
       "      <td>آب باشد</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>آبد</td>\n",
       "      <td>آید</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>آبری</td>\n",
       "      <td>عابری</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  #misspelt corrected  error-category\n",
       "0      آاهي     آگاهي               1\n",
       "1      آبات      آیات               1\n",
       "2    آبباشد   آب باشد               2\n",
       "3       آبد       آید               1\n",
       "4      آبری     عابری               0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import faspell_main\n",
    "data_faspell = pd.read_csv('data/faspell_main.txt', sep='\\t')\n",
    "data_faspell.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_faspell.drop('error-category', axis = 1, inplace=True)\n",
    "data_faspell.rename({'#misspelt':'misspelt'}, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4858 entries, 0 to 4857\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   misspelt   4858 non-null   object\n",
      " 1   corrected  4858 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 76.0+ KB\n"
     ]
    }
   ],
   "source": [
    "data_faspell.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspelt</th>\n",
       "      <th>corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>آاهي</td>\n",
       "      <td>آگاهي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>آبات</td>\n",
       "      <td>آیات</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>آبباشد</td>\n",
       "      <td>آب باشد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>آبد</td>\n",
       "      <td>آید</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>آبری</td>\n",
       "      <td>عابری</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  misspelt corrected\n",
       "0     آاهي     آگاهي\n",
       "1     آبات      آیات\n",
       "2   آبباشد   آب باشد\n",
       "3      آبد       آید\n",
       "4     آبری     عابری"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_faspell.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 context sensitive\n",
    "\n",
    "This is a real-world test set for errors and context sensitive spelling errors for Persian language. This test set contains 1100 context sensitive errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correct</th>\n",
       "      <th>misspelt</th>\n",
       "      <th>error_cat</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>فرنگي</td>\n",
       "      <td>فرهنگي</td>\n",
       "      <td>Insertion</td>\n",
       "      <td>كاهش قيمت گوجه فرنگي و خيار در ميادين ميوه و ت...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>فرنگي</td>\n",
       "      <td>فرهنگي</td>\n",
       "      <td>Insertion</td>\n",
       "      <td>خوردن گوجه فرهنگي كه حرام است !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>فرنگي</td>\n",
       "      <td>فرهنگي</td>\n",
       "      <td>Insertion</td>\n",
       "      <td>گوجه فرهنگي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>فرنگي</td>\n",
       "      <td>فرهنگي</td>\n",
       "      <td>Insertion</td>\n",
       "      <td>سيب زميني 58000 كيلو ، گوجه فرهنگي 18000 كيلو</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>فرنگي</td>\n",
       "      <td>فرهنگي</td>\n",
       "      <td>Insertion</td>\n",
       "      <td>اجازه فروش كارخانه كشمش ، رب گوجه فرهنگي تاكست...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  correct misspelt  error_cat  \\\n",
       "0   فرنگي   فرهنگي  Insertion   \n",
       "1   فرنگي   فرهنگي  Insertion   \n",
       "2   فرنگي   فرهنگي  Insertion   \n",
       "3   فرنگي   فرهنگي  Insertion   \n",
       "4   فرنگي   فرهنگي  Insertion   \n",
       "\n",
       "                                            sentence  \n",
       "0  كاهش قيمت گوجه فرنگي و خيار در ميادين ميوه و ت...  \n",
       "1                   خوردن گوجه فرهنگي كه حرام است !   \n",
       "2                                        گوجه فرهنگي  \n",
       "3      سيب زميني 58000 كيلو ، گوجه فرهنگي 18000 كيلو  \n",
       "4  اجازه فروش كارخانه كشمش ، رب گوجه فرهنگي تاكست...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_context = pd.read_csv('data/context_sensitive.txt', header = 0, \n",
    "                            names = ['correct', 'misspelt', 'error_cat', 'sentence', 'nan'], \n",
    "                            sep = '\\t').drop('nan', axis=1)\n",
    "data_context.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Combining the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Preprocess the data\n",
    "\n",
    "In this step we will ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocabulary dictionary\n",
    "# map each persian dictionary to a unique number\n",
    "\n",
    "vocab = {' ':0 , 'آ':1 , 'ا':2 , 'ب':3 , 'پ':4 , 'ت':5 , 'ث':6 , 'ج':7 , 'چ':8 , 'ح':9 , 'خ':10 ,\n",
    " 'د':11 , 'ذ':12 , 'ر':13 , 'ز':14 , 'ژ':15 , 'س':16 , 'ش':17 , 'ص':18 , 'ض':19 , 'ط':20 , 'ظ':21 , \n",
    " 'ع':22 , 'غ':23 , 'ف':24 , 'ق':25 , 'ک':26 , 'گ':27 , 'ل':28 , 'م':29 , 'ن':30 , 'و':31 , 'ه':32 , \n",
    " 'ی':33, '<unk>':34, '<pad>':35}\n",
    "\n",
    "# create inverse vocabulary dictionary\n",
    "# map each number to its corresponding index in dictionary\n",
    "\n",
    "inv_vocab = {0:' ' , 1:'آ' , 2:'ا' , 3:'ب' , 4:'پ' , 5:'ت' , 6:'ث' , 7:'ج' , 8:'چ' , 9:'ح' , 10:'خ' ,\n",
    " 11:'د' , 12:'ذ' , 13:'ر' , 14:'ز', 15:'ژ' , 16:'س' , 17:'ش' , 18:'ص' , 19:'ض' , 20:'ط' , 21:'ظ' , \n",
    " 22:'ع' , 23:'غ' , 24:'ف' , 25:'ق' , 26:'ک' , 27:'گ' , 28:'ل' , 29:'م' , 30:'ن' , 31:'و' , 32:'ه' , \n",
    " 33:'ی', 34:'<unk>', 35:'<pad>'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will set T=50\n",
    "    * We assume T is the maximum length of the query.\n",
    "    * If we get a longer input, we would have to truncate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (4858, 50)\n",
      "Y.shape: (4858, 50)\n",
      "Xoh.shape: (4858, 50, 36)\n",
      "Yoh.shape: (4858, 50, 36)\n"
     ]
    }
   ],
   "source": [
    "T = 50\n",
    "X, Y, Xoh, Yoh = preprocess_data(list(data_faspell.itertuples(index=False, name=None)), vocab, T)\n",
    "\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)\n",
    "print(\"Xoh.shape:\", Xoh.shape)\n",
    "print(\"Yoh.shape:\", Yoh.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have:\n",
    "\n",
    "* `X`: a processed version of the human queries in the training set.\n",
    "    - Each character in X is replaced by an index (integer) mapped to the character using `vocab`.\n",
    "    - Each date is padded to ensure a length of `T` using a special character (< pad >).\n",
    "    - `X.shape = (m, T)` where m is the number of training examples in a batch.\n",
    "    \n",
    "* `Y`: a processed version of the machine readable dates in the training set.\n",
    "    - Each character is replaced by the index (integer) it is mapped to in `vocab`.\n",
    "    - `Y.shape = (m, T)`.\n",
    "* `Xoh`: one-hot version of `X`\n",
    "    - Each index in X is converted to the one-hot representation (if the index is 2, the one-hot version has the index position 2 set to 1, and the remaining positions are 0.\n",
    "    - `Xoh.shape = (m, T, len(vocab))`.\n",
    "* `Yoh`: one-hot version of `Y`\n",
    "    - Each index in `Y` is converted to the one-hot representation.\n",
    "    - `Yoh.shape = (m, T, len(vocab))`.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's also look at an example of preprocessed training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Source after preprocessing (indices): [ 1  2 32 33 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35\n",
      " 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35\n",
      " 35 35]\n",
      "Target after preprocessing (indices): [ 1 27  2 32 33 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35\n",
      " 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35 35\n",
      " 35 35]\n",
      "\n",
      "Source after preprocessing (one-hot): [[0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "Target after preprocessing (one-hot): [[0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "# print(\"Source query:\", dataset[index][0])\n",
    "# print(\"Target query:\", dataset[index][1])\n",
    "print()\n",
    "print(\"Source after preprocessing (indices):\", X[index])\n",
    "print(\"Target after preprocessing (indices):\", Y[index])\n",
    "print()\n",
    "print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
    "print(\"Target after preprocessing (one-hot):\", Yoh[index])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Spelling correction using LSTMs with Attention\n",
    "\n",
    "* If you had to translate a book's paragraph from French to English, you would not read the whole paragraph, then close the book and translate. \n",
    "* Even during the translation process, you would read/re-read and focus on the parts of the French paragraph corresponding to the parts of the English you are writing down. \n",
    "* The attention mechanism tells a Neural Machine Translation model where it should pay attention to at any step. \n",
    "\n",
    "### 3.1 - Attention Mechanism\n",
    "\n",
    "In this part, we will implement the attention mechanism presented in the lecture videos. \n",
    "* Here is a figure to remind you how the model works. \n",
    "    * The diagram on the left shows the attention model. \n",
    "    * The diagram on the right shows what one \"attention\" step does to calculate the attention variables $\\alpha^{\\langle t, t' \\rangle}$.\n",
    "    * The attention variables $\\alpha^{\\langle t, t' \\rangle}$ are used to compute the context variable $context^{\\langle t \\rangle}$ for each timestep in the output ($t=1, \\ldots, T_y$). \n",
    "\n",
    "<table>\n",
    "<td> \n",
    "<img src=\"images/attn_model.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "<td> \n",
    "<img src=\"images/attn_mechanism.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "</table>\n",
    "<caption><center> **Figure 1**: Neural machine translation with attention</center></caption>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some properties of the model that you may notice: \n",
    "\n",
    "#### Pre-attention and Post-attention LSTMs on both sides of the attention mechanism\n",
    "- There are two separate LSTMs in this model (see diagram on the left): pre-attention and post-attention LSTMs.\n",
    "- *Pre-attention* Bi-LSTM is the one at the bottom of the picture is a Bi-directional LSTM and comes *before* the attention mechanism.\n",
    "    - The attention mechanism is shown in the middle of the left-hand diagram.\n",
    "    - The pre-attention Bi-LSTM goes through $T_x$ time steps\n",
    "- *Post-attention* LSTM: at the top of the diagram comes *after* the attention mechanism. \n",
    "    - The post-attention LSTM goes through $T_y$ time steps. \n",
    "\n",
    "- The post-attention LSTM passes the hidden state $s^{\\langle t \\rangle}$ and cell state $c^{\\langle t \\rangle}$ from one time step to the next. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An LSTM has both a hidden state and cell state\n",
    "* In the lecture videos, we were using only a basic RNN for the post-attention sequence model\n",
    "    * This means that the state captured by the RNN was outputting only the hidden state $s^{\\langle t\\rangle}$. \n",
    "* In this assignment, we are using an LSTM instead of a basic RNN.\n",
    "    * So the LSTM has both the hidden state $s^{\\langle t\\rangle}$ and the cell state $c^{\\langle t\\rangle}$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each time step does not use predictions from the previous time step\n",
    "* The post-attention LSTM at time $t$ does not take the previous time step's prediction $y^{\\langle t-1 \\rangle}$ as input.\n",
    "* The post-attention LSTM at time 't' only takes the hidden state $s^{\\langle t\\rangle}$ and cell state $c^{\\langle t\\rangle}$ as input. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenation of hidden states from the forward and backward pre-attention LSTMs\n",
    "- $\\overrightarrow{a}^{\\langle t \\rangle}$: hidden state of the forward-direction, pre-attention LSTM.\n",
    "- $\\overleftarrow{a}^{\\langle t \\rangle}$: hidden state of the backward-direction, pre-attention LSTM.\n",
    "- $a^{\\langle t \\rangle} = [\\overrightarrow{a}^{\\langle t \\rangle}, \\overleftarrow{a}^{\\langle t \\rangle}]$: the concatenation of the activations of both the forward-direction $\\overrightarrow{a}^{\\langle t \\rangle}$ and backward-directions $\\overleftarrow{a}^{\\langle t \\rangle}$ of the pre-attention Bi-LSTM. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing \"energies\" $e^{\\langle t, t' \\rangle}$ as a function of $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t' \\rangle}$\n",
    "- \"e\" is called the \"energies\" variable.\n",
    "- $s^{\\langle t-1 \\rangle}$ is the hidden state of the post-attention LSTM\n",
    "- $a^{\\langle t' \\rangle}$ is the hidden state of the pre-attention LSTM.\n",
    "- $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t \\rangle}$ are fed into a simple neural network, which learns the function to output $e^{\\langle t, t' \\rangle}$.\n",
    "- $e^{\\langle t, t' \\rangle}$ is then used when computing the attention $a^{\\langle t, t' \\rangle}$ that $y^{\\langle t \\rangle}$ should pay to $a^{\\langle t' \\rangle}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The diagram on the right of figure 1 uses a `RepeatVector` node to copy $s^{\\langle t-1 \\rangle}$'s value $T_x$ times.\n",
    "- Then it uses `Concatenation` to concatenate $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t \\rangle}$.\n",
    "- The concatenation of $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t \\rangle}$ is fed into a \"Dense\" layer, which computes $e^{\\langle t, t' \\rangle}$. \n",
    "- $e^{\\langle t, t' \\rangle}$ is then passed through a softmax to compute $\\alpha^{\\langle t, t' \\rangle}$.\n",
    "- Note that the diagram doesn't explicitly show variable $e^{\\langle t, t' \\rangle}$, but $e^{\\langle t, t' \\rangle}$ is above the Dense layer and below the Softmax layer in the diagram in the right half of figure 1.\n",
    "- We'll explain how to use `RepeatVector` and `Concatenation` in Keras below. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation Details\n",
    "   \n",
    "Let's implement this neural translator. We will start by implementing two functions: `one_step_attention()` and `model()`.\n",
    "\n",
    "#### one_step_attention\n",
    "* The inputs to the one_step_attention at time step $t$ are:\n",
    "    - $[a^{<1>},a^{<2>}, ..., a^{<T_x>}]$: all hidden states of the pre-attention Bi-LSTM.\n",
    "    - $s^{<t-1>}$: the previous hidden state of the post-attention LSTM \n",
    "* one_step_attention computes:\n",
    "    - $[\\alpha^{<t,1>},\\alpha^{<t,2>}, ..., \\alpha^{<t,T_x>}]$: the attention weights\n",
    "    - $context^{ \\langle t \\rangle }$: the context vector:\n",
    "    \n",
    "$$context^{<t>} = \\sum_{t' = 1}^{T_x} \\alpha^{<t,t'>}a^{<t'>}\\tag{1}$$ \n",
    "\n",
    "##### Clarifying 'context' and 'c'\n",
    "- In the project, we are calling the context $context^{\\langle t \\rangle}$.\n",
    "    - This is to avoid confusion with the post-attention LSTM's internal memory cell variable, which is also denoted $c^{\\langle t \\rangle}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b1c37460846ba095889c981f6bdf7dbe3db3422cb2efaf84f879792979df9daa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
