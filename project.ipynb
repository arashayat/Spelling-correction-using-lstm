{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spelling correction using LSTMs\n",
    "\n",
    "This is my undergraduate final project.\n",
    "We'll try and train a RNN for spelling correction of search queries."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing relevant data and libraries.\n",
    "\n",
    "In this section we will import the necessary libraries and the datasets and preprocess them for the model to train on.\n",
    "\n",
    "### 1.1. Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sc_utils import preprocess_data, string_to_int, softmax\n",
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Importing and preprocessing the datasets\n",
    "\n",
    "#### 1.2.1. faspell\n",
    "FASpell dataset was developed for the evaluation of spell checking algorithms. It contains a set of pairs of misspelled Persian words and their corresponding corrected forms similar to the ASpell dataset used for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#misspelt</th>\n",
       "      <th>corrected</th>\n",
       "      <th>error-category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>آاهي</td>\n",
       "      <td>آگاهي</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>آبات</td>\n",
       "      <td>آیات</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>آبباشد</td>\n",
       "      <td>آب باشد</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>آبد</td>\n",
       "      <td>آید</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>آبری</td>\n",
       "      <td>عابری</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  #misspelt corrected  error-category\n",
       "0      آاهي     آگاهي               1\n",
       "1      آبات      آیات               1\n",
       "2    آبباشد   آب باشد               2\n",
       "3       آبد       آید               1\n",
       "4      آبری     عابری               0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import faspell_main\n",
    "data_faspell = pd.read_csv('data/faspell_main.txt', sep='\\t')\n",
    "data_faspell.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_faspell.drop('error-category', axis = 1, inplace=True)\n",
    "data_faspell.rename({'#misspelt':'misspelt'}, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4858 entries, 0 to 4857\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   misspelt   4858 non-null   object\n",
      " 1   corrected  4858 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 76.0+ KB\n"
     ]
    }
   ],
   "source": [
    "data_faspell.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>misspelt</th>\n",
       "      <th>corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>آاهي</td>\n",
       "      <td>آگاهي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>آبات</td>\n",
       "      <td>آیات</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>آبباشد</td>\n",
       "      <td>آب باشد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>آبد</td>\n",
       "      <td>آید</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>آبری</td>\n",
       "      <td>عابری</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  misspelt corrected\n",
       "0     آاهي     آگاهي\n",
       "1     آبات      آیات\n",
       "2   آبباشد   آب باشد\n",
       "3      آبد       آید\n",
       "4     آبری     عابری"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_faspell.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 context sensitive\n",
    "\n",
    "This is a real-world test set for errors and context sensitive spelling errors for Persian language. This test set contains 1100 context sensitive errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corrected</th>\n",
       "      <th>misspelt</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>فرنگي</td>\n",
       "      <td>فرهنگي</td>\n",
       "      <td>كاهش قيمت گوجه فرنگي و خيار در ميادين ميوه و ت...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>فرنگي</td>\n",
       "      <td>فرهنگي</td>\n",
       "      <td>خوردن گوجه فرهنگي كه حرام است !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>فرنگي</td>\n",
       "      <td>فرهنگي</td>\n",
       "      <td>گوجه فرهنگي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>فرنگي</td>\n",
       "      <td>فرهنگي</td>\n",
       "      <td>سيب زميني 58000 كيلو ، گوجه فرهنگي 18000 كيلو</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>فرنگي</td>\n",
       "      <td>فرهنگي</td>\n",
       "      <td>اجازه فروش كارخانه كشمش ، رب گوجه فرهنگي تاكست...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  corrected misspelt                                           sentence\n",
       "0     فرنگي   فرهنگي  كاهش قيمت گوجه فرنگي و خيار در ميادين ميوه و ت...\n",
       "1     فرنگي   فرهنگي                   خوردن گوجه فرهنگي كه حرام است ! \n",
       "2     فرنگي   فرهنگي                                        گوجه فرهنگي\n",
       "3     فرنگي   فرهنگي      سيب زميني 58000 كيلو ، گوجه فرهنگي 18000 كيلو\n",
       "4     فرنگي   فرهنگي  اجازه فروش كارخانه كشمش ، رب گوجه فرهنگي تاكست..."
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_context = pd.read_csv('data/context_sensitive.txt', header = 0, \n",
    "                            names = ['corrected', 'misspelt', 'error_cat', 'sentence', 'nan'], \n",
    "                            sep = '\\t').drop('nan', axis=1)\n",
    "data_context.drop('error_cat', inplace=True, axis=1)\n",
    "data_context.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will now make the dataset ready for our use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corrected</th>\n",
       "      <th>misspelt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>كاهش قيمت گوجه فرنگي و خيار در ميادين ميوه و ت...</td>\n",
       "      <td>كاهش قيمت گوجه فرنگي و خيار در ميادين ميوه و ت...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>خوردن گوجه فرنگي كه حرام است !</td>\n",
       "      <td>خوردن گوجه فرهنگي كه حرام است !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>گوجه فرنگي</td>\n",
       "      <td>گوجه فرهنگي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>سيب زميني 58000 كيلو ، گوجه فرنگي 18000 كيلو</td>\n",
       "      <td>سيب زميني 58000 كيلو ، گوجه فرهنگي 18000 كيلو</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>اجازه فروش كارخانه كشمش ، رب گوجه فرنگي تاكستا...</td>\n",
       "      <td>اجازه فروش كارخانه كشمش ، رب گوجه فرهنگي تاكست...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           corrected  \\\n",
       "0  كاهش قيمت گوجه فرنگي و خيار در ميادين ميوه و ت...   \n",
       "1                    خوردن گوجه فرنگي كه حرام است !    \n",
       "2                                         گوجه فرنگي   \n",
       "3       سيب زميني 58000 كيلو ، گوجه فرنگي 18000 كيلو   \n",
       "4  اجازه فروش كارخانه كشمش ، رب گوجه فرنگي تاكستا...   \n",
       "\n",
       "                                            misspelt  \n",
       "0  كاهش قيمت گوجه فرنگي و خيار در ميادين ميوه و ت...  \n",
       "1                   خوردن گوجه فرهنگي كه حرام است !   \n",
       "2                                        گوجه فرهنگي  \n",
       "3      سيب زميني 58000 كيلو ، گوجه فرهنگي 18000 كيلو  \n",
       "4  اجازه فروش كارخانه كشمش ، رب گوجه فرهنگي تاكست...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_context['corrected'] = data_context.apply(lambda x: x['sentence'].replace(x.misspelt, x.corrected), axis = 1)\n",
    "data_context['misspelt'] = data_context['sentence']\n",
    "data_context.drop('sentence', inplace=True, axis=1)\n",
    "data_context.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 synthetic\n",
    "A comprehensive parallel dataset designed for the task of spell checking in Persian. Misspelled sentences together with the correct form are produced using a massive confusion matrix, which is gathered from many sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic = pd.DataFrame(columns = ['correct','misspelt'])\n",
    "synthetic['corrected'] = pd.read_csv('data/correct_synthetic.txt')\n",
    "synthetic['misspelt'] = pd.read_csv('data/wrong_synthetic.txt')\n",
    "\n",
    "synthetic.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Combining the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Preprocess the data\n",
    "\n",
    "In this step we will ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocabulary dictionary\n",
    "# map each persian dictionary to a unique number\n",
    "\n",
    "vocab = {' ':0 , 'آ':1 , 'ا':2 , 'ب':3 , 'پ':4 , 'ت':5 , 'ث':6 , 'ج':7 , 'چ':8 , 'ح':9 , 'خ':10 ,\n",
    " 'د':11 , 'ذ':12 , 'ر':13 , 'ز':14 , 'ژ':15 , 'س':16 , 'ش':17 , 'ص':18 , 'ض':19 , 'ط':20 , 'ظ':21 , \n",
    " 'ع':22 , 'غ':23 , 'ف':24 , 'ق':25 , 'ک':26 , 'گ':27 , 'ل':28 , 'م':29 , 'ن':30 , 'و':31 , 'ه':32 , \n",
    " 'ی':33, '<unk>':34, '<pad>':35}\n",
    "\n",
    "# create inverse vocabulary dictionary\n",
    "# map each number to its corresponding index in dictionary\n",
    "\n",
    "inv_vocab = {0:' ' , 1:'آ' , 2:'ا' , 3:'ب' , 4:'پ' , 5:'ت' , 6:'ث' , 7:'ج' , 8:'چ' , 9:'ح' , 10:'خ' ,\n",
    " 11:'د' , 12:'ذ' , 13:'ر' , 14:'ز', 15:'ژ' , 16:'س' , 17:'ش' , 18:'ص' , 19:'ض' , 20:'ط' , 21:'ظ' , \n",
    " 22:'ع' , 23:'غ' , 24:'ف' , 25:'ق' , 26:'ک' , 27:'گ' , 28:'ل' , 29:'م' , 30:'ن' , 31:'و' , 32:'ه' , \n",
    " 33:'ی', 34:'<unk>', 35:'<pad>'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will set T=50\n",
    "    * We assume T is the maximum length of the query.\n",
    "    * If we get a longer input, we would have to truncate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32ma:\\Uni\\Project\\project\\project.ipynb Cell 21\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/a%3A/Uni/Project/project/project.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m T \u001b[39m=\u001b[39m \u001b[39m50\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/a%3A/Uni/Project/project/project.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m X, Y, Xoh, Yoh \u001b[39m=\u001b[39m preprocess_data(\u001b[39mlist\u001b[39;49m(synthetic\u001b[39m.\u001b[39;49mitertuples(index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, name\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)), vocab, T)\n\u001b[0;32m      <a href='vscode-notebook-cell:/a%3A/Uni/Project/project/project.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m m \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/a%3A/Uni/Project/project/project.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mX.shape:\u001b[39m\u001b[39m\"\u001b[39m, X\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32ma:\\Uni\\Project\\project\\sc_utils.py:23\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[1;34m(dataset, vocab, T)\u001b[0m\n\u001b[0;32m     20\u001b[0m X, Y \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mdataset)\n\u001b[0;32m     22\u001b[0m X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([string_to_int(i, T, vocab) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m X])\n\u001b[1;32m---> 23\u001b[0m Y \u001b[39m=\u001b[39m [string_to_int(t, T, vocab) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m Y]\n\u001b[0;32m     25\u001b[0m Xoh \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m x: to_categorical(x, num_classes\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(vocab)), X)))\n\u001b[0;32m     26\u001b[0m Yoh \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m x: to_categorical(x, num_classes\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(vocab)), Y)))\n",
      "File \u001b[1;32ma:\\Uni\\Project\\project\\sc_utils.py:23\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     20\u001b[0m X, Y \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mdataset)\n\u001b[0;32m     22\u001b[0m X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([string_to_int(i, T, vocab) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m X])\n\u001b[1;32m---> 23\u001b[0m Y \u001b[39m=\u001b[39m [string_to_int(t, T, vocab) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m Y]\n\u001b[0;32m     25\u001b[0m Xoh \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m x: to_categorical(x, num_classes\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(vocab)), X)))\n\u001b[0;32m     26\u001b[0m Yoh \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m x: to_categorical(x, num_classes\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(vocab)), Y)))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "T = 50\n",
    "X, Y, Xoh, Yoh = preprocess_data(list(synthetic.itertuples(index=False, name=None)), vocab, T)\n",
    "m = X.shape[0]\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)\n",
    "print(\"Xoh.shape:\", Xoh.shape)\n",
    "print(\"Yoh.shape:\", Yoh.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have:\n",
    "\n",
    "* `X`: a processed version of the human queries in the training set.\n",
    "    - Each character in X is replaced by an index (integer) mapped to the character using `vocab`.\n",
    "    - Each date is padded to ensure a length of `T` using a special character (< pad >).\n",
    "    - `X.shape = (m, T)` where m is the number of training examples in a batch.\n",
    "    \n",
    "* `Y`: a processed version of the machine readable dates in the training set.\n",
    "    - Each character is replaced by the index (integer) it is mapped to in `vocab`.\n",
    "    - `Y.shape = (m, T)`.\n",
    "* `Xoh`: one-hot version of `X`\n",
    "    - Each index in X is converted to the one-hot representation (if the index is 2, the one-hot version has the index position 2 set to 1, and the remaining positions are 0.\n",
    "    - `Xoh.shape = (m, T, len(vocab))`.\n",
    "* `Yoh`: one-hot version of `Y`\n",
    "    - Each index in `Y` is converted to the one-hot representation.\n",
    "    - `Yoh.shape = (m, T, len(vocab))`.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's also look at an example of preprocessed training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32ma:\\Uni\\Project\\project\\project.ipynb Cell 24\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/a%3A/Uni/Project/project/project.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# print(\"Source query:\", dataset[index][0])\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/a%3A/Uni/Project/project/project.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# print(\"Target query:\", dataset[index][1])\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/a%3A/Uni/Project/project/project.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/a%3A/Uni/Project/project/project.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mSource after preprocessing (indices):\u001b[39m\u001b[39m\"\u001b[39m, X[index])\n\u001b[0;32m      <a href='vscode-notebook-cell:/a%3A/Uni/Project/project/project.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTarget after preprocessing (indices):\u001b[39m\u001b[39m\"\u001b[39m, Y[index])\n\u001b[0;32m      <a href='vscode-notebook-cell:/a%3A/Uni/Project/project/project.ipynb#X25sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "# print(\"Source query:\", dataset[index][0])\n",
    "# print(\"Target query:\", dataset[index][1])\n",
    "print()\n",
    "print(\"Source after preprocessing (indices):\", X[index])\n",
    "print(\"Target after preprocessing (indices):\", Y[index])\n",
    "print()\n",
    "print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
    "print(\"Target after preprocessing (one-hot):\", Yoh[index])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Spelling correction using LSTMs with Attention\n",
    "\n",
    "* If you had to translate a book's paragraph from French to English, you would not read the whole paragraph, then close the book and translate. \n",
    "* Even during the translation process, you would read/re-read and focus on the parts of the French paragraph corresponding to the parts of the English you are writing down. \n",
    "* The attention mechanism tells a Neural Machine Translation model where it should pay attention to at any step. \n",
    "\n",
    "### 3.1 - Attention Mechanism\n",
    "\n",
    "In this part, we will implement the attention mechanism presented in the lecture videos. \n",
    "* Here is a figure to remind you how the model works. \n",
    "    * The diagram on the left shows the attention model. \n",
    "    * The diagram on the right shows what one \"attention\" step does to calculate the attention variables $\\alpha^{\\langle t, t' \\rangle}$.\n",
    "    * The attention variables $\\alpha^{\\langle t, t' \\rangle}$ are used to compute the context variable $context^{\\langle t \\rangle}$ for each timestep in the output ($t=1, \\ldots, T_y$). \n",
    "\n",
    "<table>\n",
    "<td> \n",
    "<img src=\"images/attn_model.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "<td> \n",
    "<img src=\"images/attn_mechanism.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "</table>\n",
    "<caption><center> **Figure 1**: Neural machine translation with attention</center></caption>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some properties of the model that you may notice: \n",
    "\n",
    "#### Pre-attention and Post-attention LSTMs on both sides of the attention mechanism\n",
    "- There are two separate LSTMs in this model (see diagram on the left): pre-attention and post-attention LSTMs.\n",
    "- *Pre-attention* Bi-LSTM is the one at the bottom of the picture is a Bi-directional LSTM and comes *before* the attention mechanism.\n",
    "    - The attention mechanism is shown in the middle of the left-hand diagram.\n",
    "    - The pre-attention Bi-LSTM goes through $T_x$ time steps\n",
    "- *Post-attention* LSTM: at the top of the diagram comes *after* the attention mechanism. \n",
    "    - The post-attention LSTM goes through $T_y$ time steps. \n",
    "\n",
    "- The post-attention LSTM passes the hidden state $s^{\\langle t \\rangle}$ and cell state $c^{\\langle t \\rangle}$ from one time step to the next. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An LSTM has both a hidden state and cell state\n",
    "* In the lecture videos, we were using only a basic RNN for the post-attention sequence model\n",
    "    * This means that the state captured by the RNN was outputting only the hidden state $s^{\\langle t\\rangle}$. \n",
    "* In this assignment, we are using an LSTM instead of a basic RNN.\n",
    "    * So the LSTM has both the hidden state $s^{\\langle t\\rangle}$ and the cell state $c^{\\langle t\\rangle}$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each time step does not use predictions from the previous time step\n",
    "* The post-attention LSTM at time $t$ does not take the previous time step's prediction $y^{\\langle t-1 \\rangle}$ as input.\n",
    "* The post-attention LSTM at time 't' only takes the hidden state $s^{\\langle t\\rangle}$ and cell state $c^{\\langle t\\rangle}$ as input. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenation of hidden states from the forward and backward pre-attention LSTMs\n",
    "- $\\overrightarrow{a}^{\\langle t \\rangle}$: hidden state of the forward-direction, pre-attention LSTM.\n",
    "- $\\overleftarrow{a}^{\\langle t \\rangle}$: hidden state of the backward-direction, pre-attention LSTM.\n",
    "- $a^{\\langle t \\rangle} = [\\overrightarrow{a}^{\\langle t \\rangle}, \\overleftarrow{a}^{\\langle t \\rangle}]$: the concatenation of the activations of both the forward-direction $\\overrightarrow{a}^{\\langle t \\rangle}$ and backward-directions $\\overleftarrow{a}^{\\langle t \\rangle}$ of the pre-attention Bi-LSTM. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing \"energies\" $e^{\\langle t, t' \\rangle}$ as a function of $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t' \\rangle}$\n",
    "- \"e\" is called the \"energies\" variable.\n",
    "- $s^{\\langle t-1 \\rangle}$ is the hidden state of the post-attention LSTM\n",
    "- $a^{\\langle t' \\rangle}$ is the hidden state of the pre-attention LSTM.\n",
    "- $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t \\rangle}$ are fed into a simple neural network, which learns the function to output $e^{\\langle t, t' \\rangle}$.\n",
    "- $e^{\\langle t, t' \\rangle}$ is then used when computing the attention $a^{\\langle t, t' \\rangle}$ that $y^{\\langle t \\rangle}$ should pay to $a^{\\langle t' \\rangle}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The diagram on the right of figure 1 uses a `RepeatVector` node to copy $s^{\\langle t-1 \\rangle}$'s value $T_x$ times.\n",
    "- Then it uses `Concatenation` to concatenate $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t \\rangle}$.\n",
    "- The concatenation of $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t \\rangle}$ is fed into a \"Dense\" layer, which computes $e^{\\langle t, t' \\rangle}$. \n",
    "- $e^{\\langle t, t' \\rangle}$ is then passed through a softmax to compute $\\alpha^{\\langle t, t' \\rangle}$.\n",
    "- Note that the diagram doesn't explicitly show variable $e^{\\langle t, t' \\rangle}$, but $e^{\\langle t, t' \\rangle}$ is above the Dense layer and below the Softmax layer in the diagram in the right half of figure 1.\n",
    "- We'll explain how to use `RepeatVector` and `Concatenation` in Keras below. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation Details\n",
    "   \n",
    "Let's implement this neural translator. We will start by implementing two functions: `one_step_attention()` and `model()`.\n",
    "\n",
    "#### one_step_attention\n",
    "* The inputs to the one_step_attention at time step $t$ are:\n",
    "    - $[a^{<1>},a^{<2>}, ..., a^{<T_x>}]$: all hidden states of the pre-attention Bi-LSTM.\n",
    "    - $s^{<t-1>}$: the previous hidden state of the post-attention LSTM \n",
    "* one_step_attention computes:\n",
    "    - $[\\alpha^{<t,1>},\\alpha^{<t,2>}, ..., \\alpha^{<t,T_x>}]$: the attention weights\n",
    "    - $context^{ \\langle t \\rangle }$: the context vector:\n",
    "    \n",
    "$$context^{<t>} = \\sum_{t' = 1}^{T_x} \\alpha^{<t,t'>}a^{<t'>}\\tag{1}$$ \n",
    "\n",
    "##### Clarifying 'context' and 'c'\n",
    "- In the project, we are calling the context $context^{\\langle t \\rangle}$.\n",
    "    - This is to avoid confusion with the post-attention LSTM's internal memory cell variable, which is also denoted $c^{\\langle t \\rangle}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement `one_step_attention()`. \n",
    "\n",
    "* The function `model()` will call the layers in `one_step_attention()` $T_y$ times using a for-loop.\n",
    "* It is important that all $T_y$ copies have the same weights. \n",
    "    * It should not reinitialize the weights every time. \n",
    "    * In other words, all $T_y$ steps should have shared weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined shared layers as global variables\n",
    "repeator = RepeatVector(T)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor1 = Dense(10, activation = \"tanh\")\n",
    "densor2 = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attention) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that \n",
    "    # you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
    "    s_prev = repeator(s_prev)\n",
    "    # Use concatenator to concatenate a and s_prev on the last axis \n",
    "    # For grading purposes, please list 'a' first and 's_prev' second, in this order.\n",
    "    concat = concatenator([a,s_prev])\n",
    "    # Use densor1 to propagate concat through a small fully-connected neural \n",
    "    # network to compute the \"intermediate energies\" variable e.\n",
    "    e = densor1(concat)\n",
    "    # Use densor2 to propagate e through a small fully-connected \n",
    "    # neural network to compute the \"energies\" variable energies. \n",
    "    energies = densor2(e)\n",
    "    # Use \"activator\" on \"energies\" to compute the attention weights \"alphas\"\n",
    "    alphas = activator(energies)\n",
    "    # Use dotor together with \"alphas\" and \"a\", in this order, \n",
    "    # to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
    "    context = dotor([alphas,a])\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modelf\n",
    "\n",
    "Implement `modelf()` as explained in figure 1:\n",
    "\n",
    "* `modelf` first runs the input through a Bi-LSTM to get $[a^{<1>},a^{<2>}, ..., a^{<T_x>}]$. \n",
    "* Then, `modelf` calls `one_step_attention()` $T_y$ times using a `for` loop.  At each iteration of this loop:\n",
    "    - It gives the computed context vector $context^{<t>}$ to the post-attention LSTM.\n",
    "    - It runs the output of the post-attention LSTM through a dense layer with softmax activation.\n",
    "    - The softmax generates a prediction $\\hat{y}^{<t>}$.\n",
    "    \n",
    "Again, we have defined global layers that will share weights to be used in `modelf()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = 64 # number of units for the pre-attention, bi-directional LSTM's hidden state 'a'\n",
    "n_s = 128 # number of units for the post-attention LSTM's hidden state \"s\"\n",
    "\n",
    "# Please note, this is the post attention LSTM cell.  \n",
    "post_activation_LSTM_cell = LSTM(n_s, return_state = True) # Please do not modify this global variable.\n",
    "output_layer = Dense(len(vocab), activation=softmax)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use these layers $T_y$ times in a `for` loop to generate the outputs, and their parameters will not be reinitialized. You will have to carry out the following steps: \n",
    "\n",
    "1. Propagate the input `X` into a bi-directional LSTM.\n",
    "    * [Bidirectional](https://keras.io/layers/wrappers/#bidirectional) \n",
    "    * [LSTM](https://keras.io/layers/recurrent/#lstm)\n",
    "    * Remember that we want the LSTM to return a full sequence instead of just the last hidden state.  \n",
    "    \n",
    "Sample code:\n",
    "\n",
    "```Python\n",
    "sequence_of_hidden_states = Bidirectional(LSTM(units=..., return_sequences=...))(the_input_X)\n",
    "```\n",
    "    \n",
    "2. Iterate for $t = 0, \\cdots, T_y-1$: \n",
    "    1. Call `one_step_attention()`, passing in the sequence of hidden states $[a^{\\langle 1 \\rangle},a^{\\langle 2 \\rangle}, ..., a^{ \\langle T_x \\rangle}]$ from the pre-attention bi-directional LSTM, and the previous hidden state $s^{<t-1>}$ from the post-attention LSTM to calculate the context vector $context^{<t>}$.\n",
    "    2. Give $context^{<t>}$ to the post-attention LSTM cell. \n",
    "        - Remember to pass in the previous hidden-state $s^{\\langle t-1\\rangle}$ and cell-states $c^{\\langle t-1\\rangle}$ of this LSTM \n",
    "        * This outputs the new hidden state $s^{<t>}$ and the new cell state $c^{<t>}$.  \n",
    "\n",
    "        Sample code:\n",
    "        ```Python\n",
    "        next_hidden_state, _ , next_cell_state = \n",
    "            post_activation_LSTM_cell(inputs=..., initial_state=[prev_hidden_state, prev_cell_state])\n",
    "        ```   \n",
    "        Please note that the layer is actually the \"post attention LSTM cell\".  For the purposes of passing the automatic grader, please do not modify the naming of this global variable.  This will be fixed when we deploy updates to the automatic grader.\n",
    "    3. Apply a dense, softmax layer to $s^{<t>}$, get the output.  \n",
    "        Sample code:\n",
    "        ```Python\n",
    "        output = output_layer(inputs=...)\n",
    "        ```\n",
    "    4. Save the output by adding it to the list of outputs.\n",
    "\n",
    "3. Create your Keras model instance.\n",
    "    * It should have three inputs:\n",
    "        * `X`, the one-hot encoded inputs to the model, of shape ($T_{x}, humanVocabSize)$\n",
    "        * $s^{\\langle 0 \\rangle}$, the initial hidden state of the post-attention LSTM\n",
    "        * $c^{\\langle 0 \\rangle}$, the initial cell state of the post-attention LSTM\n",
    "    * The output is the list of outputs.  \n",
    "    Sample code\n",
    "    ```Python\n",
    "    model = Model(inputs=[...,...,...], outputs=...)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def modelf(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the inputs of your model with a shape (Tx,)\n",
    "    # Define s0 (initial hidden state) and c0 (initial cell state)\n",
    "    # for the decoder LSTM with shape (n_s,)\n",
    "    X = Input(shape=(Tx, human_vocab_size))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    # Initialize empty list of outputs\n",
    "    outputs = []\n",
    "    \n",
    "    #Define our pre-attention Bi-LSTM.\n",
    "    a = Bidirectional(LSTM(n_a, return_sequences=True))(X)\n",
    "    \n",
    "    # Iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "\n",
    "        # mechanism to get back the context vector at step t \n",
    "        context = one_step_attention(a, s)\n",
    "        \n",
    "        # Don't forget to pass: initial_state = [hidden state, cell state]\n",
    "        s, _, c = post_activation_LSTM_cell(context,initial_state=[s, c])\n",
    "\n",
    "        # output of the post-attention LSTM\n",
    "        out = output_layer(s)\n",
    "        \n",
    "        outputs.append(out)\n",
    "\n",
    "    # inputs and returning the list of outputs.\n",
    "    model = Model(inputs=[X, s0, c0],outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = modelf(T, T, n_a, n_s, len(vocab), len(vocab))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a summary of the model to check if it matches the expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 20, 36)]     0           []                               \n",
      "                                                                                                  \n",
      " s0 (InputLayer)                [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 20, 64)       17664       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " repeat_vector (RepeatVector)   (None, 20, 64)       0           ['s0[0][0]',                     \n",
      "                                                                  'lstm[0][0]',                   \n",
      "                                                                  'lstm[1][0]',                   \n",
      "                                                                  'lstm[2][0]',                   \n",
      "                                                                  'lstm[3][0]',                   \n",
      "                                                                  'lstm[4][0]',                   \n",
      "                                                                  'lstm[5][0]',                   \n",
      "                                                                  'lstm[6][0]',                   \n",
      "                                                                  'lstm[7][0]',                   \n",
      "                                                                  'lstm[8][0]',                   \n",
      "                                                                  'lstm[9][0]',                   \n",
      "                                                                  'lstm[10][0]',                  \n",
      "                                                                  'lstm[11][0]',                  \n",
      "                                                                  'lstm[12][0]',                  \n",
      "                                                                  'lstm[13][0]',                  \n",
      "                                                                  'lstm[14][0]',                  \n",
      "                                                                  'lstm[15][0]',                  \n",
      "                                                                  'lstm[16][0]',                  \n",
      "                                                                  'lstm[17][0]',                  \n",
      "                                                                  'lstm[18][0]']                  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 20, 128)      0           ['bidirectional[0][0]',          \n",
      "                                                                  'repeat_vector[0][0]',          \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'repeat_vector[1][0]',          \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'repeat_vector[2][0]',          \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'repeat_vector[3][0]',          \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'repeat_vector[4][0]',          \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'repeat_vector[5][0]',          \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'repeat_vector[6][0]',          \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'repeat_vector[7][0]',          \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'repeat_vector[8][0]',          \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'repeat_vector[9][0]',          \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'repeat_vector[10][0]',         \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'repeat_vector[11][0]',         \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'repeat_vector[12][0]',         \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'repeat_vector[13][0]',         \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'repeat_vector[14][0]',         \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'repeat_vector[15][0]',         \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'repeat_vector[16][0]',         \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'repeat_vector[17][0]',         \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'repeat_vector[18][0]',         \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'repeat_vector[19][0]']         \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 20, 10)       1290        ['concatenate[0][0]',            \n",
      "                                                                  'concatenate[1][0]',            \n",
      "                                                                  'concatenate[2][0]',            \n",
      "                                                                  'concatenate[3][0]',            \n",
      "                                                                  'concatenate[4][0]',            \n",
      "                                                                  'concatenate[5][0]',            \n",
      "                                                                  'concatenate[6][0]',            \n",
      "                                                                  'concatenate[7][0]',            \n",
      "                                                                  'concatenate[8][0]',            \n",
      "                                                                  'concatenate[9][0]',            \n",
      "                                                                  'concatenate[10][0]',           \n",
      "                                                                  'concatenate[11][0]',           \n",
      "                                                                  'concatenate[12][0]',           \n",
      "                                                                  'concatenate[13][0]',           \n",
      "                                                                  'concatenate[14][0]',           \n",
      "                                                                  'concatenate[15][0]',           \n",
      "                                                                  'concatenate[16][0]',           \n",
      "                                                                  'concatenate[17][0]',           \n",
      "                                                                  'concatenate[18][0]',           \n",
      "                                                                  'concatenate[19][0]']           \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 20, 1)        11          ['dense[0][0]',                  \n",
      "                                                                  'dense[1][0]',                  \n",
      "                                                                  'dense[2][0]',                  \n",
      "                                                                  'dense[3][0]',                  \n",
      "                                                                  'dense[4][0]',                  \n",
      "                                                                  'dense[5][0]',                  \n",
      "                                                                  'dense[6][0]',                  \n",
      "                                                                  'dense[7][0]',                  \n",
      "                                                                  'dense[8][0]',                  \n",
      "                                                                  'dense[9][0]',                  \n",
      "                                                                  'dense[10][0]',                 \n",
      "                                                                  'dense[11][0]',                 \n",
      "                                                                  'dense[12][0]',                 \n",
      "                                                                  'dense[13][0]',                 \n",
      "                                                                  'dense[14][0]',                 \n",
      "                                                                  'dense[15][0]',                 \n",
      "                                                                  'dense[16][0]',                 \n",
      "                                                                  'dense[17][0]',                 \n",
      "                                                                  'dense[18][0]',                 \n",
      "                                                                  'dense[19][0]']                 \n",
      "                                                                                                  \n",
      " attention_weights (Activation)  (None, 20, 1)       0           ['dense_1[0][0]',                \n",
      "                                                                  'dense_1[1][0]',                \n",
      "                                                                  'dense_1[2][0]',                \n",
      "                                                                  'dense_1[3][0]',                \n",
      "                                                                  'dense_1[4][0]',                \n",
      "                                                                  'dense_1[5][0]',                \n",
      "                                                                  'dense_1[6][0]',                \n",
      "                                                                  'dense_1[7][0]',                \n",
      "                                                                  'dense_1[8][0]',                \n",
      "                                                                  'dense_1[9][0]',                \n",
      "                                                                  'dense_1[10][0]',               \n",
      "                                                                  'dense_1[11][0]',               \n",
      "                                                                  'dense_1[12][0]',               \n",
      "                                                                  'dense_1[13][0]',               \n",
      "                                                                  'dense_1[14][0]',               \n",
      "                                                                  'dense_1[15][0]',               \n",
      "                                                                  'dense_1[16][0]',               \n",
      "                                                                  'dense_1[17][0]',               \n",
      "                                                                  'dense_1[18][0]',               \n",
      "                                                                  'dense_1[19][0]']               \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 1, 64)        0           ['attention_weights[0][0]',      \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'attention_weights[1][0]',      \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'attention_weights[2][0]',      \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'attention_weights[3][0]',      \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'attention_weights[4][0]',      \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'attention_weights[5][0]',      \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'attention_weights[6][0]',      \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'attention_weights[7][0]',      \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'attention_weights[8][0]',      \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'attention_weights[9][0]',      \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'attention_weights[10][0]',     \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'attention_weights[11][0]',     \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'attention_weights[12][0]',     \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'attention_weights[13][0]',     \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'attention_weights[14][0]',     \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'attention_weights[15][0]',     \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'attention_weights[16][0]',     \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'attention_weights[17][0]',     \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'attention_weights[18][0]',     \n",
      "                                                                  'bidirectional[0][0]',          \n",
      "                                                                  'attention_weights[19][0]',     \n",
      "                                                                  'bidirectional[0][0]']          \n",
      "                                                                                                  \n",
      " c0 (InputLayer)                [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 64),         33024       ['dot[0][0]',                    \n",
      "                                 (None, 64),                      's0[0][0]',                     \n",
      "                                 (None, 64)]                      'c0[0][0]',                     \n",
      "                                                                  'dot[1][0]',                    \n",
      "                                                                  'lstm[0][0]',                   \n",
      "                                                                  'lstm[0][2]',                   \n",
      "                                                                  'dot[2][0]',                    \n",
      "                                                                  'lstm[1][0]',                   \n",
      "                                                                  'lstm[1][2]',                   \n",
      "                                                                  'dot[3][0]',                    \n",
      "                                                                  'lstm[2][0]',                   \n",
      "                                                                  'lstm[2][2]',                   \n",
      "                                                                  'dot[4][0]',                    \n",
      "                                                                  'lstm[3][0]',                   \n",
      "                                                                  'lstm[3][2]',                   \n",
      "                                                                  'dot[5][0]',                    \n",
      "                                                                  'lstm[4][0]',                   \n",
      "                                                                  'lstm[4][2]',                   \n",
      "                                                                  'dot[6][0]',                    \n",
      "                                                                  'lstm[5][0]',                   \n",
      "                                                                  'lstm[5][2]',                   \n",
      "                                                                  'dot[7][0]',                    \n",
      "                                                                  'lstm[6][0]',                   \n",
      "                                                                  'lstm[6][2]',                   \n",
      "                                                                  'dot[8][0]',                    \n",
      "                                                                  'lstm[7][0]',                   \n",
      "                                                                  'lstm[7][2]',                   \n",
      "                                                                  'dot[9][0]',                    \n",
      "                                                                  'lstm[8][0]',                   \n",
      "                                                                  'lstm[8][2]',                   \n",
      "                                                                  'dot[10][0]',                   \n",
      "                                                                  'lstm[9][0]',                   \n",
      "                                                                  'lstm[9][2]',                   \n",
      "                                                                  'dot[11][0]',                   \n",
      "                                                                  'lstm[10][0]',                  \n",
      "                                                                  'lstm[10][2]',                  \n",
      "                                                                  'dot[12][0]',                   \n",
      "                                                                  'lstm[11][0]',                  \n",
      "                                                                  'lstm[11][2]',                  \n",
      "                                                                  'dot[13][0]',                   \n",
      "                                                                  'lstm[12][0]',                  \n",
      "                                                                  'lstm[12][2]',                  \n",
      "                                                                  'dot[14][0]',                   \n",
      "                                                                  'lstm[13][0]',                  \n",
      "                                                                  'lstm[13][2]',                  \n",
      "                                                                  'dot[15][0]',                   \n",
      "                                                                  'lstm[14][0]',                  \n",
      "                                                                  'lstm[14][2]',                  \n",
      "                                                                  'dot[16][0]',                   \n",
      "                                                                  'lstm[15][0]',                  \n",
      "                                                                  'lstm[15][2]',                  \n",
      "                                                                  'dot[17][0]',                   \n",
      "                                                                  'lstm[16][0]',                  \n",
      "                                                                  'lstm[16][2]',                  \n",
      "                                                                  'dot[18][0]',                   \n",
      "                                                                  'lstm[17][0]',                  \n",
      "                                                                  'lstm[17][2]',                  \n",
      "                                                                  'dot[19][0]',                   \n",
      "                                                                  'lstm[18][0]',                  \n",
      "                                                                  'lstm[18][2]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 36)           2340        ['lstm[0][0]',                   \n",
      "                                                                  'lstm[1][0]',                   \n",
      "                                                                  'lstm[2][0]',                   \n",
      "                                                                  'lstm[3][0]',                   \n",
      "                                                                  'lstm[4][0]',                   \n",
      "                                                                  'lstm[5][0]',                   \n",
      "                                                                  'lstm[6][0]',                   \n",
      "                                                                  'lstm[7][0]',                   \n",
      "                                                                  'lstm[8][0]',                   \n",
      "                                                                  'lstm[9][0]',                   \n",
      "                                                                  'lstm[10][0]',                  \n",
      "                                                                  'lstm[11][0]',                  \n",
      "                                                                  'lstm[12][0]',                  \n",
      "                                                                  'lstm[13][0]',                  \n",
      "                                                                  'lstm[14][0]',                  \n",
      "                                                                  'lstm[15][0]',                  \n",
      "                                                                  'lstm[16][0]',                  \n",
      "                                                                  'lstm[17][0]',                  \n",
      "                                                                  'lstm[18][0]',                  \n",
      "                                                                  'lstm[19][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 54,329\n",
      "Trainable params: 54,329\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile the Model\n",
    "\n",
    "* After creating your model in Keras, you need to compile it and define the loss function, optimizer and metrics you want to use. \n",
    "    * Loss function: 'categorical_crossentropy'.\n",
    "    * Optimizer: [Adam](https://keras.io/optimizers/#adam) [optimizer](https://keras.io/optimizers/#usage-of-optimizers)\n",
    "        - learning rate = 0.005 \n",
    "        - $\\beta_1 = 0.9$\n",
    "        - $\\beta_2 = 0.999$\n",
    "        - decay = 0.01  \n",
    "    * metric: 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arash\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define inputs and outputs, and fit the model\n",
    "The last step is to define all our inputs and outputs to fit the model:\n",
    "- You need to create `s0` and `c0` to initialize your `post_attention_LSTM_cell` with zeros.\n",
    "    - The list `outputs[i][0], ..., outputs[i][Ty]` represents the true labels (characters) corresponding to the $i^{th}$ training example (`X[i]`). \n",
    "    - `outputs[i][j]` is the true label of the $j^{th}$ character in the $i^{th}$ training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(Yoh.swapaxes(0,1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now fit the model and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "49/49 [==============================] - 50s 109ms/step - loss: 25.7543 - dense_2_loss: 3.5274 - dense_2_1_loss: 3.4345 - dense_2_2_loss: 3.5348 - dense_2_3_loss: 3.5142 - dense_2_4_loss: 3.0345 - dense_2_5_loss: 2.1599 - dense_2_6_loss: 1.5091 - dense_2_7_loss: 1.0058 - dense_2_8_loss: 0.6768 - dense_2_9_loss: 0.4913 - dense_2_10_loss: 0.3775 - dense_2_11_loss: 0.3188 - dense_2_12_loss: 0.2911 - dense_2_13_loss: 0.2786 - dense_2_14_loss: 0.2726 - dense_2_15_loss: 0.2708 - dense_2_16_loss: 0.2666 - dense_2_17_loss: 0.2648 - dense_2_18_loss: 0.2631 - dense_2_19_loss: 0.2623 - dense_2_accuracy: 0.0358 - dense_2_1_accuracy: 0.0360 - dense_2_2_accuracy: 0.0309 - dense_2_3_accuracy: 0.0994 - dense_2_4_accuracy: 0.2851 - dense_2_5_accuracy: 0.5403 - dense_2_6_accuracy: 0.7145 - dense_2_7_accuracy: 0.8337 - dense_2_8_accuracy: 0.9113 - dense_2_9_accuracy: 0.9547 - dense_2_10_accuracy: 0.9763 - dense_2_11_accuracy: 0.9891 - dense_2_12_accuracy: 0.9949 - dense_2_13_accuracy: 0.9965 - dense_2_14_accuracy: 0.9971 - dense_2_15_accuracy: 0.9975 - dense_2_16_accuracy: 0.9979 - dense_2_17_accuracy: 0.9981 - dense_2_18_accuracy: 0.9981 - dense_2_19_accuracy: 0.9981\n",
      "Epoch 2/100\n",
      "49/49 [==============================] - 6s 119ms/step - loss: 19.1356 - dense_2_loss: 3.4067 - dense_2_1_loss: 3.1536 - dense_2_2_loss: 3.1374 - dense_2_3_loss: 2.9667 - dense_2_4_loss: 2.4257 - dense_2_5_loss: 1.6040 - dense_2_6_loss: 1.0423 - dense_2_7_loss: 0.6247 - dense_2_8_loss: 0.3443 - dense_2_9_loss: 0.1962 - dense_2_10_loss: 0.0963 - dense_2_11_loss: 0.0489 - dense_2_12_loss: 0.0261 - dense_2_13_loss: 0.0186 - dense_2_14_loss: 0.0148 - dense_2_15_loss: 0.0085 - dense_2_16_loss: 0.0066 - dense_2_17_loss: 0.0054 - dense_2_18_loss: 0.0046 - dense_2_19_loss: 0.0043 - dense_2_accuracy: 0.0797 - dense_2_1_accuracy: 0.1418 - dense_2_2_accuracy: 0.1546 - dense_2_3_accuracy: 0.1669 - dense_2_4_accuracy: 0.3427 - dense_2_5_accuracy: 0.5700 - dense_2_6_accuracy: 0.7275 - dense_2_7_accuracy: 0.8419 - dense_2_8_accuracy: 0.9135 - dense_2_9_accuracy: 0.9541 - dense_2_10_accuracy: 0.9776 - dense_2_11_accuracy: 0.9893 - dense_2_12_accuracy: 0.9949 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9977 - dense_2_15_accuracy: 0.9990 - dense_2_16_accuracy: 0.9992 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 3/100\n",
      "49/49 [==============================] - 6s 122ms/step - loss: 18.5741 - dense_2_loss: 3.3570 - dense_2_1_loss: 3.0909 - dense_2_2_loss: 3.0795 - dense_2_3_loss: 2.9015 - dense_2_4_loss: 2.3435 - dense_2_5_loss: 1.5362 - dense_2_6_loss: 0.9828 - dense_2_7_loss: 0.5844 - dense_2_8_loss: 0.3138 - dense_2_9_loss: 0.1772 - dense_2_10_loss: 0.0874 - dense_2_11_loss: 0.0473 - dense_2_12_loss: 0.0256 - dense_2_13_loss: 0.0153 - dense_2_14_loss: 0.0100 - dense_2_15_loss: 0.0061 - dense_2_16_loss: 0.0045 - dense_2_17_loss: 0.0039 - dense_2_18_loss: 0.0037 - dense_2_19_loss: 0.0033 - dense_2_accuracy: 0.0797 - dense_2_1_accuracy: 0.1416 - dense_2_2_accuracy: 0.1601 - dense_2_3_accuracy: 0.1848 - dense_2_4_accuracy: 0.3586 - dense_2_5_accuracy: 0.5875 - dense_2_6_accuracy: 0.7347 - dense_2_7_accuracy: 0.8454 - dense_2_8_accuracy: 0.9162 - dense_2_9_accuracy: 0.9549 - dense_2_10_accuracy: 0.9767 - dense_2_11_accuracy: 0.9889 - dense_2_12_accuracy: 0.9944 - dense_2_13_accuracy: 0.9969 - dense_2_14_accuracy: 0.9981 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 4/100\n",
      "49/49 [==============================] - 6s 122ms/step - loss: 18.3700 - dense_2_loss: 3.3232 - dense_2_1_loss: 3.0669 - dense_2_2_loss: 3.0610 - dense_2_3_loss: 2.8788 - dense_2_4_loss: 2.3211 - dense_2_5_loss: 1.5223 - dense_2_6_loss: 0.9680 - dense_2_7_loss: 0.5704 - dense_2_8_loss: 0.3017 - dense_2_9_loss: 0.1659 - dense_2_10_loss: 0.0819 - dense_2_11_loss: 0.0429 - dense_2_12_loss: 0.0231 - dense_2_13_loss: 0.0132 - dense_2_14_loss: 0.0098 - dense_2_15_loss: 0.0055 - dense_2_16_loss: 0.0042 - dense_2_17_loss: 0.0037 - dense_2_18_loss: 0.0033 - dense_2_19_loss: 0.0029 - dense_2_accuracy: 0.0797 - dense_2_1_accuracy: 0.1418 - dense_2_2_accuracy: 0.1601 - dense_2_3_accuracy: 0.1902 - dense_2_4_accuracy: 0.3604 - dense_2_5_accuracy: 0.5830 - dense_2_6_accuracy: 0.7332 - dense_2_7_accuracy: 0.8477 - dense_2_8_accuracy: 0.9177 - dense_2_9_accuracy: 0.9559 - dense_2_10_accuracy: 0.9780 - dense_2_11_accuracy: 0.9895 - dense_2_12_accuracy: 0.9949 - dense_2_13_accuracy: 0.9973 - dense_2_14_accuracy: 0.9981 - dense_2_15_accuracy: 0.9996 - dense_2_16_accuracy: 0.9994 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 5/100\n",
      "49/49 [==============================] - 6s 124ms/step - loss: 18.2673 - dense_2_loss: 3.2995 - dense_2_1_loss: 3.0537 - dense_2_2_loss: 3.0483 - dense_2_3_loss: 2.8625 - dense_2_4_loss: 2.3071 - dense_2_5_loss: 1.5156 - dense_2_6_loss: 0.9609 - dense_2_7_loss: 0.5664 - dense_2_8_loss: 0.2984 - dense_2_9_loss: 0.1653 - dense_2_10_loss: 0.0811 - dense_2_11_loss: 0.0429 - dense_2_12_loss: 0.0240 - dense_2_13_loss: 0.0132 - dense_2_14_loss: 0.0105 - dense_2_15_loss: 0.0051 - dense_2_16_loss: 0.0037 - dense_2_17_loss: 0.0034 - dense_2_18_loss: 0.0031 - dense_2_19_loss: 0.0027 - dense_2_accuracy: 0.0797 - dense_2_1_accuracy: 0.1420 - dense_2_2_accuracy: 0.1630 - dense_2_3_accuracy: 0.1960 - dense_2_4_accuracy: 0.3584 - dense_2_5_accuracy: 0.5823 - dense_2_6_accuracy: 0.7382 - dense_2_7_accuracy: 0.8466 - dense_2_8_accuracy: 0.9193 - dense_2_9_accuracy: 0.9564 - dense_2_10_accuracy: 0.9784 - dense_2_11_accuracy: 0.9895 - dense_2_12_accuracy: 0.9942 - dense_2_13_accuracy: 0.9977 - dense_2_14_accuracy: 0.9981 - dense_2_15_accuracy: 0.9996 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 6/100\n",
      "49/49 [==============================] - 6s 128ms/step - loss: 18.1288 - dense_2_loss: 3.2788 - dense_2_1_loss: 3.0344 - dense_2_2_loss: 3.0263 - dense_2_3_loss: 2.8352 - dense_2_4_loss: 2.2799 - dense_2_5_loss: 1.4988 - dense_2_6_loss: 0.9522 - dense_2_7_loss: 0.5646 - dense_2_8_loss: 0.2969 - dense_2_9_loss: 0.1686 - dense_2_10_loss: 0.0834 - dense_2_11_loss: 0.0453 - dense_2_12_loss: 0.0229 - dense_2_13_loss: 0.0137 - dense_2_14_loss: 0.0098 - dense_2_15_loss: 0.0050 - dense_2_16_loss: 0.0036 - dense_2_17_loss: 0.0035 - dense_2_18_loss: 0.0030 - dense_2_19_loss: 0.0026 - dense_2_accuracy: 0.0797 - dense_2_1_accuracy: 0.1420 - dense_2_2_accuracy: 0.1721 - dense_2_3_accuracy: 0.2176 - dense_2_4_accuracy: 0.3716 - dense_2_5_accuracy: 0.5875 - dense_2_6_accuracy: 0.7357 - dense_2_7_accuracy: 0.8493 - dense_2_8_accuracy: 0.9197 - dense_2_9_accuracy: 0.9562 - dense_2_10_accuracy: 0.9784 - dense_2_11_accuracy: 0.9891 - dense_2_12_accuracy: 0.9946 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9981 - dense_2_15_accuracy: 0.9996 - dense_2_16_accuracy: 0.9994 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 7/100\n",
      "49/49 [==============================] - 6s 123ms/step - loss: 17.9352 - dense_2_loss: 3.2589 - dense_2_1_loss: 3.0058 - dense_2_2_loss: 2.9891 - dense_2_3_loss: 2.8052 - dense_2_4_loss: 2.2509 - dense_2_5_loss: 1.4822 - dense_2_6_loss: 0.9413 - dense_2_7_loss: 0.5579 - dense_2_8_loss: 0.2915 - dense_2_9_loss: 0.1655 - dense_2_10_loss: 0.0808 - dense_2_11_loss: 0.0422 - dense_2_12_loss: 0.0224 - dense_2_13_loss: 0.0131 - dense_2_14_loss: 0.0100 - dense_2_15_loss: 0.0054 - dense_2_16_loss: 0.0039 - dense_2_17_loss: 0.0036 - dense_2_18_loss: 0.0030 - dense_2_19_loss: 0.0026 - dense_2_accuracy: 0.0856 - dense_2_1_accuracy: 0.1562 - dense_2_2_accuracy: 0.1869 - dense_2_3_accuracy: 0.2225 - dense_2_4_accuracy: 0.3761 - dense_2_5_accuracy: 0.5904 - dense_2_6_accuracy: 0.7353 - dense_2_7_accuracy: 0.8479 - dense_2_8_accuracy: 0.9193 - dense_2_9_accuracy: 0.9574 - dense_2_10_accuracy: 0.9778 - dense_2_11_accuracy: 0.9903 - dense_2_12_accuracy: 0.9949 - dense_2_13_accuracy: 0.9975 - dense_2_14_accuracy: 0.9979 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9994 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 8/100\n",
      "49/49 [==============================] - 6s 124ms/step - loss: 17.8007 - dense_2_loss: 3.2391 - dense_2_1_loss: 2.9873 - dense_2_2_loss: 2.9680 - dense_2_3_loss: 2.7774 - dense_2_4_loss: 2.2310 - dense_2_5_loss: 1.4675 - dense_2_6_loss: 0.9328 - dense_2_7_loss: 0.5526 - dense_2_8_loss: 0.2884 - dense_2_9_loss: 0.1650 - dense_2_10_loss: 0.0814 - dense_2_11_loss: 0.0441 - dense_2_12_loss: 0.0233 - dense_2_13_loss: 0.0136 - dense_2_14_loss: 0.0105 - dense_2_15_loss: 0.0057 - dense_2_16_loss: 0.0039 - dense_2_17_loss: 0.0035 - dense_2_18_loss: 0.0029 - dense_2_19_loss: 0.0025 - dense_2_accuracy: 0.1075 - dense_2_1_accuracy: 0.1719 - dense_2_2_accuracy: 0.1914 - dense_2_3_accuracy: 0.2270 - dense_2_4_accuracy: 0.3775 - dense_2_5_accuracy: 0.5928 - dense_2_6_accuracy: 0.7382 - dense_2_7_accuracy: 0.8483 - dense_2_8_accuracy: 0.9205 - dense_2_9_accuracy: 0.9564 - dense_2_10_accuracy: 0.9772 - dense_2_11_accuracy: 0.9891 - dense_2_12_accuracy: 0.9946 - dense_2_13_accuracy: 0.9977 - dense_2_14_accuracy: 0.9977 - dense_2_15_accuracy: 0.9990 - dense_2_16_accuracy: 0.9994 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 9/100\n",
      "49/49 [==============================] - 6s 121ms/step - loss: 17.7713 - dense_2_loss: 3.2221 - dense_2_1_loss: 2.9760 - dense_2_2_loss: 2.9497 - dense_2_3_loss: 2.7631 - dense_2_4_loss: 2.2185 - dense_2_5_loss: 1.4693 - dense_2_6_loss: 0.9357 - dense_2_7_loss: 0.5626 - dense_2_8_loss: 0.3008 - dense_2_9_loss: 0.1730 - dense_2_10_loss: 0.0859 - dense_2_11_loss: 0.0461 - dense_2_12_loss: 0.0245 - dense_2_13_loss: 0.0149 - dense_2_14_loss: 0.0111 - dense_2_15_loss: 0.0058 - dense_2_16_loss: 0.0034 - dense_2_17_loss: 0.0033 - dense_2_18_loss: 0.0029 - dense_2_19_loss: 0.0025 - dense_2_accuracy: 0.1169 - dense_2_1_accuracy: 0.1737 - dense_2_2_accuracy: 0.1918 - dense_2_3_accuracy: 0.2254 - dense_2_4_accuracy: 0.3771 - dense_2_5_accuracy: 0.5883 - dense_2_6_accuracy: 0.7363 - dense_2_7_accuracy: 0.8483 - dense_2_8_accuracy: 0.9193 - dense_2_9_accuracy: 0.9553 - dense_2_10_accuracy: 0.9765 - dense_2_11_accuracy: 0.9885 - dense_2_12_accuracy: 0.9946 - dense_2_13_accuracy: 0.9973 - dense_2_14_accuracy: 0.9981 - dense_2_15_accuracy: 0.9990 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 10/100\n",
      "49/49 [==============================] - 6s 126ms/step - loss: 17.5348 - dense_2_loss: 3.2096 - dense_2_1_loss: 2.9603 - dense_2_2_loss: 2.9287 - dense_2_3_loss: 2.7307 - dense_2_4_loss: 2.1806 - dense_2_5_loss: 1.4389 - dense_2_6_loss: 0.9087 - dense_2_7_loss: 0.5370 - dense_2_8_loss: 0.2853 - dense_2_9_loss: 0.1649 - dense_2_10_loss: 0.0818 - dense_2_11_loss: 0.0427 - dense_2_12_loss: 0.0233 - dense_2_13_loss: 0.0145 - dense_2_14_loss: 0.0103 - dense_2_15_loss: 0.0052 - dense_2_16_loss: 0.0036 - dense_2_17_loss: 0.0032 - dense_2_18_loss: 0.0028 - dense_2_19_loss: 0.0025 - dense_2_accuracy: 0.1223 - dense_2_1_accuracy: 0.1762 - dense_2_2_accuracy: 0.1933 - dense_2_3_accuracy: 0.2285 - dense_2_4_accuracy: 0.3779 - dense_2_5_accuracy: 0.5914 - dense_2_6_accuracy: 0.7458 - dense_2_7_accuracy: 0.8479 - dense_2_8_accuracy: 0.9201 - dense_2_9_accuracy: 0.9564 - dense_2_10_accuracy: 0.9774 - dense_2_11_accuracy: 0.9899 - dense_2_12_accuracy: 0.9949 - dense_2_13_accuracy: 0.9973 - dense_2_14_accuracy: 0.9981 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 11/100\n",
      "49/49 [==============================] - 6s 124ms/step - loss: 17.3392 - dense_2_loss: 3.1894 - dense_2_1_loss: 2.9399 - dense_2_2_loss: 2.8981 - dense_2_3_loss: 2.6920 - dense_2_4_loss: 2.1447 - dense_2_5_loss: 1.4190 - dense_2_6_loss: 0.8933 - dense_2_7_loss: 0.5283 - dense_2_8_loss: 0.2817 - dense_2_9_loss: 0.1628 - dense_2_10_loss: 0.0825 - dense_2_11_loss: 0.0421 - dense_2_12_loss: 0.0233 - dense_2_13_loss: 0.0141 - dense_2_14_loss: 0.0103 - dense_2_15_loss: 0.0052 - dense_2_16_loss: 0.0037 - dense_2_17_loss: 0.0033 - dense_2_18_loss: 0.0028 - dense_2_19_loss: 0.0025 - dense_2_accuracy: 0.1256 - dense_2_1_accuracy: 0.1814 - dense_2_2_accuracy: 0.1921 - dense_2_3_accuracy: 0.2351 - dense_2_4_accuracy: 0.3849 - dense_2_5_accuracy: 0.5914 - dense_2_6_accuracy: 0.7452 - dense_2_7_accuracy: 0.8514 - dense_2_8_accuracy: 0.9203 - dense_2_9_accuracy: 0.9555 - dense_2_10_accuracy: 0.9776 - dense_2_11_accuracy: 0.9895 - dense_2_12_accuracy: 0.9944 - dense_2_13_accuracy: 0.9973 - dense_2_14_accuracy: 0.9979 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 12/100\n",
      "49/49 [==============================] - 6s 124ms/step - loss: 17.1569 - dense_2_loss: 3.1725 - dense_2_1_loss: 2.9213 - dense_2_2_loss: 2.8680 - dense_2_3_loss: 2.6527 - dense_2_4_loss: 2.1133 - dense_2_5_loss: 1.4008 - dense_2_6_loss: 0.8795 - dense_2_7_loss: 0.5199 - dense_2_8_loss: 0.2789 - dense_2_9_loss: 0.1621 - dense_2_10_loss: 0.0809 - dense_2_11_loss: 0.0423 - dense_2_12_loss: 0.0237 - dense_2_13_loss: 0.0140 - dense_2_14_loss: 0.0097 - dense_2_15_loss: 0.0054 - dense_2_16_loss: 0.0035 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0028 - dense_2_19_loss: 0.0025 - dense_2_accuracy: 0.1274 - dense_2_1_accuracy: 0.1851 - dense_2_2_accuracy: 0.1943 - dense_2_3_accuracy: 0.2538 - dense_2_4_accuracy: 0.3895 - dense_2_5_accuracy: 0.5947 - dense_2_6_accuracy: 0.7464 - dense_2_7_accuracy: 0.8497 - dense_2_8_accuracy: 0.9216 - dense_2_9_accuracy: 0.9564 - dense_2_10_accuracy: 0.9778 - dense_2_11_accuracy: 0.9893 - dense_2_12_accuracy: 0.9940 - dense_2_13_accuracy: 0.9977 - dense_2_14_accuracy: 0.9981 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 13/100\n",
      "49/49 [==============================] - 6s 124ms/step - loss: 16.9760 - dense_2_loss: 3.1567 - dense_2_1_loss: 2.9028 - dense_2_2_loss: 2.8430 - dense_2_3_loss: 2.6156 - dense_2_4_loss: 2.0767 - dense_2_5_loss: 1.3811 - dense_2_6_loss: 0.8644 - dense_2_7_loss: 0.5140 - dense_2_8_loss: 0.2746 - dense_2_9_loss: 0.1594 - dense_2_10_loss: 0.0805 - dense_2_11_loss: 0.0423 - dense_2_12_loss: 0.0232 - dense_2_13_loss: 0.0135 - dense_2_14_loss: 0.0094 - dense_2_15_loss: 0.0051 - dense_2_16_loss: 0.0044 - dense_2_17_loss: 0.0037 - dense_2_18_loss: 0.0030 - dense_2_19_loss: 0.0025 - dense_2_accuracy: 0.1326 - dense_2_1_accuracy: 0.1842 - dense_2_2_accuracy: 0.2015 - dense_2_3_accuracy: 0.2629 - dense_2_4_accuracy: 0.3944 - dense_2_5_accuracy: 0.5976 - dense_2_6_accuracy: 0.7478 - dense_2_7_accuracy: 0.8514 - dense_2_8_accuracy: 0.9222 - dense_2_9_accuracy: 0.9576 - dense_2_10_accuracy: 0.9784 - dense_2_11_accuracy: 0.9895 - dense_2_12_accuracy: 0.9946 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9981 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9992 - dense_2_17_accuracy: 0.9996 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 14/100\n",
      "49/49 [==============================] - 6s 125ms/step - loss: 16.8190 - dense_2_loss: 3.1406 - dense_2_1_loss: 2.8888 - dense_2_2_loss: 2.8183 - dense_2_3_loss: 2.5829 - dense_2_4_loss: 2.0475 - dense_2_5_loss: 1.3621 - dense_2_6_loss: 0.8538 - dense_2_7_loss: 0.5074 - dense_2_8_loss: 0.2723 - dense_2_9_loss: 0.1594 - dense_2_10_loss: 0.0795 - dense_2_11_loss: 0.0411 - dense_2_12_loss: 0.0236 - dense_2_13_loss: 0.0136 - dense_2_14_loss: 0.0101 - dense_2_15_loss: 0.0052 - dense_2_16_loss: 0.0039 - dense_2_17_loss: 0.0034 - dense_2_18_loss: 0.0029 - dense_2_19_loss: 0.0025 - dense_2_accuracy: 0.1398 - dense_2_1_accuracy: 0.1824 - dense_2_2_accuracy: 0.2069 - dense_2_3_accuracy: 0.2736 - dense_2_4_accuracy: 0.4020 - dense_2_5_accuracy: 0.5998 - dense_2_6_accuracy: 0.7472 - dense_2_7_accuracy: 0.8530 - dense_2_8_accuracy: 0.9216 - dense_2_9_accuracy: 0.9568 - dense_2_10_accuracy: 0.9788 - dense_2_11_accuracy: 0.9899 - dense_2_12_accuracy: 0.9944 - dense_2_13_accuracy: 0.9969 - dense_2_14_accuracy: 0.9977 - dense_2_15_accuracy: 0.9996 - dense_2_16_accuracy: 0.9994 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 15/100\n",
      "49/49 [==============================] - 6s 126ms/step - loss: 16.6728 - dense_2_loss: 3.1271 - dense_2_1_loss: 2.8696 - dense_2_2_loss: 2.7954 - dense_2_3_loss: 2.5548 - dense_2_4_loss: 2.0256 - dense_2_5_loss: 1.3455 - dense_2_6_loss: 0.8431 - dense_2_7_loss: 0.5016 - dense_2_8_loss: 0.2703 - dense_2_9_loss: 0.1571 - dense_2_10_loss: 0.0796 - dense_2_11_loss: 0.0419 - dense_2_12_loss: 0.0223 - dense_2_13_loss: 0.0132 - dense_2_14_loss: 0.0092 - dense_2_15_loss: 0.0048 - dense_2_16_loss: 0.0035 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0027 - dense_2_19_loss: 0.0025 - dense_2_accuracy: 0.1497 - dense_2_1_accuracy: 0.1894 - dense_2_2_accuracy: 0.2128 - dense_2_3_accuracy: 0.2793 - dense_2_4_accuracy: 0.4055 - dense_2_5_accuracy: 0.6050 - dense_2_6_accuracy: 0.7511 - dense_2_7_accuracy: 0.8538 - dense_2_8_accuracy: 0.9218 - dense_2_9_accuracy: 0.9570 - dense_2_10_accuracy: 0.9780 - dense_2_11_accuracy: 0.9893 - dense_2_12_accuracy: 0.9955 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9981 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 16/100\n",
      "49/49 [==============================] - 6s 124ms/step - loss: 16.4962 - dense_2_loss: 3.1104 - dense_2_1_loss: 2.8525 - dense_2_2_loss: 2.7726 - dense_2_3_loss: 2.5227 - dense_2_4_loss: 1.9932 - dense_2_5_loss: 1.3292 - dense_2_6_loss: 0.8291 - dense_2_7_loss: 0.4897 - dense_2_8_loss: 0.2648 - dense_2_9_loss: 0.1539 - dense_2_10_loss: 0.0766 - dense_2_11_loss: 0.0403 - dense_2_12_loss: 0.0214 - dense_2_13_loss: 0.0129 - dense_2_14_loss: 0.0095 - dense_2_15_loss: 0.0048 - dense_2_16_loss: 0.0037 - dense_2_17_loss: 0.0033 - dense_2_18_loss: 0.0027 - dense_2_19_loss: 0.0025 - dense_2_accuracy: 0.1542 - dense_2_1_accuracy: 0.1923 - dense_2_2_accuracy: 0.2151 - dense_2_3_accuracy: 0.2919 - dense_2_4_accuracy: 0.4131 - dense_2_5_accuracy: 0.6058 - dense_2_6_accuracy: 0.7501 - dense_2_7_accuracy: 0.8541 - dense_2_8_accuracy: 0.9230 - dense_2_9_accuracy: 0.9594 - dense_2_10_accuracy: 0.9790 - dense_2_11_accuracy: 0.9899 - dense_2_12_accuracy: 0.9949 - dense_2_13_accuracy: 0.9973 - dense_2_14_accuracy: 0.9981 - dense_2_15_accuracy: 0.9996 - dense_2_16_accuracy: 0.9994 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 17/100\n",
      "49/49 [==============================] - 6s 124ms/step - loss: 16.3667 - dense_2_loss: 3.0934 - dense_2_1_loss: 2.8338 - dense_2_2_loss: 2.7479 - dense_2_3_loss: 2.4942 - dense_2_4_loss: 1.9665 - dense_2_5_loss: 1.3131 - dense_2_6_loss: 0.8221 - dense_2_7_loss: 0.4925 - dense_2_8_loss: 0.2643 - dense_2_9_loss: 0.1560 - dense_2_10_loss: 0.0790 - dense_2_11_loss: 0.0426 - dense_2_12_loss: 0.0222 - dense_2_13_loss: 0.0134 - dense_2_14_loss: 0.0092 - dense_2_15_loss: 0.0046 - dense_2_16_loss: 0.0035 - dense_2_17_loss: 0.0034 - dense_2_18_loss: 0.0027 - dense_2_19_loss: 0.0024 - dense_2_accuracy: 0.1604 - dense_2_1_accuracy: 0.1995 - dense_2_2_accuracy: 0.2217 - dense_2_3_accuracy: 0.2993 - dense_2_4_accuracy: 0.4207 - dense_2_5_accuracy: 0.6095 - dense_2_6_accuracy: 0.7538 - dense_2_7_accuracy: 0.8555 - dense_2_8_accuracy: 0.9222 - dense_2_9_accuracy: 0.9576 - dense_2_10_accuracy: 0.9784 - dense_2_11_accuracy: 0.9893 - dense_2_12_accuracy: 0.9949 - dense_2_13_accuracy: 0.9969 - dense_2_14_accuracy: 0.9981 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9994 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 18/100\n",
      "49/49 [==============================] - 6s 122ms/step - loss: 16.1704 - dense_2_loss: 3.0759 - dense_2_1_loss: 2.8144 - dense_2_2_loss: 2.7168 - dense_2_3_loss: 2.4578 - dense_2_4_loss: 1.9342 - dense_2_5_loss: 1.2894 - dense_2_6_loss: 0.8047 - dense_2_7_loss: 0.4823 - dense_2_8_loss: 0.2610 - dense_2_9_loss: 0.1548 - dense_2_10_loss: 0.0777 - dense_2_11_loss: 0.0411 - dense_2_12_loss: 0.0215 - dense_2_13_loss: 0.0129 - dense_2_14_loss: 0.0093 - dense_2_15_loss: 0.0049 - dense_2_16_loss: 0.0033 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0027 - dense_2_19_loss: 0.0025 - dense_2_accuracy: 0.1653 - dense_2_1_accuracy: 0.2093 - dense_2_2_accuracy: 0.2293 - dense_2_3_accuracy: 0.3059 - dense_2_4_accuracy: 0.4212 - dense_2_5_accuracy: 0.6081 - dense_2_6_accuracy: 0.7563 - dense_2_7_accuracy: 0.8578 - dense_2_8_accuracy: 0.9234 - dense_2_9_accuracy: 0.9584 - dense_2_10_accuracy: 0.9784 - dense_2_11_accuracy: 0.9889 - dense_2_12_accuracy: 0.9955 - dense_2_13_accuracy: 0.9973 - dense_2_14_accuracy: 0.9981 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 19/100\n",
      "49/49 [==============================] - 6s 125ms/step - loss: 15.9840 - dense_2_loss: 3.0596 - dense_2_1_loss: 2.7890 - dense_2_2_loss: 2.6871 - dense_2_3_loss: 2.4248 - dense_2_4_loss: 1.9054 - dense_2_5_loss: 1.2682 - dense_2_6_loss: 0.7905 - dense_2_7_loss: 0.4744 - dense_2_8_loss: 0.2550 - dense_2_9_loss: 0.1503 - dense_2_10_loss: 0.0768 - dense_2_11_loss: 0.0411 - dense_2_12_loss: 0.0218 - dense_2_13_loss: 0.0138 - dense_2_14_loss: 0.0096 - dense_2_15_loss: 0.0048 - dense_2_16_loss: 0.0033 - dense_2_17_loss: 0.0032 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0024 - dense_2_accuracy: 0.1717 - dense_2_1_accuracy: 0.2213 - dense_2_2_accuracy: 0.2291 - dense_2_3_accuracy: 0.3094 - dense_2_4_accuracy: 0.4292 - dense_2_5_accuracy: 0.6077 - dense_2_6_accuracy: 0.7546 - dense_2_7_accuracy: 0.8569 - dense_2_8_accuracy: 0.9232 - dense_2_9_accuracy: 0.9597 - dense_2_10_accuracy: 0.9769 - dense_2_11_accuracy: 0.9891 - dense_2_12_accuracy: 0.9942 - dense_2_13_accuracy: 0.9969 - dense_2_14_accuracy: 0.9981 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9994 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 20/100\n",
      "49/49 [==============================] - 6s 123ms/step - loss: 15.8478 - dense_2_loss: 3.0402 - dense_2_1_loss: 2.7675 - dense_2_2_loss: 2.6606 - dense_2_3_loss: 2.3927 - dense_2_4_loss: 1.8853 - dense_2_5_loss: 1.2560 - dense_2_6_loss: 0.7871 - dense_2_7_loss: 0.4761 - dense_2_8_loss: 0.2543 - dense_2_9_loss: 0.1506 - dense_2_10_loss: 0.0754 - dense_2_11_loss: 0.0408 - dense_2_12_loss: 0.0216 - dense_2_13_loss: 0.0131 - dense_2_14_loss: 0.0096 - dense_2_15_loss: 0.0050 - dense_2_16_loss: 0.0035 - dense_2_17_loss: 0.0033 - dense_2_18_loss: 0.0027 - dense_2_19_loss: 0.0025 - dense_2_accuracy: 0.1746 - dense_2_1_accuracy: 0.2291 - dense_2_2_accuracy: 0.2312 - dense_2_3_accuracy: 0.3147 - dense_2_4_accuracy: 0.4284 - dense_2_5_accuracy: 0.6107 - dense_2_6_accuracy: 0.7538 - dense_2_7_accuracy: 0.8569 - dense_2_8_accuracy: 0.9247 - dense_2_9_accuracy: 0.9599 - dense_2_10_accuracy: 0.9774 - dense_2_11_accuracy: 0.9891 - dense_2_12_accuracy: 0.9949 - dense_2_13_accuracy: 0.9973 - dense_2_14_accuracy: 0.9981 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9994 - dense_2_17_accuracy: 0.9996 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 21/100\n",
      "49/49 [==============================] - 6s 125ms/step - loss: 15.7445 - dense_2_loss: 3.0241 - dense_2_1_loss: 2.7435 - dense_2_2_loss: 2.6390 - dense_2_3_loss: 2.3744 - dense_2_4_loss: 1.8653 - dense_2_5_loss: 1.2524 - dense_2_6_loss: 0.7823 - dense_2_7_loss: 0.4727 - dense_2_8_loss: 0.2554 - dense_2_9_loss: 0.1511 - dense_2_10_loss: 0.0772 - dense_2_11_loss: 0.0416 - dense_2_12_loss: 0.0231 - dense_2_13_loss: 0.0151 - dense_2_14_loss: 0.0107 - dense_2_15_loss: 0.0049 - dense_2_16_loss: 0.0033 - dense_2_17_loss: 0.0034 - dense_2_18_loss: 0.0027 - dense_2_19_loss: 0.0024 - dense_2_accuracy: 0.1805 - dense_2_1_accuracy: 0.2355 - dense_2_2_accuracy: 0.2308 - dense_2_3_accuracy: 0.3154 - dense_2_4_accuracy: 0.4333 - dense_2_5_accuracy: 0.6126 - dense_2_6_accuracy: 0.7546 - dense_2_7_accuracy: 0.8578 - dense_2_8_accuracy: 0.9226 - dense_2_9_accuracy: 0.9592 - dense_2_10_accuracy: 0.9778 - dense_2_11_accuracy: 0.9897 - dense_2_12_accuracy: 0.9942 - dense_2_13_accuracy: 0.9969 - dense_2_14_accuracy: 0.9981 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9994 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998    \n",
      "Epoch 22/100\n",
      "49/49 [==============================] - 7s 135ms/step - loss: 15.5792 - dense_2_loss: 3.0047 - dense_2_1_loss: 2.7234 - dense_2_2_loss: 2.6168 - dense_2_3_loss: 2.3446 - dense_2_4_loss: 1.8425 - dense_2_5_loss: 1.2336 - dense_2_6_loss: 0.7730 - dense_2_7_loss: 0.4648 - dense_2_8_loss: 0.2515 - dense_2_9_loss: 0.1478 - dense_2_10_loss: 0.0748 - dense_2_11_loss: 0.0407 - dense_2_12_loss: 0.0217 - dense_2_13_loss: 0.0138 - dense_2_14_loss: 0.0100 - dense_2_15_loss: 0.0047 - dense_2_16_loss: 0.0029 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0023 - dense_2_accuracy: 0.1861 - dense_2_1_accuracy: 0.2359 - dense_2_2_accuracy: 0.2322 - dense_2_3_accuracy: 0.3217 - dense_2_4_accuracy: 0.4352 - dense_2_5_accuracy: 0.6165 - dense_2_6_accuracy: 0.7540 - dense_2_7_accuracy: 0.8611 - dense_2_8_accuracy: 0.9247 - dense_2_9_accuracy: 0.9597 - dense_2_10_accuracy: 0.9784 - dense_2_11_accuracy: 0.9897 - dense_2_12_accuracy: 0.9953 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9984 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 23/100\n",
      "49/49 [==============================] - 6s 121ms/step - loss: 15.4431 - dense_2_loss: 2.9879 - dense_2_1_loss: 2.7003 - dense_2_2_loss: 2.5970 - dense_2_3_loss: 2.3253 - dense_2_4_loss: 1.8268 - dense_2_5_loss: 1.2206 - dense_2_6_loss: 0.7604 - dense_2_7_loss: 0.4582 - dense_2_8_loss: 0.2469 - dense_2_9_loss: 0.1452 - dense_2_10_loss: 0.0732 - dense_2_11_loss: 0.0403 - dense_2_12_loss: 0.0223 - dense_2_13_loss: 0.0130 - dense_2_14_loss: 0.0092 - dense_2_15_loss: 0.0048 - dense_2_16_loss: 0.0034 - dense_2_17_loss: 0.0033 - dense_2_18_loss: 0.0027 - dense_2_19_loss: 0.0024 - dense_2_accuracy: 0.1896 - dense_2_1_accuracy: 0.2427 - dense_2_2_accuracy: 0.2353 - dense_2_3_accuracy: 0.3263 - dense_2_4_accuracy: 0.4405 - dense_2_5_accuracy: 0.6157 - dense_2_6_accuracy: 0.7581 - dense_2_7_accuracy: 0.8598 - dense_2_8_accuracy: 0.9253 - dense_2_9_accuracy: 0.9592 - dense_2_10_accuracy: 0.9792 - dense_2_11_accuracy: 0.9897 - dense_2_12_accuracy: 0.9951 - dense_2_13_accuracy: 0.9975 - dense_2_14_accuracy: 0.9984 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9994 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 24/100\n",
      "49/49 [==============================] - 6s 121ms/step - loss: 15.3391 - dense_2_loss: 2.9679 - dense_2_1_loss: 2.6809 - dense_2_2_loss: 2.5787 - dense_2_3_loss: 2.3061 - dense_2_4_loss: 1.8118 - dense_2_5_loss: 1.2121 - dense_2_6_loss: 0.7592 - dense_2_7_loss: 0.4564 - dense_2_8_loss: 0.2473 - dense_2_9_loss: 0.1444 - dense_2_10_loss: 0.0739 - dense_2_11_loss: 0.0400 - dense_2_12_loss: 0.0214 - dense_2_13_loss: 0.0129 - dense_2_14_loss: 0.0093 - dense_2_15_loss: 0.0048 - dense_2_16_loss: 0.0032 - dense_2_17_loss: 0.0034 - dense_2_18_loss: 0.0029 - dense_2_19_loss: 0.0025 - dense_2_accuracy: 0.1999 - dense_2_1_accuracy: 0.2456 - dense_2_2_accuracy: 0.2361 - dense_2_3_accuracy: 0.3261 - dense_2_4_accuracy: 0.4422 - dense_2_5_accuracy: 0.6175 - dense_2_6_accuracy: 0.7590 - dense_2_7_accuracy: 0.8604 - dense_2_8_accuracy: 0.9255 - dense_2_9_accuracy: 0.9603 - dense_2_10_accuracy: 0.9786 - dense_2_11_accuracy: 0.9903 - dense_2_12_accuracy: 0.9951 - dense_2_13_accuracy: 0.9973 - dense_2_14_accuracy: 0.9984 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9996 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 25/100\n",
      "49/49 [==============================] - 6s 122ms/step - loss: 15.2347 - dense_2_loss: 2.9485 - dense_2_1_loss: 2.6586 - dense_2_2_loss: 2.5611 - dense_2_3_loss: 2.2889 - dense_2_4_loss: 1.7984 - dense_2_5_loss: 1.2024 - dense_2_6_loss: 0.7523 - dense_2_7_loss: 0.4526 - dense_2_8_loss: 0.2480 - dense_2_9_loss: 0.1457 - dense_2_10_loss: 0.0753 - dense_2_11_loss: 0.0411 - dense_2_12_loss: 0.0224 - dense_2_13_loss: 0.0136 - dense_2_14_loss: 0.0098 - dense_2_15_loss: 0.0046 - dense_2_16_loss: 0.0032 - dense_2_17_loss: 0.0032 - dense_2_18_loss: 0.0027 - dense_2_19_loss: 0.0024 - dense_2_accuracy: 0.2048 - dense_2_1_accuracy: 0.2505 - dense_2_2_accuracy: 0.2423 - dense_2_3_accuracy: 0.3298 - dense_2_4_accuracy: 0.4399 - dense_2_5_accuracy: 0.6198 - dense_2_6_accuracy: 0.7600 - dense_2_7_accuracy: 0.8625 - dense_2_8_accuracy: 0.9251 - dense_2_9_accuracy: 0.9607 - dense_2_10_accuracy: 0.9784 - dense_2_11_accuracy: 0.9893 - dense_2_12_accuracy: 0.9951 - dense_2_13_accuracy: 0.9969 - dense_2_14_accuracy: 0.9984 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 26/100\n",
      "49/49 [==============================] - 6s 124ms/step - loss: 15.1300 - dense_2_loss: 2.9295 - dense_2_1_loss: 2.6394 - dense_2_2_loss: 2.5456 - dense_2_3_loss: 2.2738 - dense_2_4_loss: 1.7860 - dense_2_5_loss: 1.2000 - dense_2_6_loss: 0.7477 - dense_2_7_loss: 0.4478 - dense_2_8_loss: 0.2449 - dense_2_9_loss: 0.1434 - dense_2_10_loss: 0.0729 - dense_2_11_loss: 0.0394 - dense_2_12_loss: 0.0211 - dense_2_13_loss: 0.0126 - dense_2_14_loss: 0.0095 - dense_2_15_loss: 0.0047 - dense_2_16_loss: 0.0034 - dense_2_17_loss: 0.0032 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0023 - dense_2_accuracy: 0.2124 - dense_2_1_accuracy: 0.2575 - dense_2_2_accuracy: 0.2433 - dense_2_3_accuracy: 0.3324 - dense_2_4_accuracy: 0.4420 - dense_2_5_accuracy: 0.6175 - dense_2_6_accuracy: 0.7604 - dense_2_7_accuracy: 0.8623 - dense_2_8_accuracy: 0.9242 - dense_2_9_accuracy: 0.9603 - dense_2_10_accuracy: 0.9790 - dense_2_11_accuracy: 0.9899 - dense_2_12_accuracy: 0.9949 - dense_2_13_accuracy: 0.9973 - dense_2_14_accuracy: 0.9984 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 27/100\n",
      "49/49 [==============================] - 6s 124ms/step - loss: 15.0222 - dense_2_loss: 2.9090 - dense_2_1_loss: 2.6184 - dense_2_2_loss: 2.5282 - dense_2_3_loss: 2.2563 - dense_2_4_loss: 1.7733 - dense_2_5_loss: 1.1914 - dense_2_6_loss: 0.7440 - dense_2_7_loss: 0.4455 - dense_2_8_loss: 0.2446 - dense_2_9_loss: 0.1401 - dense_2_10_loss: 0.0723 - dense_2_11_loss: 0.0392 - dense_2_12_loss: 0.0213 - dense_2_13_loss: 0.0129 - dense_2_14_loss: 0.0096 - dense_2_15_loss: 0.0046 - dense_2_16_loss: 0.0032 - dense_2_17_loss: 0.0032 - dense_2_18_loss: 0.0027 - dense_2_19_loss: 0.0023 - dense_2_accuracy: 0.2229 - dense_2_1_accuracy: 0.2622 - dense_2_2_accuracy: 0.2421 - dense_2_3_accuracy: 0.3296 - dense_2_4_accuracy: 0.4487 - dense_2_5_accuracy: 0.6190 - dense_2_6_accuracy: 0.7602 - dense_2_7_accuracy: 0.8600 - dense_2_8_accuracy: 0.9232 - dense_2_9_accuracy: 0.9615 - dense_2_10_accuracy: 0.9784 - dense_2_11_accuracy: 0.9899 - dense_2_12_accuracy: 0.9951 - dense_2_13_accuracy: 0.9973 - dense_2_14_accuracy: 0.9984 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 28/100\n",
      "49/49 [==============================] - 6s 124ms/step - loss: 14.9356 - dense_2_loss: 2.8892 - dense_2_1_loss: 2.5980 - dense_2_2_loss: 2.5119 - dense_2_3_loss: 2.2443 - dense_2_4_loss: 1.7627 - dense_2_5_loss: 1.1853 - dense_2_6_loss: 0.7423 - dense_2_7_loss: 0.4448 - dense_2_8_loss: 0.2442 - dense_2_9_loss: 0.1403 - dense_2_10_loss: 0.0717 - dense_2_11_loss: 0.0393 - dense_2_12_loss: 0.0220 - dense_2_13_loss: 0.0129 - dense_2_14_loss: 0.0102 - dense_2_15_loss: 0.0051 - dense_2_16_loss: 0.0032 - dense_2_17_loss: 0.0032 - dense_2_18_loss: 0.0027 - dense_2_19_loss: 0.0024 - dense_2_accuracy: 0.2316 - dense_2_1_accuracy: 0.2651 - dense_2_2_accuracy: 0.2431 - dense_2_3_accuracy: 0.3345 - dense_2_4_accuracy: 0.4487 - dense_2_5_accuracy: 0.6208 - dense_2_6_accuracy: 0.7610 - dense_2_7_accuracy: 0.8592 - dense_2_8_accuracy: 0.9238 - dense_2_9_accuracy: 0.9619 - dense_2_10_accuracy: 0.9786 - dense_2_11_accuracy: 0.9901 - dense_2_12_accuracy: 0.9949 - dense_2_13_accuracy: 0.9973 - dense_2_14_accuracy: 0.9979 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 29/100\n",
      "49/49 [==============================] - 6s 123ms/step - loss: 14.8252 - dense_2_loss: 2.8677 - dense_2_1_loss: 2.5808 - dense_2_2_loss: 2.4978 - dense_2_3_loss: 2.2291 - dense_2_4_loss: 1.7496 - dense_2_5_loss: 1.1769 - dense_2_6_loss: 0.7323 - dense_2_7_loss: 0.4392 - dense_2_8_loss: 0.2433 - dense_2_9_loss: 0.1388 - dense_2_10_loss: 0.0710 - dense_2_11_loss: 0.0392 - dense_2_12_loss: 0.0214 - dense_2_13_loss: 0.0124 - dense_2_14_loss: 0.0095 - dense_2_15_loss: 0.0048 - dense_2_16_loss: 0.0034 - dense_2_17_loss: 0.0032 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0023 - dense_2_accuracy: 0.2380 - dense_2_1_accuracy: 0.2705 - dense_2_2_accuracy: 0.2462 - dense_2_3_accuracy: 0.3368 - dense_2_4_accuracy: 0.4508 - dense_2_5_accuracy: 0.6206 - dense_2_6_accuracy: 0.7645 - dense_2_7_accuracy: 0.8613 - dense_2_8_accuracy: 0.9228 - dense_2_9_accuracy: 0.9607 - dense_2_10_accuracy: 0.9782 - dense_2_11_accuracy: 0.9897 - dense_2_12_accuracy: 0.9946 - dense_2_13_accuracy: 0.9973 - dense_2_14_accuracy: 0.9979 - dense_2_15_accuracy: 0.9996 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 30/100\n",
      "49/49 [==============================] - 6s 123ms/step - loss: 14.7289 - dense_2_loss: 2.8461 - dense_2_1_loss: 2.5595 - dense_2_2_loss: 2.4796 - dense_2_3_loss: 2.2125 - dense_2_4_loss: 1.7393 - dense_2_5_loss: 1.1728 - dense_2_6_loss: 0.7313 - dense_2_7_loss: 0.4368 - dense_2_8_loss: 0.2430 - dense_2_9_loss: 0.1386 - dense_2_10_loss: 0.0703 - dense_2_11_loss: 0.0395 - dense_2_12_loss: 0.0206 - dense_2_13_loss: 0.0128 - dense_2_14_loss: 0.0100 - dense_2_15_loss: 0.0047 - dense_2_16_loss: 0.0032 - dense_2_17_loss: 0.0033 - dense_2_18_loss: 0.0027 - dense_2_19_loss: 0.0023 - dense_2_accuracy: 0.2452 - dense_2_1_accuracy: 0.2808 - dense_2_2_accuracy: 0.2483 - dense_2_3_accuracy: 0.3341 - dense_2_4_accuracy: 0.4539 - dense_2_5_accuracy: 0.6247 - dense_2_6_accuracy: 0.7657 - dense_2_7_accuracy: 0.8643 - dense_2_8_accuracy: 0.9236 - dense_2_9_accuracy: 0.9607 - dense_2_10_accuracy: 0.9786 - dense_2_11_accuracy: 0.9901 - dense_2_12_accuracy: 0.9951 - dense_2_13_accuracy: 0.9973 - dense_2_14_accuracy: 0.9979 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9998 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 31/100\n",
      "49/49 [==============================] - 6s 123ms/step - loss: 14.6369 - dense_2_loss: 2.8259 - dense_2_1_loss: 2.5402 - dense_2_2_loss: 2.4659 - dense_2_3_loss: 2.1996 - dense_2_4_loss: 1.7265 - dense_2_5_loss: 1.1671 - dense_2_6_loss: 0.7292 - dense_2_7_loss: 0.4368 - dense_2_8_loss: 0.2418 - dense_2_9_loss: 0.1365 - dense_2_10_loss: 0.0693 - dense_2_11_loss: 0.0387 - dense_2_12_loss: 0.0208 - dense_2_13_loss: 0.0125 - dense_2_14_loss: 0.0097 - dense_2_15_loss: 0.0046 - dense_2_16_loss: 0.0032 - dense_2_17_loss: 0.0035 - dense_2_18_loss: 0.0029 - dense_2_19_loss: 0.0024 - dense_2_accuracy: 0.2515 - dense_2_1_accuracy: 0.2828 - dense_2_2_accuracy: 0.2526 - dense_2_3_accuracy: 0.3357 - dense_2_4_accuracy: 0.4527 - dense_2_5_accuracy: 0.6237 - dense_2_6_accuracy: 0.7651 - dense_2_7_accuracy: 0.8625 - dense_2_8_accuracy: 0.9234 - dense_2_9_accuracy: 0.9601 - dense_2_10_accuracy: 0.9788 - dense_2_11_accuracy: 0.9911 - dense_2_12_accuracy: 0.9951 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9981 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9998 - dense_2_17_accuracy: 0.9996 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 32/100\n",
      "49/49 [==============================] - 6s 122ms/step - loss: 14.5398 - dense_2_loss: 2.8021 - dense_2_1_loss: 2.5174 - dense_2_2_loss: 2.4505 - dense_2_3_loss: 2.1863 - dense_2_4_loss: 1.7200 - dense_2_5_loss: 1.1624 - dense_2_6_loss: 0.7257 - dense_2_7_loss: 0.4323 - dense_2_8_loss: 0.2403 - dense_2_9_loss: 0.1362 - dense_2_10_loss: 0.0696 - dense_2_11_loss: 0.0383 - dense_2_12_loss: 0.0204 - dense_2_13_loss: 0.0120 - dense_2_14_loss: 0.0099 - dense_2_15_loss: 0.0048 - dense_2_16_loss: 0.0034 - dense_2_17_loss: 0.0033 - dense_2_18_loss: 0.0027 - dense_2_19_loss: 0.0023 - dense_2_accuracy: 0.2590 - dense_2_1_accuracy: 0.2900 - dense_2_2_accuracy: 0.2540 - dense_2_3_accuracy: 0.3392 - dense_2_4_accuracy: 0.4541 - dense_2_5_accuracy: 0.6231 - dense_2_6_accuracy: 0.7672 - dense_2_7_accuracy: 0.8621 - dense_2_8_accuracy: 0.9247 - dense_2_9_accuracy: 0.9605 - dense_2_10_accuracy: 0.9786 - dense_2_11_accuracy: 0.9909 - dense_2_12_accuracy: 0.9953 - dense_2_13_accuracy: 0.9973 - dense_2_14_accuracy: 0.9979 - dense_2_15_accuracy: 0.9996 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 33/100\n",
      "49/49 [==============================] - 6s 125ms/step - loss: 14.4486 - dense_2_loss: 2.7785 - dense_2_1_loss: 2.4972 - dense_2_2_loss: 2.4347 - dense_2_3_loss: 2.1735 - dense_2_4_loss: 1.7166 - dense_2_5_loss: 1.1574 - dense_2_6_loss: 0.7192 - dense_2_7_loss: 0.4306 - dense_2_8_loss: 0.2393 - dense_2_9_loss: 0.1358 - dense_2_10_loss: 0.0686 - dense_2_11_loss: 0.0379 - dense_2_12_loss: 0.0204 - dense_2_13_loss: 0.0122 - dense_2_14_loss: 0.0104 - dense_2_15_loss: 0.0047 - dense_2_16_loss: 0.0032 - dense_2_17_loss: 0.0033 - dense_2_18_loss: 0.0027 - dense_2_19_loss: 0.0023 - dense_2_accuracy: 0.2674 - dense_2_1_accuracy: 0.2954 - dense_2_2_accuracy: 0.2561 - dense_2_3_accuracy: 0.3425 - dense_2_4_accuracy: 0.4549 - dense_2_5_accuracy: 0.6227 - dense_2_6_accuracy: 0.7688 - dense_2_7_accuracy: 0.8613 - dense_2_8_accuracy: 0.9251 - dense_2_9_accuracy: 0.9621 - dense_2_10_accuracy: 0.9788 - dense_2_11_accuracy: 0.9907 - dense_2_12_accuracy: 0.9951 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9979 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9998 - dense_2_17_accuracy: 0.9996 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 34/100\n",
      "49/49 [==============================] - 6s 120ms/step - loss: 14.3675 - dense_2_loss: 2.7571 - dense_2_1_loss: 2.4783 - dense_2_2_loss: 2.4202 - dense_2_3_loss: 2.1663 - dense_2_4_loss: 1.7025 - dense_2_5_loss: 1.1550 - dense_2_6_loss: 0.7160 - dense_2_7_loss: 0.4282 - dense_2_8_loss: 0.2410 - dense_2_9_loss: 0.1367 - dense_2_10_loss: 0.0692 - dense_2_11_loss: 0.0381 - dense_2_12_loss: 0.0203 - dense_2_13_loss: 0.0120 - dense_2_14_loss: 0.0096 - dense_2_15_loss: 0.0049 - dense_2_16_loss: 0.0034 - dense_2_17_loss: 0.0035 - dense_2_18_loss: 0.0028 - dense_2_19_loss: 0.0023 - dense_2_accuracy: 0.2748 - dense_2_1_accuracy: 0.3012 - dense_2_2_accuracy: 0.2579 - dense_2_3_accuracy: 0.3394 - dense_2_4_accuracy: 0.4590 - dense_2_5_accuracy: 0.6237 - dense_2_6_accuracy: 0.7678 - dense_2_7_accuracy: 0.8643 - dense_2_8_accuracy: 0.9251 - dense_2_9_accuracy: 0.9607 - dense_2_10_accuracy: 0.9788 - dense_2_11_accuracy: 0.9914 - dense_2_12_accuracy: 0.9951 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9979 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9996 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 35/100\n",
      "49/49 [==============================] - 6s 120ms/step - loss: 14.2646 - dense_2_loss: 2.7328 - dense_2_1_loss: 2.4596 - dense_2_2_loss: 2.4052 - dense_2_3_loss: 2.1523 - dense_2_4_loss: 1.6945 - dense_2_5_loss: 1.1478 - dense_2_6_loss: 0.7138 - dense_2_7_loss: 0.4243 - dense_2_8_loss: 0.2370 - dense_2_9_loss: 0.1333 - dense_2_10_loss: 0.0670 - dense_2_11_loss: 0.0379 - dense_2_12_loss: 0.0205 - dense_2_13_loss: 0.0123 - dense_2_14_loss: 0.0097 - dense_2_15_loss: 0.0048 - dense_2_16_loss: 0.0034 - dense_2_17_loss: 0.0034 - dense_2_18_loss: 0.0027 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.2834 - dense_2_1_accuracy: 0.3063 - dense_2_2_accuracy: 0.2622 - dense_2_3_accuracy: 0.3413 - dense_2_4_accuracy: 0.4599 - dense_2_5_accuracy: 0.6243 - dense_2_6_accuracy: 0.7730 - dense_2_7_accuracy: 0.8646 - dense_2_8_accuracy: 0.9257 - dense_2_9_accuracy: 0.9615 - dense_2_10_accuracy: 0.9794 - dense_2_11_accuracy: 0.9901 - dense_2_12_accuracy: 0.9946 - dense_2_13_accuracy: 0.9965 - dense_2_14_accuracy: 0.9977 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9996 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998  \n",
      "Epoch 36/100\n",
      "49/49 [==============================] - 6s 123ms/step - loss: 14.1917 - dense_2_loss: 2.7087 - dense_2_1_loss: 2.4391 - dense_2_2_loss: 2.3955 - dense_2_3_loss: 2.1395 - dense_2_4_loss: 1.6881 - dense_2_5_loss: 1.1461 - dense_2_6_loss: 0.7106 - dense_2_7_loss: 0.4249 - dense_2_8_loss: 0.2391 - dense_2_9_loss: 0.1345 - dense_2_10_loss: 0.0687 - dense_2_11_loss: 0.0388 - dense_2_12_loss: 0.0207 - dense_2_13_loss: 0.0119 - dense_2_14_loss: 0.0097 - dense_2_15_loss: 0.0047 - dense_2_16_loss: 0.0031 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.2929 - dense_2_1_accuracy: 0.3117 - dense_2_2_accuracy: 0.2643 - dense_2_3_accuracy: 0.3450 - dense_2_4_accuracy: 0.4607 - dense_2_5_accuracy: 0.6217 - dense_2_6_accuracy: 0.7688 - dense_2_7_accuracy: 0.8623 - dense_2_8_accuracy: 0.9253 - dense_2_9_accuracy: 0.9601 - dense_2_10_accuracy: 0.9788 - dense_2_11_accuracy: 0.9905 - dense_2_12_accuracy: 0.9944 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9977 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998  \n",
      "Epoch 37/100\n",
      "49/49 [==============================] - 6s 122ms/step - loss: 14.1164 - dense_2_loss: 2.6851 - dense_2_1_loss: 2.4205 - dense_2_2_loss: 2.3824 - dense_2_3_loss: 2.1345 - dense_2_4_loss: 1.6842 - dense_2_5_loss: 1.1406 - dense_2_6_loss: 0.7106 - dense_2_7_loss: 0.4242 - dense_2_8_loss: 0.2350 - dense_2_9_loss: 0.1340 - dense_2_10_loss: 0.0676 - dense_2_11_loss: 0.0373 - dense_2_12_loss: 0.0207 - dense_2_13_loss: 0.0121 - dense_2_14_loss: 0.0098 - dense_2_15_loss: 0.0048 - dense_2_16_loss: 0.0038 - dense_2_17_loss: 0.0038 - dense_2_18_loss: 0.0029 - dense_2_19_loss: 0.0023 - dense_2_accuracy: 0.2966 - dense_2_1_accuracy: 0.3164 - dense_2_2_accuracy: 0.2664 - dense_2_3_accuracy: 0.3413 - dense_2_4_accuracy: 0.4619 - dense_2_5_accuracy: 0.6252 - dense_2_6_accuracy: 0.7653 - dense_2_7_accuracy: 0.8627 - dense_2_8_accuracy: 0.9259 - dense_2_9_accuracy: 0.9611 - dense_2_10_accuracy: 0.9800 - dense_2_11_accuracy: 0.9901 - dense_2_12_accuracy: 0.9944 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9981 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9994 - dense_2_17_accuracy: 0.9994 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 38/100\n",
      "49/49 [==============================] - 6s 124ms/step - loss: 14.0300 - dense_2_loss: 2.6629 - dense_2_1_loss: 2.4044 - dense_2_2_loss: 2.3677 - dense_2_3_loss: 2.1271 - dense_2_4_loss: 1.6780 - dense_2_5_loss: 1.1372 - dense_2_6_loss: 0.7008 - dense_2_7_loss: 0.4205 - dense_2_8_loss: 0.2345 - dense_2_9_loss: 0.1335 - dense_2_10_loss: 0.0671 - dense_2_11_loss: 0.0369 - dense_2_12_loss: 0.0210 - dense_2_13_loss: 0.0125 - dense_2_14_loss: 0.0100 - dense_2_15_loss: 0.0047 - dense_2_16_loss: 0.0033 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.3051 - dense_2_1_accuracy: 0.3215 - dense_2_2_accuracy: 0.2730 - dense_2_3_accuracy: 0.3427 - dense_2_4_accuracy: 0.4605 - dense_2_5_accuracy: 0.6237 - dense_2_6_accuracy: 0.7695 - dense_2_7_accuracy: 0.8617 - dense_2_8_accuracy: 0.9257 - dense_2_9_accuracy: 0.9599 - dense_2_10_accuracy: 0.9786 - dense_2_11_accuracy: 0.9897 - dense_2_12_accuracy: 0.9946 - dense_2_13_accuracy: 0.9969 - dense_2_14_accuracy: 0.9979 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9994 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 39/100\n",
      "49/49 [==============================] - 6s 124ms/step - loss: 13.9282 - dense_2_loss: 2.6366 - dense_2_1_loss: 2.3833 - dense_2_2_loss: 2.3576 - dense_2_3_loss: 2.1108 - dense_2_4_loss: 1.6694 - dense_2_5_loss: 1.1279 - dense_2_6_loss: 0.6994 - dense_2_7_loss: 0.4187 - dense_2_8_loss: 0.2331 - dense_2_9_loss: 0.1304 - dense_2_10_loss: 0.0671 - dense_2_11_loss: 0.0366 - dense_2_12_loss: 0.0200 - dense_2_13_loss: 0.0118 - dense_2_14_loss: 0.0098 - dense_2_15_loss: 0.0047 - dense_2_16_loss: 0.0032 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0025 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.3112 - dense_2_1_accuracy: 0.3252 - dense_2_2_accuracy: 0.2744 - dense_2_3_accuracy: 0.3442 - dense_2_4_accuracy: 0.4644 - dense_2_5_accuracy: 0.6266 - dense_2_6_accuracy: 0.7701 - dense_2_7_accuracy: 0.8621 - dense_2_8_accuracy: 0.9273 - dense_2_9_accuracy: 0.9611 - dense_2_10_accuracy: 0.9794 - dense_2_11_accuracy: 0.9905 - dense_2_12_accuracy: 0.9946 - dense_2_13_accuracy: 0.9967 - dense_2_14_accuracy: 0.9979 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 40/100\n",
      "49/49 [==============================] - 6s 124ms/step - loss: 13.8496 - dense_2_loss: 2.6133 - dense_2_1_loss: 2.3670 - dense_2_2_loss: 2.3445 - dense_2_3_loss: 2.1043 - dense_2_4_loss: 1.6640 - dense_2_5_loss: 1.1242 - dense_2_6_loss: 0.6950 - dense_2_7_loss: 0.4156 - dense_2_8_loss: 0.2314 - dense_2_9_loss: 0.1303 - dense_2_10_loss: 0.0667 - dense_2_11_loss: 0.0361 - dense_2_12_loss: 0.0200 - dense_2_13_loss: 0.0115 - dense_2_14_loss: 0.0094 - dense_2_15_loss: 0.0048 - dense_2_16_loss: 0.0035 - dense_2_17_loss: 0.0033 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.3156 - dense_2_1_accuracy: 0.3298 - dense_2_2_accuracy: 0.2767 - dense_2_3_accuracy: 0.3462 - dense_2_4_accuracy: 0.4623 - dense_2_5_accuracy: 0.6276 - dense_2_6_accuracy: 0.7701 - dense_2_7_accuracy: 0.8654 - dense_2_8_accuracy: 0.9267 - dense_2_9_accuracy: 0.9607 - dense_2_10_accuracy: 0.9800 - dense_2_11_accuracy: 0.9909 - dense_2_12_accuracy: 0.9951 - dense_2_13_accuracy: 0.9973 - dense_2_14_accuracy: 0.9981 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9994 - dense_2_17_accuracy: 0.9996 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 41/100\n",
      "49/49 [==============================] - 6s 128ms/step - loss: 13.7862 - dense_2_loss: 2.5874 - dense_2_1_loss: 2.3480 - dense_2_2_loss: 2.3357 - dense_2_3_loss: 2.0988 - dense_2_4_loss: 1.6605 - dense_2_5_loss: 1.1232 - dense_2_6_loss: 0.6942 - dense_2_7_loss: 0.4176 - dense_2_8_loss: 0.2292 - dense_2_9_loss: 0.1295 - dense_2_10_loss: 0.0671 - dense_2_11_loss: 0.0371 - dense_2_12_loss: 0.0204 - dense_2_13_loss: 0.0121 - dense_2_14_loss: 0.0095 - dense_2_15_loss: 0.0047 - dense_2_16_loss: 0.0033 - dense_2_17_loss: 0.0032 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.3242 - dense_2_1_accuracy: 0.3378 - dense_2_2_accuracy: 0.2816 - dense_2_3_accuracy: 0.3460 - dense_2_4_accuracy: 0.4636 - dense_2_5_accuracy: 0.6235 - dense_2_6_accuracy: 0.7727 - dense_2_7_accuracy: 0.8643 - dense_2_8_accuracy: 0.9267 - dense_2_9_accuracy: 0.9613 - dense_2_10_accuracy: 0.9798 - dense_2_11_accuracy: 0.9895 - dense_2_12_accuracy: 0.9946 - dense_2_13_accuracy: 0.9965 - dense_2_14_accuracy: 0.9977 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 42/100\n",
      "49/49 [==============================] - 6s 127ms/step - loss: 13.7135 - dense_2_loss: 2.5603 - dense_2_1_loss: 2.3317 - dense_2_2_loss: 2.3237 - dense_2_3_loss: 2.0899 - dense_2_4_loss: 1.6589 - dense_2_5_loss: 1.1144 - dense_2_6_loss: 0.6949 - dense_2_7_loss: 0.4151 - dense_2_8_loss: 0.2309 - dense_2_9_loss: 0.1305 - dense_2_10_loss: 0.0680 - dense_2_11_loss: 0.0366 - dense_2_12_loss: 0.0206 - dense_2_13_loss: 0.0126 - dense_2_14_loss: 0.0100 - dense_2_15_loss: 0.0045 - dense_2_16_loss: 0.0030 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0025 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.3322 - dense_2_1_accuracy: 0.3394 - dense_2_2_accuracy: 0.2841 - dense_2_3_accuracy: 0.3462 - dense_2_4_accuracy: 0.4638 - dense_2_5_accuracy: 0.6334 - dense_2_6_accuracy: 0.7705 - dense_2_7_accuracy: 0.8652 - dense_2_8_accuracy: 0.9280 - dense_2_9_accuracy: 0.9619 - dense_2_10_accuracy: 0.9802 - dense_2_11_accuracy: 0.9899 - dense_2_12_accuracy: 0.9942 - dense_2_13_accuracy: 0.9969 - dense_2_14_accuracy: 0.9979 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9998 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 43/100\n",
      "49/49 [==============================] - 6s 126ms/step - loss: 13.6217 - dense_2_loss: 2.5362 - dense_2_1_loss: 2.3138 - dense_2_2_loss: 2.3157 - dense_2_3_loss: 2.0795 - dense_2_4_loss: 1.6456 - dense_2_5_loss: 1.1128 - dense_2_6_loss: 0.6892 - dense_2_7_loss: 0.4137 - dense_2_8_loss: 0.2271 - dense_2_9_loss: 0.1277 - dense_2_10_loss: 0.0668 - dense_2_11_loss: 0.0365 - dense_2_12_loss: 0.0201 - dense_2_13_loss: 0.0118 - dense_2_14_loss: 0.0094 - dense_2_15_loss: 0.0046 - dense_2_16_loss: 0.0034 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.3384 - dense_2_1_accuracy: 0.3448 - dense_2_2_accuracy: 0.2834 - dense_2_3_accuracy: 0.3532 - dense_2_4_accuracy: 0.4662 - dense_2_5_accuracy: 0.6260 - dense_2_6_accuracy: 0.7717 - dense_2_7_accuracy: 0.8641 - dense_2_8_accuracy: 0.9280 - dense_2_9_accuracy: 0.9605 - dense_2_10_accuracy: 0.9798 - dense_2_11_accuracy: 0.9903 - dense_2_12_accuracy: 0.9949 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9979 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 44/100\n",
      "49/49 [==============================] - 6s 123ms/step - loss: 13.5369 - dense_2_loss: 2.5096 - dense_2_1_loss: 2.2969 - dense_2_2_loss: 2.3043 - dense_2_3_loss: 2.0720 - dense_2_4_loss: 1.6415 - dense_2_5_loss: 1.1056 - dense_2_6_loss: 0.6852 - dense_2_7_loss: 0.4109 - dense_2_8_loss: 0.2247 - dense_2_9_loss: 0.1275 - dense_2_10_loss: 0.0660 - dense_2_11_loss: 0.0362 - dense_2_12_loss: 0.0200 - dense_2_13_loss: 0.0116 - dense_2_14_loss: 0.0090 - dense_2_15_loss: 0.0046 - dense_2_16_loss: 0.0034 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.3479 - dense_2_1_accuracy: 0.3514 - dense_2_2_accuracy: 0.2872 - dense_2_3_accuracy: 0.3576 - dense_2_4_accuracy: 0.4687 - dense_2_5_accuracy: 0.6293 - dense_2_6_accuracy: 0.7736 - dense_2_7_accuracy: 0.8652 - dense_2_8_accuracy: 0.9298 - dense_2_9_accuracy: 0.9623 - dense_2_10_accuracy: 0.9802 - dense_2_11_accuracy: 0.9899 - dense_2_12_accuracy: 0.9944 - dense_2_13_accuracy: 0.9977 - dense_2_14_accuracy: 0.9979 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9992 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 45/100\n",
      "49/49 [==============================] - 6s 127ms/step - loss: 13.4683 - dense_2_loss: 2.4829 - dense_2_1_loss: 2.2791 - dense_2_2_loss: 2.2950 - dense_2_3_loss: 2.0637 - dense_2_4_loss: 1.6384 - dense_2_5_loss: 1.1037 - dense_2_6_loss: 0.6824 - dense_2_7_loss: 0.4108 - dense_2_8_loss: 0.2245 - dense_2_9_loss: 0.1276 - dense_2_10_loss: 0.0665 - dense_2_11_loss: 0.0368 - dense_2_12_loss: 0.0198 - dense_2_13_loss: 0.0123 - dense_2_14_loss: 0.0093 - dense_2_15_loss: 0.0046 - dense_2_16_loss: 0.0032 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.3545 - dense_2_1_accuracy: 0.3573 - dense_2_2_accuracy: 0.2874 - dense_2_3_accuracy: 0.3543 - dense_2_4_accuracy: 0.4706 - dense_2_5_accuracy: 0.6334 - dense_2_6_accuracy: 0.7727 - dense_2_7_accuracy: 0.8666 - dense_2_8_accuracy: 0.9294 - dense_2_9_accuracy: 0.9605 - dense_2_10_accuracy: 0.9800 - dense_2_11_accuracy: 0.9897 - dense_2_12_accuracy: 0.9949 - dense_2_13_accuracy: 0.9969 - dense_2_14_accuracy: 0.9981 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9998 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 46/100\n",
      "49/49 [==============================] - 6s 130ms/step - loss: 13.3865 - dense_2_loss: 2.4569 - dense_2_1_loss: 2.2630 - dense_2_2_loss: 2.2858 - dense_2_3_loss: 2.0543 - dense_2_4_loss: 1.6304 - dense_2_5_loss: 1.0972 - dense_2_6_loss: 0.6817 - dense_2_7_loss: 0.4099 - dense_2_8_loss: 0.2233 - dense_2_9_loss: 0.1273 - dense_2_10_loss: 0.0653 - dense_2_11_loss: 0.0351 - dense_2_12_loss: 0.0197 - dense_2_13_loss: 0.0116 - dense_2_14_loss: 0.0092 - dense_2_15_loss: 0.0047 - dense_2_16_loss: 0.0033 - dense_2_17_loss: 0.0030 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.3608 - dense_2_1_accuracy: 0.3600 - dense_2_2_accuracy: 0.2911 - dense_2_3_accuracy: 0.3534 - dense_2_4_accuracy: 0.4706 - dense_2_5_accuracy: 0.6307 - dense_2_6_accuracy: 0.7732 - dense_2_7_accuracy: 0.8662 - dense_2_8_accuracy: 0.9290 - dense_2_9_accuracy: 0.9621 - dense_2_10_accuracy: 0.9800 - dense_2_11_accuracy: 0.9905 - dense_2_12_accuracy: 0.9946 - dense_2_13_accuracy: 0.9973 - dense_2_14_accuracy: 0.9979 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9998 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 47/100\n",
      "49/49 [==============================] - 6s 127ms/step - loss: 13.3117 - dense_2_loss: 2.4291 - dense_2_1_loss: 2.2477 - dense_2_2_loss: 2.2763 - dense_2_3_loss: 2.0470 - dense_2_4_loss: 1.6259 - dense_2_5_loss: 1.0949 - dense_2_6_loss: 0.6772 - dense_2_7_loss: 0.4068 - dense_2_8_loss: 0.2226 - dense_2_9_loss: 0.1265 - dense_2_10_loss: 0.0651 - dense_2_11_loss: 0.0357 - dense_2_12_loss: 0.0204 - dense_2_13_loss: 0.0116 - dense_2_14_loss: 0.0090 - dense_2_15_loss: 0.0047 - dense_2_16_loss: 0.0035 - dense_2_17_loss: 0.0029 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.3732 - dense_2_1_accuracy: 0.3641 - dense_2_2_accuracy: 0.2921 - dense_2_3_accuracy: 0.3547 - dense_2_4_accuracy: 0.4710 - dense_2_5_accuracy: 0.6293 - dense_2_6_accuracy: 0.7740 - dense_2_7_accuracy: 0.8656 - dense_2_8_accuracy: 0.9292 - dense_2_9_accuracy: 0.9615 - dense_2_10_accuracy: 0.9804 - dense_2_11_accuracy: 0.9905 - dense_2_12_accuracy: 0.9949 - dense_2_13_accuracy: 0.9969 - dense_2_14_accuracy: 0.9979 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9994 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 48/100\n",
      "49/49 [==============================] - 6s 127ms/step - loss: 13.2346 - dense_2_loss: 2.4017 - dense_2_1_loss: 2.2304 - dense_2_2_loss: 2.2680 - dense_2_3_loss: 2.0383 - dense_2_4_loss: 1.6215 - dense_2_5_loss: 1.0884 - dense_2_6_loss: 0.6764 - dense_2_7_loss: 0.4056 - dense_2_8_loss: 0.2217 - dense_2_9_loss: 0.1264 - dense_2_10_loss: 0.0648 - dense_2_11_loss: 0.0358 - dense_2_12_loss: 0.0191 - dense_2_13_loss: 0.0116 - dense_2_14_loss: 0.0089 - dense_2_15_loss: 0.0047 - dense_2_16_loss: 0.0033 - dense_2_17_loss: 0.0030 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.3816 - dense_2_1_accuracy: 0.3672 - dense_2_2_accuracy: 0.2939 - dense_2_3_accuracy: 0.3576 - dense_2_4_accuracy: 0.4712 - dense_2_5_accuracy: 0.6367 - dense_2_6_accuracy: 0.7748 - dense_2_7_accuracy: 0.8654 - dense_2_8_accuracy: 0.9286 - dense_2_9_accuracy: 0.9623 - dense_2_10_accuracy: 0.9802 - dense_2_11_accuracy: 0.9897 - dense_2_12_accuracy: 0.9951 - dense_2_13_accuracy: 0.9969 - dense_2_14_accuracy: 0.9979 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9998 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 49/100\n",
      "49/49 [==============================] - 6s 127ms/step - loss: 13.1708 - dense_2_loss: 2.3750 - dense_2_1_loss: 2.2138 - dense_2_2_loss: 2.2560 - dense_2_3_loss: 2.0311 - dense_2_4_loss: 1.6176 - dense_2_5_loss: 1.0886 - dense_2_6_loss: 0.6765 - dense_2_7_loss: 0.4066 - dense_2_8_loss: 0.2224 - dense_2_9_loss: 0.1273 - dense_2_10_loss: 0.0649 - dense_2_11_loss: 0.0351 - dense_2_12_loss: 0.0194 - dense_2_13_loss: 0.0118 - dense_2_14_loss: 0.0089 - dense_2_15_loss: 0.0046 - dense_2_16_loss: 0.0031 - dense_2_17_loss: 0.0030 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.3886 - dense_2_1_accuracy: 0.3707 - dense_2_2_accuracy: 0.2972 - dense_2_3_accuracy: 0.3573 - dense_2_4_accuracy: 0.4697 - dense_2_5_accuracy: 0.6348 - dense_2_6_accuracy: 0.7732 - dense_2_7_accuracy: 0.8664 - dense_2_8_accuracy: 0.9298 - dense_2_9_accuracy: 0.9627 - dense_2_10_accuracy: 0.9798 - dense_2_11_accuracy: 0.9905 - dense_2_12_accuracy: 0.9946 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9984 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9998 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 50/100\n",
      "49/49 [==============================] - 6s 127ms/step - loss: 13.0887 - dense_2_loss: 2.3454 - dense_2_1_loss: 2.1992 - dense_2_2_loss: 2.2484 - dense_2_3_loss: 2.0223 - dense_2_4_loss: 1.6100 - dense_2_5_loss: 1.0813 - dense_2_6_loss: 0.6712 - dense_2_7_loss: 0.4045 - dense_2_8_loss: 0.2215 - dense_2_9_loss: 0.1266 - dense_2_10_loss: 0.0647 - dense_2_11_loss: 0.0359 - dense_2_12_loss: 0.0199 - dense_2_13_loss: 0.0118 - dense_2_14_loss: 0.0095 - dense_2_15_loss: 0.0048 - dense_2_16_loss: 0.0036 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.3969 - dense_2_1_accuracy: 0.3746 - dense_2_2_accuracy: 0.2977 - dense_2_3_accuracy: 0.3613 - dense_2_4_accuracy: 0.4714 - dense_2_5_accuracy: 0.6301 - dense_2_6_accuracy: 0.7795 - dense_2_7_accuracy: 0.8662 - dense_2_8_accuracy: 0.9290 - dense_2_9_accuracy: 0.9613 - dense_2_10_accuracy: 0.9823 - dense_2_11_accuracy: 0.9897 - dense_2_12_accuracy: 0.9951 - dense_2_13_accuracy: 0.9975 - dense_2_14_accuracy: 0.9979 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 51/100\n",
      "49/49 [==============================] - 6s 127ms/step - loss: 13.0059 - dense_2_loss: 2.3195 - dense_2_1_loss: 2.1834 - dense_2_2_loss: 2.2395 - dense_2_3_loss: 2.0141 - dense_2_4_loss: 1.6058 - dense_2_5_loss: 1.0748 - dense_2_6_loss: 0.6686 - dense_2_7_loss: 0.4008 - dense_2_8_loss: 0.2187 - dense_2_9_loss: 0.1245 - dense_2_10_loss: 0.0651 - dense_2_11_loss: 0.0357 - dense_2_12_loss: 0.0191 - dense_2_13_loss: 0.0114 - dense_2_14_loss: 0.0092 - dense_2_15_loss: 0.0046 - dense_2_16_loss: 0.0034 - dense_2_17_loss: 0.0030 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.4041 - dense_2_1_accuracy: 0.3781 - dense_2_2_accuracy: 0.3001 - dense_2_3_accuracy: 0.3590 - dense_2_4_accuracy: 0.4741 - dense_2_5_accuracy: 0.6332 - dense_2_6_accuracy: 0.7781 - dense_2_7_accuracy: 0.8643 - dense_2_8_accuracy: 0.9294 - dense_2_9_accuracy: 0.9632 - dense_2_10_accuracy: 0.9813 - dense_2_11_accuracy: 0.9899 - dense_2_12_accuracy: 0.9944 - dense_2_13_accuracy: 0.9975 - dense_2_14_accuracy: 0.9979 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9994 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 52/100\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 12.9233 - dense_2_loss: 2.2895 - dense_2_1_loss: 2.1677 - dense_2_2_loss: 2.2300 - dense_2_3_loss: 2.0066 - dense_2_4_loss: 1.5965 - dense_2_5_loss: 1.0712 - dense_2_6_loss: 0.6649 - dense_2_7_loss: 0.3987 - dense_2_8_loss: 0.2176 - dense_2_9_loss: 0.1255 - dense_2_10_loss: 0.0640 - dense_2_11_loss: 0.0358 - dense_2_12_loss: 0.0193 - dense_2_13_loss: 0.0116 - dense_2_14_loss: 0.0087 - dense_2_15_loss: 0.0047 - dense_2_16_loss: 0.0033 - dense_2_17_loss: 0.0030 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.4148 - dense_2_1_accuracy: 0.3825 - dense_2_2_accuracy: 0.3014 - dense_2_3_accuracy: 0.3643 - dense_2_4_accuracy: 0.4772 - dense_2_5_accuracy: 0.6361 - dense_2_6_accuracy: 0.7806 - dense_2_7_accuracy: 0.8685 - dense_2_8_accuracy: 0.9292 - dense_2_9_accuracy: 0.9629 - dense_2_10_accuracy: 0.9798 - dense_2_11_accuracy: 0.9899 - dense_2_12_accuracy: 0.9951 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9981 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9998 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 53/100\n",
      "49/49 [==============================] - 6s 129ms/step - loss: 12.8502 - dense_2_loss: 2.2624 - dense_2_1_loss: 2.1516 - dense_2_2_loss: 2.2191 - dense_2_3_loss: 1.9999 - dense_2_4_loss: 1.5923 - dense_2_5_loss: 1.0651 - dense_2_6_loss: 0.6635 - dense_2_7_loss: 0.3981 - dense_2_8_loss: 0.2172 - dense_2_9_loss: 0.1253 - dense_2_10_loss: 0.0646 - dense_2_11_loss: 0.0352 - dense_2_12_loss: 0.0196 - dense_2_13_loss: 0.0118 - dense_2_14_loss: 0.0084 - dense_2_15_loss: 0.0048 - dense_2_16_loss: 0.0035 - dense_2_17_loss: 0.0029 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.4249 - dense_2_1_accuracy: 0.3855 - dense_2_2_accuracy: 0.3077 - dense_2_3_accuracy: 0.3652 - dense_2_4_accuracy: 0.4772 - dense_2_5_accuracy: 0.6359 - dense_2_6_accuracy: 0.7806 - dense_2_7_accuracy: 0.8654 - dense_2_8_accuracy: 0.9304 - dense_2_9_accuracy: 0.9623 - dense_2_10_accuracy: 0.9802 - dense_2_11_accuracy: 0.9905 - dense_2_12_accuracy: 0.9946 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9977 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9994 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 54/100\n",
      "49/49 [==============================] - 6s 127ms/step - loss: 12.7796 - dense_2_loss: 2.2324 - dense_2_1_loss: 2.1377 - dense_2_2_loss: 2.2106 - dense_2_3_loss: 1.9899 - dense_2_4_loss: 1.5883 - dense_2_5_loss: 1.0609 - dense_2_6_loss: 0.6643 - dense_2_7_loss: 0.3987 - dense_2_8_loss: 0.2168 - dense_2_9_loss: 0.1251 - dense_2_10_loss: 0.0634 - dense_2_11_loss: 0.0358 - dense_2_12_loss: 0.0192 - dense_2_13_loss: 0.0120 - dense_2_14_loss: 0.0087 - dense_2_15_loss: 0.0047 - dense_2_16_loss: 0.0034 - dense_2_17_loss: 0.0029 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.4339 - dense_2_1_accuracy: 0.3886 - dense_2_2_accuracy: 0.3092 - dense_2_3_accuracy: 0.3672 - dense_2_4_accuracy: 0.4765 - dense_2_5_accuracy: 0.6398 - dense_2_6_accuracy: 0.7783 - dense_2_7_accuracy: 0.8697 - dense_2_8_accuracy: 0.9296 - dense_2_9_accuracy: 0.9615 - dense_2_10_accuracy: 0.9807 - dense_2_11_accuracy: 0.9897 - dense_2_12_accuracy: 0.9949 - dense_2_13_accuracy: 0.9969 - dense_2_14_accuracy: 0.9981 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9994 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 55/100\n",
      "49/49 [==============================] - 7s 142ms/step - loss: 12.7144 - dense_2_loss: 2.2038 - dense_2_1_loss: 2.1214 - dense_2_2_loss: 2.2027 - dense_2_3_loss: 1.9804 - dense_2_4_loss: 1.5842 - dense_2_5_loss: 1.0633 - dense_2_6_loss: 0.6633 - dense_2_7_loss: 0.3982 - dense_2_8_loss: 0.2169 - dense_2_9_loss: 0.1249 - dense_2_10_loss: 0.0644 - dense_2_11_loss: 0.0354 - dense_2_12_loss: 0.0191 - dense_2_13_loss: 0.0117 - dense_2_14_loss: 0.0081 - dense_2_15_loss: 0.0048 - dense_2_16_loss: 0.0037 - dense_2_17_loss: 0.0032 - dense_2_18_loss: 0.0027 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.4455 - dense_2_1_accuracy: 0.3911 - dense_2_2_accuracy: 0.3077 - dense_2_3_accuracy: 0.3689 - dense_2_4_accuracy: 0.4813 - dense_2_5_accuracy: 0.6392 - dense_2_6_accuracy: 0.7812 - dense_2_7_accuracy: 0.8668 - dense_2_8_accuracy: 0.9300 - dense_2_9_accuracy: 0.9611 - dense_2_10_accuracy: 0.9811 - dense_2_11_accuracy: 0.9897 - dense_2_12_accuracy: 0.9946 - dense_2_13_accuracy: 0.9969 - dense_2_14_accuracy: 0.9986 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 56/100\n",
      "49/49 [==============================] - 6s 129ms/step - loss: 12.6178 - dense_2_loss: 2.1745 - dense_2_1_loss: 2.1086 - dense_2_2_loss: 2.1904 - dense_2_3_loss: 1.9729 - dense_2_4_loss: 1.5767 - dense_2_5_loss: 1.0529 - dense_2_6_loss: 0.6571 - dense_2_7_loss: 0.3923 - dense_2_8_loss: 0.2155 - dense_2_9_loss: 0.1254 - dense_2_10_loss: 0.0634 - dense_2_11_loss: 0.0344 - dense_2_12_loss: 0.0185 - dense_2_13_loss: 0.0114 - dense_2_14_loss: 0.0083 - dense_2_15_loss: 0.0047 - dense_2_16_loss: 0.0032 - dense_2_17_loss: 0.0029 - dense_2_18_loss: 0.0025 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.4551 - dense_2_1_accuracy: 0.3975 - dense_2_2_accuracy: 0.3108 - dense_2_3_accuracy: 0.3707 - dense_2_4_accuracy: 0.4831 - dense_2_5_accuracy: 0.6383 - dense_2_6_accuracy: 0.7795 - dense_2_7_accuracy: 0.8658 - dense_2_8_accuracy: 0.9288 - dense_2_9_accuracy: 0.9627 - dense_2_10_accuracy: 0.9800 - dense_2_11_accuracy: 0.9909 - dense_2_12_accuracy: 0.9951 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9984 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 57/100\n",
      "49/49 [==============================] - 6s 128ms/step - loss: 12.5437 - dense_2_loss: 2.1452 - dense_2_1_loss: 2.0920 - dense_2_2_loss: 2.1815 - dense_2_3_loss: 1.9613 - dense_2_4_loss: 1.5727 - dense_2_5_loss: 1.0506 - dense_2_6_loss: 0.6563 - dense_2_7_loss: 0.3930 - dense_2_8_loss: 0.2138 - dense_2_9_loss: 0.1241 - dense_2_10_loss: 0.0632 - dense_2_11_loss: 0.0354 - dense_2_12_loss: 0.0187 - dense_2_13_loss: 0.0113 - dense_2_14_loss: 0.0086 - dense_2_15_loss: 0.0049 - dense_2_16_loss: 0.0033 - dense_2_17_loss: 0.0029 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.4617 - dense_2_1_accuracy: 0.4028 - dense_2_2_accuracy: 0.3129 - dense_2_3_accuracy: 0.3720 - dense_2_4_accuracy: 0.4825 - dense_2_5_accuracy: 0.6402 - dense_2_6_accuracy: 0.7814 - dense_2_7_accuracy: 0.8678 - dense_2_8_accuracy: 0.9296 - dense_2_9_accuracy: 0.9625 - dense_2_10_accuracy: 0.9813 - dense_2_11_accuracy: 0.9903 - dense_2_12_accuracy: 0.9949 - dense_2_13_accuracy: 0.9973 - dense_2_14_accuracy: 0.9981 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 58/100\n",
      "49/49 [==============================] - 6s 128ms/step - loss: 12.4730 - dense_2_loss: 2.1158 - dense_2_1_loss: 2.0765 - dense_2_2_loss: 2.1714 - dense_2_3_loss: 1.9535 - dense_2_4_loss: 1.5643 - dense_2_5_loss: 1.0462 - dense_2_6_loss: 0.6578 - dense_2_7_loss: 0.3943 - dense_2_8_loss: 0.2148 - dense_2_9_loss: 0.1247 - dense_2_10_loss: 0.0642 - dense_2_11_loss: 0.0350 - dense_2_12_loss: 0.0181 - dense_2_13_loss: 0.0113 - dense_2_14_loss: 0.0088 - dense_2_15_loss: 0.0049 - dense_2_16_loss: 0.0034 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.4712 - dense_2_1_accuracy: 0.4063 - dense_2_2_accuracy: 0.3176 - dense_2_3_accuracy: 0.3753 - dense_2_4_accuracy: 0.4895 - dense_2_5_accuracy: 0.6404 - dense_2_6_accuracy: 0.7800 - dense_2_7_accuracy: 0.8674 - dense_2_8_accuracy: 0.9298 - dense_2_9_accuracy: 0.9613 - dense_2_10_accuracy: 0.9807 - dense_2_11_accuracy: 0.9905 - dense_2_12_accuracy: 0.9953 - dense_2_13_accuracy: 0.9975 - dense_2_14_accuracy: 0.9979 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9998 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 59/100\n",
      "49/49 [==============================] - 6s 128ms/step - loss: 12.3768 - dense_2_loss: 2.0867 - dense_2_1_loss: 2.0619 - dense_2_2_loss: 2.1616 - dense_2_3_loss: 1.9431 - dense_2_4_loss: 1.5581 - dense_2_5_loss: 1.0383 - dense_2_6_loss: 0.6490 - dense_2_7_loss: 0.3897 - dense_2_8_loss: 0.2127 - dense_2_9_loss: 0.1234 - dense_2_10_loss: 0.0636 - dense_2_11_loss: 0.0349 - dense_2_12_loss: 0.0184 - dense_2_13_loss: 0.0112 - dense_2_14_loss: 0.0080 - dense_2_15_loss: 0.0048 - dense_2_16_loss: 0.0034 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.4784 - dense_2_1_accuracy: 0.4082 - dense_2_2_accuracy: 0.3172 - dense_2_3_accuracy: 0.3790 - dense_2_4_accuracy: 0.4895 - dense_2_5_accuracy: 0.6445 - dense_2_6_accuracy: 0.7808 - dense_2_7_accuracy: 0.8689 - dense_2_8_accuracy: 0.9310 - dense_2_9_accuracy: 0.9638 - dense_2_10_accuracy: 0.9804 - dense_2_11_accuracy: 0.9901 - dense_2_12_accuracy: 0.9949 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9986 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9998 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 60/100\n",
      "49/49 [==============================] - 6s 128ms/step - loss: 12.3101 - dense_2_loss: 2.0548 - dense_2_1_loss: 2.0489 - dense_2_2_loss: 2.1502 - dense_2_3_loss: 1.9364 - dense_2_4_loss: 1.5536 - dense_2_5_loss: 1.0380 - dense_2_6_loss: 0.6508 - dense_2_7_loss: 0.3915 - dense_2_8_loss: 0.2114 - dense_2_9_loss: 0.1229 - dense_2_10_loss: 0.0635 - dense_2_11_loss: 0.0344 - dense_2_12_loss: 0.0182 - dense_2_13_loss: 0.0113 - dense_2_14_loss: 0.0084 - dense_2_15_loss: 0.0049 - dense_2_16_loss: 0.0032 - dense_2_17_loss: 0.0030 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.4895 - dense_2_1_accuracy: 0.4119 - dense_2_2_accuracy: 0.3197 - dense_2_3_accuracy: 0.3757 - dense_2_4_accuracy: 0.4916 - dense_2_5_accuracy: 0.6466 - dense_2_6_accuracy: 0.7861 - dense_2_7_accuracy: 0.8672 - dense_2_8_accuracy: 0.9304 - dense_2_9_accuracy: 0.9636 - dense_2_10_accuracy: 0.9811 - dense_2_11_accuracy: 0.9905 - dense_2_12_accuracy: 0.9955 - dense_2_13_accuracy: 0.9969 - dense_2_14_accuracy: 0.9986 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 61/100\n",
      "49/49 [==============================] - 6s 128ms/step - loss: 12.2273 - dense_2_loss: 2.0273 - dense_2_1_loss: 2.0346 - dense_2_2_loss: 2.1410 - dense_2_3_loss: 1.9260 - dense_2_4_loss: 1.5460 - dense_2_5_loss: 1.0301 - dense_2_6_loss: 0.6467 - dense_2_7_loss: 0.3892 - dense_2_8_loss: 0.2119 - dense_2_9_loss: 0.1241 - dense_2_10_loss: 0.0627 - dense_2_11_loss: 0.0344 - dense_2_12_loss: 0.0180 - dense_2_13_loss: 0.0110 - dense_2_14_loss: 0.0081 - dense_2_15_loss: 0.0050 - dense_2_16_loss: 0.0035 - dense_2_17_loss: 0.0030 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.4984 - dense_2_1_accuracy: 0.4189 - dense_2_2_accuracy: 0.3242 - dense_2_3_accuracy: 0.3794 - dense_2_4_accuracy: 0.4918 - dense_2_5_accuracy: 0.6443 - dense_2_6_accuracy: 0.7814 - dense_2_7_accuracy: 0.8699 - dense_2_8_accuracy: 0.9300 - dense_2_9_accuracy: 0.9642 - dense_2_10_accuracy: 0.9815 - dense_2_11_accuracy: 0.9901 - dense_2_12_accuracy: 0.9951 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9984 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9994 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 62/100\n",
      "49/49 [==============================] - 6s 127ms/step - loss: 12.1502 - dense_2_loss: 1.9972 - dense_2_1_loss: 2.0193 - dense_2_2_loss: 2.1297 - dense_2_3_loss: 1.9174 - dense_2_4_loss: 1.5409 - dense_2_5_loss: 1.0298 - dense_2_6_loss: 0.6446 - dense_2_7_loss: 0.3872 - dense_2_8_loss: 0.2100 - dense_2_9_loss: 0.1225 - dense_2_10_loss: 0.0623 - dense_2_11_loss: 0.0341 - dense_2_12_loss: 0.0188 - dense_2_13_loss: 0.0118 - dense_2_14_loss: 0.0084 - dense_2_15_loss: 0.0051 - dense_2_16_loss: 0.0034 - dense_2_17_loss: 0.0030 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0021 - dense_2_accuracy: 0.5107 - dense_2_1_accuracy: 0.4203 - dense_2_2_accuracy: 0.3244 - dense_2_3_accuracy: 0.3800 - dense_2_4_accuracy: 0.4936 - dense_2_5_accuracy: 0.6410 - dense_2_6_accuracy: 0.7802 - dense_2_7_accuracy: 0.8672 - dense_2_8_accuracy: 0.9280 - dense_2_9_accuracy: 0.9646 - dense_2_10_accuracy: 0.9809 - dense_2_11_accuracy: 0.9901 - dense_2_12_accuracy: 0.9949 - dense_2_13_accuracy: 0.9967 - dense_2_14_accuracy: 0.9984 - dense_2_15_accuracy: 0.9990 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 63/100\n",
      "49/49 [==============================] - 6s 127ms/step - loss: 12.0718 - dense_2_loss: 1.9658 - dense_2_1_loss: 2.0045 - dense_2_2_loss: 2.1178 - dense_2_3_loss: 1.9086 - dense_2_4_loss: 1.5357 - dense_2_5_loss: 1.0239 - dense_2_6_loss: 0.6450 - dense_2_7_loss: 0.3876 - dense_2_8_loss: 0.2094 - dense_2_9_loss: 0.1222 - dense_2_10_loss: 0.0632 - dense_2_11_loss: 0.0352 - dense_2_12_loss: 0.0184 - dense_2_13_loss: 0.0106 - dense_2_14_loss: 0.0076 - dense_2_15_loss: 0.0050 - dense_2_16_loss: 0.0036 - dense_2_17_loss: 0.0032 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.5206 - dense_2_1_accuracy: 0.4242 - dense_2_2_accuracy: 0.3298 - dense_2_3_accuracy: 0.3858 - dense_2_4_accuracy: 0.4953 - dense_2_5_accuracy: 0.6462 - dense_2_6_accuracy: 0.7843 - dense_2_7_accuracy: 0.8672 - dense_2_8_accuracy: 0.9288 - dense_2_9_accuracy: 0.9619 - dense_2_10_accuracy: 0.9811 - dense_2_11_accuracy: 0.9903 - dense_2_12_accuracy: 0.9949 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9984 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9998 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 64/100\n",
      "49/49 [==============================] - 6s 128ms/step - loss: 12.0145 - dense_2_loss: 1.9367 - dense_2_1_loss: 1.9906 - dense_2_2_loss: 2.1080 - dense_2_3_loss: 1.8982 - dense_2_4_loss: 1.5343 - dense_2_5_loss: 1.0249 - dense_2_6_loss: 0.6440 - dense_2_7_loss: 0.3876 - dense_2_8_loss: 0.2109 - dense_2_9_loss: 0.1256 - dense_2_10_loss: 0.0633 - dense_2_11_loss: 0.0364 - dense_2_12_loss: 0.0187 - dense_2_13_loss: 0.0112 - dense_2_14_loss: 0.0078 - dense_2_15_loss: 0.0050 - dense_2_16_loss: 0.0034 - dense_2_17_loss: 0.0030 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.5309 - dense_2_1_accuracy: 0.4282 - dense_2_2_accuracy: 0.3306 - dense_2_3_accuracy: 0.3870 - dense_2_4_accuracy: 0.4944 - dense_2_5_accuracy: 0.6445 - dense_2_6_accuracy: 0.7824 - dense_2_7_accuracy: 0.8674 - dense_2_8_accuracy: 0.9292 - dense_2_9_accuracy: 0.9613 - dense_2_10_accuracy: 0.9811 - dense_2_11_accuracy: 0.9901 - dense_2_12_accuracy: 0.9949 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9986 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 65/100\n",
      "49/49 [==============================] - 6s 128ms/step - loss: 11.9350 - dense_2_loss: 1.9097 - dense_2_1_loss: 1.9769 - dense_2_2_loss: 2.0950 - dense_2_3_loss: 1.8881 - dense_2_4_loss: 1.5259 - dense_2_5_loss: 1.0207 - dense_2_6_loss: 0.6446 - dense_2_7_loss: 0.3860 - dense_2_8_loss: 0.2107 - dense_2_9_loss: 0.1240 - dense_2_10_loss: 0.0637 - dense_2_11_loss: 0.0343 - dense_2_12_loss: 0.0184 - dense_2_13_loss: 0.0118 - dense_2_14_loss: 0.0083 - dense_2_15_loss: 0.0052 - dense_2_16_loss: 0.0038 - dense_2_17_loss: 0.0032 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.5412 - dense_2_1_accuracy: 0.4308 - dense_2_2_accuracy: 0.3368 - dense_2_3_accuracy: 0.3890 - dense_2_4_accuracy: 0.4963 - dense_2_5_accuracy: 0.6459 - dense_2_6_accuracy: 0.7791 - dense_2_7_accuracy: 0.8681 - dense_2_8_accuracy: 0.9290 - dense_2_9_accuracy: 0.9636 - dense_2_10_accuracy: 0.9811 - dense_2_11_accuracy: 0.9901 - dense_2_12_accuracy: 0.9955 - dense_2_13_accuracy: 0.9967 - dense_2_14_accuracy: 0.9986 - dense_2_15_accuracy: 0.9990 - dense_2_16_accuracy: 0.9994 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 66/100\n",
      "49/49 [==============================] - 6s 128ms/step - loss: 11.8238 - dense_2_loss: 1.8789 - dense_2_1_loss: 1.9617 - dense_2_2_loss: 2.0850 - dense_2_3_loss: 1.8760 - dense_2_4_loss: 1.5160 - dense_2_5_loss: 1.0107 - dense_2_6_loss: 0.6360 - dense_2_7_loss: 0.3806 - dense_2_8_loss: 0.2068 - dense_2_9_loss: 0.1220 - dense_2_10_loss: 0.0623 - dense_2_11_loss: 0.0345 - dense_2_12_loss: 0.0181 - dense_2_13_loss: 0.0108 - dense_2_14_loss: 0.0079 - dense_2_15_loss: 0.0050 - dense_2_16_loss: 0.0035 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.5539 - dense_2_1_accuracy: 0.4339 - dense_2_2_accuracy: 0.3409 - dense_2_3_accuracy: 0.3909 - dense_2_4_accuracy: 0.4988 - dense_2_5_accuracy: 0.6507 - dense_2_6_accuracy: 0.7835 - dense_2_7_accuracy: 0.8689 - dense_2_8_accuracy: 0.9296 - dense_2_9_accuracy: 0.9627 - dense_2_10_accuracy: 0.9802 - dense_2_11_accuracy: 0.9903 - dense_2_12_accuracy: 0.9949 - dense_2_13_accuracy: 0.9973 - dense_2_14_accuracy: 0.9984 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9998 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 67/100\n",
      "49/49 [==============================] - 6s 129ms/step - loss: 11.7531 - dense_2_loss: 1.8487 - dense_2_1_loss: 1.9479 - dense_2_2_loss: 2.0747 - dense_2_3_loss: 1.8661 - dense_2_4_loss: 1.5120 - dense_2_5_loss: 1.0077 - dense_2_6_loss: 0.6361 - dense_2_7_loss: 0.3812 - dense_2_8_loss: 0.2062 - dense_2_9_loss: 0.1220 - dense_2_10_loss: 0.0624 - dense_2_11_loss: 0.0348 - dense_2_12_loss: 0.0183 - dense_2_13_loss: 0.0109 - dense_2_14_loss: 0.0079 - dense_2_15_loss: 0.0050 - dense_2_16_loss: 0.0035 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.5648 - dense_2_1_accuracy: 0.4356 - dense_2_2_accuracy: 0.3450 - dense_2_3_accuracy: 0.3946 - dense_2_4_accuracy: 0.5000 - dense_2_5_accuracy: 0.6484 - dense_2_6_accuracy: 0.7818 - dense_2_7_accuracy: 0.8685 - dense_2_8_accuracy: 0.9321 - dense_2_9_accuracy: 0.9627 - dense_2_10_accuracy: 0.9807 - dense_2_11_accuracy: 0.9901 - dense_2_12_accuracy: 0.9949 - dense_2_13_accuracy: 0.9973 - dense_2_14_accuracy: 0.9984 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 68/100\n",
      "49/49 [==============================] - 6s 129ms/step - loss: 11.6715 - dense_2_loss: 1.8197 - dense_2_1_loss: 1.9314 - dense_2_2_loss: 2.0626 - dense_2_3_loss: 1.8576 - dense_2_4_loss: 1.5055 - dense_2_5_loss: 1.0045 - dense_2_6_loss: 0.6334 - dense_2_7_loss: 0.3805 - dense_2_8_loss: 0.2051 - dense_2_9_loss: 0.1213 - dense_2_10_loss: 0.0623 - dense_2_11_loss: 0.0343 - dense_2_12_loss: 0.0182 - dense_2_13_loss: 0.0109 - dense_2_14_loss: 0.0079 - dense_2_15_loss: 0.0051 - dense_2_16_loss: 0.0036 - dense_2_17_loss: 0.0030 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.5788 - dense_2_1_accuracy: 0.4415 - dense_2_2_accuracy: 0.3446 - dense_2_3_accuracy: 0.3963 - dense_2_4_accuracy: 0.5043 - dense_2_5_accuracy: 0.6499 - dense_2_6_accuracy: 0.7853 - dense_2_7_accuracy: 0.8693 - dense_2_8_accuracy: 0.9315 - dense_2_9_accuracy: 0.9625 - dense_2_10_accuracy: 0.9811 - dense_2_11_accuracy: 0.9901 - dense_2_12_accuracy: 0.9953 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9984 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9994 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 69/100\n",
      "49/49 [==============================] - 6s 130ms/step - loss: 11.5982 - dense_2_loss: 1.7916 - dense_2_1_loss: 1.9179 - dense_2_2_loss: 2.0494 - dense_2_3_loss: 1.8477 - dense_2_4_loss: 1.4997 - dense_2_5_loss: 1.0024 - dense_2_6_loss: 0.6330 - dense_2_7_loss: 0.3799 - dense_2_8_loss: 0.2051 - dense_2_9_loss: 0.1209 - dense_2_10_loss: 0.0623 - dense_2_11_loss: 0.0353 - dense_2_12_loss: 0.0182 - dense_2_13_loss: 0.0107 - dense_2_14_loss: 0.0079 - dense_2_15_loss: 0.0049 - dense_2_16_loss: 0.0034 - dense_2_17_loss: 0.0030 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.5887 - dense_2_1_accuracy: 0.4455 - dense_2_2_accuracy: 0.3541 - dense_2_3_accuracy: 0.4028 - dense_2_4_accuracy: 0.5047 - dense_2_5_accuracy: 0.6511 - dense_2_6_accuracy: 0.7857 - dense_2_7_accuracy: 0.8697 - dense_2_8_accuracy: 0.9319 - dense_2_9_accuracy: 0.9642 - dense_2_10_accuracy: 0.9813 - dense_2_11_accuracy: 0.9901 - dense_2_12_accuracy: 0.9951 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9986 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 70/100\n",
      "49/49 [==============================] - 6s 131ms/step - loss: 11.5254 - dense_2_loss: 1.7627 - dense_2_1_loss: 1.9021 - dense_2_2_loss: 2.0373 - dense_2_3_loss: 1.8381 - dense_2_4_loss: 1.4950 - dense_2_5_loss: 0.9993 - dense_2_6_loss: 0.6322 - dense_2_7_loss: 0.3801 - dense_2_8_loss: 0.2056 - dense_2_9_loss: 0.1220 - dense_2_10_loss: 0.0618 - dense_2_11_loss: 0.0349 - dense_2_12_loss: 0.0188 - dense_2_13_loss: 0.0113 - dense_2_14_loss: 0.0079 - dense_2_15_loss: 0.0051 - dense_2_16_loss: 0.0036 - dense_2_17_loss: 0.0030 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.6002 - dense_2_1_accuracy: 0.4504 - dense_2_2_accuracy: 0.3571 - dense_2_3_accuracy: 0.4043 - dense_2_4_accuracy: 0.5078 - dense_2_5_accuracy: 0.6529 - dense_2_6_accuracy: 0.7820 - dense_2_7_accuracy: 0.8668 - dense_2_8_accuracy: 0.9315 - dense_2_9_accuracy: 0.9623 - dense_2_10_accuracy: 0.9807 - dense_2_11_accuracy: 0.9903 - dense_2_12_accuracy: 0.9949 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9984 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 71/100\n",
      "49/49 [==============================] - 6s 129ms/step - loss: 11.4404 - dense_2_loss: 1.7350 - dense_2_1_loss: 1.8860 - dense_2_2_loss: 2.0249 - dense_2_3_loss: 1.8271 - dense_2_4_loss: 1.4899 - dense_2_5_loss: 0.9947 - dense_2_6_loss: 0.6313 - dense_2_7_loss: 0.3779 - dense_2_8_loss: 0.2026 - dense_2_9_loss: 0.1204 - dense_2_10_loss: 0.0624 - dense_2_11_loss: 0.0352 - dense_2_12_loss: 0.0179 - dense_2_13_loss: 0.0109 - dense_2_14_loss: 0.0080 - dense_2_15_loss: 0.0051 - dense_2_16_loss: 0.0034 - dense_2_17_loss: 0.0030 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.6107 - dense_2_1_accuracy: 0.4549 - dense_2_2_accuracy: 0.3602 - dense_2_3_accuracy: 0.4084 - dense_2_4_accuracy: 0.5086 - dense_2_5_accuracy: 0.6519 - dense_2_6_accuracy: 0.7830 - dense_2_7_accuracy: 0.8691 - dense_2_8_accuracy: 0.9302 - dense_2_9_accuracy: 0.9625 - dense_2_10_accuracy: 0.9821 - dense_2_11_accuracy: 0.9899 - dense_2_12_accuracy: 0.9949 - dense_2_13_accuracy: 0.9973 - dense_2_14_accuracy: 0.9986 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 72/100\n",
      "49/49 [==============================] - 6s 130ms/step - loss: 11.3653 - dense_2_loss: 1.7075 - dense_2_1_loss: 1.8694 - dense_2_2_loss: 2.0125 - dense_2_3_loss: 1.8182 - dense_2_4_loss: 1.4858 - dense_2_5_loss: 0.9915 - dense_2_6_loss: 0.6299 - dense_2_7_loss: 0.3772 - dense_2_8_loss: 0.2026 - dense_2_9_loss: 0.1192 - dense_2_10_loss: 0.0620 - dense_2_11_loss: 0.0346 - dense_2_12_loss: 0.0188 - dense_2_13_loss: 0.0113 - dense_2_14_loss: 0.0083 - dense_2_15_loss: 0.0051 - dense_2_16_loss: 0.0035 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.6243 - dense_2_1_accuracy: 0.4609 - dense_2_2_accuracy: 0.3660 - dense_2_3_accuracy: 0.4133 - dense_2_4_accuracy: 0.5101 - dense_2_5_accuracy: 0.6527 - dense_2_6_accuracy: 0.7843 - dense_2_7_accuracy: 0.8697 - dense_2_8_accuracy: 0.9308 - dense_2_9_accuracy: 0.9621 - dense_2_10_accuracy: 0.9809 - dense_2_11_accuracy: 0.9903 - dense_2_12_accuracy: 0.9949 - dense_2_13_accuracy: 0.9969 - dense_2_14_accuracy: 0.9984 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 73/100\n",
      "49/49 [==============================] - 6s 131ms/step - loss: 11.2898 - dense_2_loss: 1.6804 - dense_2_1_loss: 1.8539 - dense_2_2_loss: 1.9987 - dense_2_3_loss: 1.8077 - dense_2_4_loss: 1.4794 - dense_2_5_loss: 0.9887 - dense_2_6_loss: 0.6288 - dense_2_7_loss: 0.3773 - dense_2_8_loss: 0.2048 - dense_2_9_loss: 0.1195 - dense_2_10_loss: 0.0623 - dense_2_11_loss: 0.0351 - dense_2_12_loss: 0.0184 - dense_2_13_loss: 0.0108 - dense_2_14_loss: 0.0075 - dense_2_15_loss: 0.0049 - dense_2_16_loss: 0.0037 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.6359 - dense_2_1_accuracy: 0.4662 - dense_2_2_accuracy: 0.3722 - dense_2_3_accuracy: 0.4160 - dense_2_4_accuracy: 0.5074 - dense_2_5_accuracy: 0.6550 - dense_2_6_accuracy: 0.7828 - dense_2_7_accuracy: 0.8691 - dense_2_8_accuracy: 0.9312 - dense_2_9_accuracy: 0.9632 - dense_2_10_accuracy: 0.9815 - dense_2_11_accuracy: 0.9899 - dense_2_12_accuracy: 0.9946 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9986 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9994 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 74/100\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 11.1908 - dense_2_loss: 1.6517 - dense_2_1_loss: 1.8377 - dense_2_2_loss: 1.9843 - dense_2_3_loss: 1.7971 - dense_2_4_loss: 1.4713 - dense_2_5_loss: 0.9807 - dense_2_6_loss: 0.6244 - dense_2_7_loss: 0.3749 - dense_2_8_loss: 0.2007 - dense_2_9_loss: 0.1189 - dense_2_10_loss: 0.0616 - dense_2_11_loss: 0.0344 - dense_2_12_loss: 0.0181 - dense_2_13_loss: 0.0109 - dense_2_14_loss: 0.0076 - dense_2_15_loss: 0.0050 - dense_2_16_loss: 0.0034 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.6521 - dense_2_1_accuracy: 0.4728 - dense_2_2_accuracy: 0.3777 - dense_2_3_accuracy: 0.4197 - dense_2_4_accuracy: 0.5126 - dense_2_5_accuracy: 0.6591 - dense_2_6_accuracy: 0.7830 - dense_2_7_accuracy: 0.8683 - dense_2_8_accuracy: 0.9315 - dense_2_9_accuracy: 0.9621 - dense_2_10_accuracy: 0.9815 - dense_2_11_accuracy: 0.9903 - dense_2_12_accuracy: 0.9953 - dense_2_13_accuracy: 0.9969 - dense_2_14_accuracy: 0.9986 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 75/100\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 11.1178 - dense_2_loss: 1.6251 - dense_2_1_loss: 1.8210 - dense_2_2_loss: 1.9737 - dense_2_3_loss: 1.7880 - dense_2_4_loss: 1.4647 - dense_2_5_loss: 0.9773 - dense_2_6_loss: 0.6226 - dense_2_7_loss: 0.3740 - dense_2_8_loss: 0.2005 - dense_2_9_loss: 0.1204 - dense_2_10_loss: 0.0624 - dense_2_11_loss: 0.0345 - dense_2_12_loss: 0.0183 - dense_2_13_loss: 0.0110 - dense_2_14_loss: 0.0079 - dense_2_15_loss: 0.0050 - dense_2_16_loss: 0.0036 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.6618 - dense_2_1_accuracy: 0.4786 - dense_2_2_accuracy: 0.3808 - dense_2_3_accuracy: 0.4290 - dense_2_4_accuracy: 0.5148 - dense_2_5_accuracy: 0.6573 - dense_2_6_accuracy: 0.7835 - dense_2_7_accuracy: 0.8685 - dense_2_8_accuracy: 0.9325 - dense_2_9_accuracy: 0.9609 - dense_2_10_accuracy: 0.9815 - dense_2_11_accuracy: 0.9903 - dense_2_12_accuracy: 0.9953 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9986 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 76/100\n",
      "49/49 [==============================] - 6s 131ms/step - loss: 11.0511 - dense_2_loss: 1.6000 - dense_2_1_loss: 1.8041 - dense_2_2_loss: 1.9584 - dense_2_3_loss: 1.7773 - dense_2_4_loss: 1.4612 - dense_2_5_loss: 0.9806 - dense_2_6_loss: 0.6227 - dense_2_7_loss: 0.3756 - dense_2_8_loss: 0.2023 - dense_2_9_loss: 0.1197 - dense_2_10_loss: 0.0610 - dense_2_11_loss: 0.0348 - dense_2_12_loss: 0.0184 - dense_2_13_loss: 0.0107 - dense_2_14_loss: 0.0077 - dense_2_15_loss: 0.0051 - dense_2_16_loss: 0.0035 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.6731 - dense_2_1_accuracy: 0.4823 - dense_2_2_accuracy: 0.3862 - dense_2_3_accuracy: 0.4310 - dense_2_4_accuracy: 0.5115 - dense_2_5_accuracy: 0.6554 - dense_2_6_accuracy: 0.7826 - dense_2_7_accuracy: 0.8703 - dense_2_8_accuracy: 0.9317 - dense_2_9_accuracy: 0.9609 - dense_2_10_accuracy: 0.9819 - dense_2_11_accuracy: 0.9909 - dense_2_12_accuracy: 0.9949 - dense_2_13_accuracy: 0.9973 - dense_2_14_accuracy: 0.9984 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 77/100\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 10.9548 - dense_2_loss: 1.5719 - dense_2_1_loss: 1.7862 - dense_2_2_loss: 1.9453 - dense_2_3_loss: 1.7675 - dense_2_4_loss: 1.4561 - dense_2_5_loss: 0.9707 - dense_2_6_loss: 0.6190 - dense_2_7_loss: 0.3725 - dense_2_8_loss: 0.1994 - dense_2_9_loss: 0.1182 - dense_2_10_loss: 0.0609 - dense_2_11_loss: 0.0338 - dense_2_12_loss: 0.0175 - dense_2_13_loss: 0.0109 - dense_2_14_loss: 0.0077 - dense_2_15_loss: 0.0052 - dense_2_16_loss: 0.0038 - dense_2_17_loss: 0.0032 - dense_2_18_loss: 0.0027 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.6844 - dense_2_1_accuracy: 0.4920 - dense_2_2_accuracy: 0.3934 - dense_2_3_accuracy: 0.4385 - dense_2_4_accuracy: 0.5146 - dense_2_5_accuracy: 0.6587 - dense_2_6_accuracy: 0.7826 - dense_2_7_accuracy: 0.8701 - dense_2_8_accuracy: 0.9321 - dense_2_9_accuracy: 0.9615 - dense_2_10_accuracy: 0.9804 - dense_2_11_accuracy: 0.9901 - dense_2_12_accuracy: 0.9951 - dense_2_13_accuracy: 0.9969 - dense_2_14_accuracy: 0.9986 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 78/100\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 10.8782 - dense_2_loss: 1.5480 - dense_2_1_loss: 1.7681 - dense_2_2_loss: 1.9315 - dense_2_3_loss: 1.7578 - dense_2_4_loss: 1.4506 - dense_2_5_loss: 0.9671 - dense_2_6_loss: 0.6183 - dense_2_7_loss: 0.3719 - dense_2_8_loss: 0.1997 - dense_2_9_loss: 0.1178 - dense_2_10_loss: 0.0606 - dense_2_11_loss: 0.0342 - dense_2_12_loss: 0.0175 - dense_2_13_loss: 0.0108 - dense_2_14_loss: 0.0077 - dense_2_15_loss: 0.0051 - dense_2_16_loss: 0.0037 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0021 - dense_2_accuracy: 0.6931 - dense_2_1_accuracy: 0.5000 - dense_2_2_accuracy: 0.3971 - dense_2_3_accuracy: 0.4382 - dense_2_4_accuracy: 0.5198 - dense_2_5_accuracy: 0.6589 - dense_2_6_accuracy: 0.7843 - dense_2_7_accuracy: 0.8703 - dense_2_8_accuracy: 0.9312 - dense_2_9_accuracy: 0.9619 - dense_2_10_accuracy: 0.9815 - dense_2_11_accuracy: 0.9899 - dense_2_12_accuracy: 0.9953 - dense_2_13_accuracy: 0.9973 - dense_2_14_accuracy: 0.9984 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9994 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 79/100\n",
      "49/49 [==============================] - 6s 131ms/step - loss: 10.7981 - dense_2_loss: 1.5207 - dense_2_1_loss: 1.7513 - dense_2_2_loss: 1.9187 - dense_2_3_loss: 1.7487 - dense_2_4_loss: 1.4440 - dense_2_5_loss: 0.9640 - dense_2_6_loss: 0.6156 - dense_2_7_loss: 0.3682 - dense_2_8_loss: 0.1995 - dense_2_9_loss: 0.1172 - dense_2_10_loss: 0.0611 - dense_2_11_loss: 0.0348 - dense_2_12_loss: 0.0182 - dense_2_13_loss: 0.0111 - dense_2_14_loss: 0.0080 - dense_2_15_loss: 0.0051 - dense_2_16_loss: 0.0037 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.7054 - dense_2_1_accuracy: 0.5072 - dense_2_2_accuracy: 0.4041 - dense_2_3_accuracy: 0.4422 - dense_2_4_accuracy: 0.5198 - dense_2_5_accuracy: 0.6639 - dense_2_6_accuracy: 0.7832 - dense_2_7_accuracy: 0.8699 - dense_2_8_accuracy: 0.9302 - dense_2_9_accuracy: 0.9621 - dense_2_10_accuracy: 0.9817 - dense_2_11_accuracy: 0.9895 - dense_2_12_accuracy: 0.9951 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9984 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 80/100\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 10.7315 - dense_2_loss: 1.4960 - dense_2_1_loss: 1.7323 - dense_2_2_loss: 1.9048 - dense_2_3_loss: 1.7406 - dense_2_4_loss: 1.4413 - dense_2_5_loss: 0.9646 - dense_2_6_loss: 0.6149 - dense_2_7_loss: 0.3709 - dense_2_8_loss: 0.1985 - dense_2_9_loss: 0.1182 - dense_2_10_loss: 0.0623 - dense_2_11_loss: 0.0343 - dense_2_12_loss: 0.0178 - dense_2_13_loss: 0.0108 - dense_2_14_loss: 0.0077 - dense_2_15_loss: 0.0051 - dense_2_16_loss: 0.0036 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.7124 - dense_2_1_accuracy: 0.5138 - dense_2_2_accuracy: 0.4090 - dense_2_3_accuracy: 0.4475 - dense_2_4_accuracy: 0.5214 - dense_2_5_accuracy: 0.6616 - dense_2_6_accuracy: 0.7855 - dense_2_7_accuracy: 0.8681 - dense_2_8_accuracy: 0.9319 - dense_2_9_accuracy: 0.9617 - dense_2_10_accuracy: 0.9807 - dense_2_11_accuracy: 0.9901 - dense_2_12_accuracy: 0.9951 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9984 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 81/100\n",
      "49/49 [==============================] - 6s 131ms/step - loss: 10.6422 - dense_2_loss: 1.4714 - dense_2_1_loss: 1.7143 - dense_2_2_loss: 1.8910 - dense_2_3_loss: 1.7311 - dense_2_4_loss: 1.4318 - dense_2_5_loss: 0.9580 - dense_2_6_loss: 0.6108 - dense_2_7_loss: 0.3696 - dense_2_8_loss: 0.1973 - dense_2_9_loss: 0.1178 - dense_2_10_loss: 0.0617 - dense_2_11_loss: 0.0344 - dense_2_12_loss: 0.0176 - dense_2_13_loss: 0.0108 - dense_2_14_loss: 0.0077 - dense_2_15_loss: 0.0052 - dense_2_16_loss: 0.0036 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.7223 - dense_2_1_accuracy: 0.5226 - dense_2_2_accuracy: 0.4160 - dense_2_3_accuracy: 0.4487 - dense_2_4_accuracy: 0.5280 - dense_2_5_accuracy: 0.6626 - dense_2_6_accuracy: 0.7863 - dense_2_7_accuracy: 0.8711 - dense_2_8_accuracy: 0.9321 - dense_2_9_accuracy: 0.9619 - dense_2_10_accuracy: 0.9804 - dense_2_11_accuracy: 0.9901 - dense_2_12_accuracy: 0.9955 - dense_2_13_accuracy: 0.9969 - dense_2_14_accuracy: 0.9986 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 82/100\n",
      "49/49 [==============================] - 7s 132ms/step - loss: 10.5628 - dense_2_loss: 1.4487 - dense_2_1_loss: 1.6938 - dense_2_2_loss: 1.8783 - dense_2_3_loss: 1.7216 - dense_2_4_loss: 1.4293 - dense_2_5_loss: 0.9544 - dense_2_6_loss: 0.6099 - dense_2_7_loss: 0.3666 - dense_2_8_loss: 0.1961 - dense_2_9_loss: 0.1161 - dense_2_10_loss: 0.0609 - dense_2_11_loss: 0.0343 - dense_2_12_loss: 0.0179 - dense_2_13_loss: 0.0109 - dense_2_14_loss: 0.0074 - dense_2_15_loss: 0.0050 - dense_2_16_loss: 0.0038 - dense_2_17_loss: 0.0032 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.7314 - dense_2_1_accuracy: 0.5294 - dense_2_2_accuracy: 0.4255 - dense_2_3_accuracy: 0.4524 - dense_2_4_accuracy: 0.5288 - dense_2_5_accuracy: 0.6663 - dense_2_6_accuracy: 0.7847 - dense_2_7_accuracy: 0.8709 - dense_2_8_accuracy: 0.9315 - dense_2_9_accuracy: 0.9617 - dense_2_10_accuracy: 0.9809 - dense_2_11_accuracy: 0.9903 - dense_2_12_accuracy: 0.9949 - dense_2_13_accuracy: 0.9969 - dense_2_14_accuracy: 0.9986 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9994 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 83/100\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 10.4910 - dense_2_loss: 1.4246 - dense_2_1_loss: 1.6753 - dense_2_2_loss: 1.8640 - dense_2_3_loss: 1.7138 - dense_2_4_loss: 1.4254 - dense_2_5_loss: 0.9531 - dense_2_6_loss: 0.6104 - dense_2_7_loss: 0.3664 - dense_2_8_loss: 0.1959 - dense_2_9_loss: 0.1166 - dense_2_10_loss: 0.0603 - dense_2_11_loss: 0.0339 - dense_2_12_loss: 0.0171 - dense_2_13_loss: 0.0106 - dense_2_14_loss: 0.0071 - dense_2_15_loss: 0.0049 - dense_2_16_loss: 0.0036 - dense_2_17_loss: 0.0032 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.7423 - dense_2_1_accuracy: 0.5371 - dense_2_2_accuracy: 0.4302 - dense_2_3_accuracy: 0.4564 - dense_2_4_accuracy: 0.5270 - dense_2_5_accuracy: 0.6674 - dense_2_6_accuracy: 0.7851 - dense_2_7_accuracy: 0.8699 - dense_2_8_accuracy: 0.9302 - dense_2_9_accuracy: 0.9609 - dense_2_10_accuracy: 0.9811 - dense_2_11_accuracy: 0.9903 - dense_2_12_accuracy: 0.9949 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9986 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 84/100\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 10.4144 - dense_2_loss: 1.4022 - dense_2_1_loss: 1.6561 - dense_2_2_loss: 1.8485 - dense_2_3_loss: 1.7070 - dense_2_4_loss: 1.4203 - dense_2_5_loss: 0.9484 - dense_2_6_loss: 0.6063 - dense_2_7_loss: 0.3665 - dense_2_8_loss: 0.1941 - dense_2_9_loss: 0.1175 - dense_2_10_loss: 0.0605 - dense_2_11_loss: 0.0341 - dense_2_12_loss: 0.0175 - dense_2_13_loss: 0.0109 - dense_2_14_loss: 0.0077 - dense_2_15_loss: 0.0051 - dense_2_16_loss: 0.0036 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.7505 - dense_2_1_accuracy: 0.5455 - dense_2_2_accuracy: 0.4385 - dense_2_3_accuracy: 0.4582 - dense_2_4_accuracy: 0.5317 - dense_2_5_accuracy: 0.6669 - dense_2_6_accuracy: 0.7855 - dense_2_7_accuracy: 0.8718 - dense_2_8_accuracy: 0.9321 - dense_2_9_accuracy: 0.9615 - dense_2_10_accuracy: 0.9802 - dense_2_11_accuracy: 0.9903 - dense_2_12_accuracy: 0.9951 - dense_2_13_accuracy: 0.9969 - dense_2_14_accuracy: 0.9984 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 85/100\n",
      "49/49 [==============================] - 6s 132ms/step - loss: 10.3340 - dense_2_loss: 1.3799 - dense_2_1_loss: 1.6355 - dense_2_2_loss: 1.8350 - dense_2_3_loss: 1.6975 - dense_2_4_loss: 1.4136 - dense_2_5_loss: 0.9451 - dense_2_6_loss: 0.6041 - dense_2_7_loss: 0.3643 - dense_2_8_loss: 0.1959 - dense_2_9_loss: 0.1161 - dense_2_10_loss: 0.0610 - dense_2_11_loss: 0.0345 - dense_2_12_loss: 0.0173 - dense_2_13_loss: 0.0103 - dense_2_14_loss: 0.0071 - dense_2_15_loss: 0.0051 - dense_2_16_loss: 0.0036 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.7598 - dense_2_1_accuracy: 0.5521 - dense_2_2_accuracy: 0.4473 - dense_2_3_accuracy: 0.4603 - dense_2_4_accuracy: 0.5329 - dense_2_5_accuracy: 0.6709 - dense_2_6_accuracy: 0.7890 - dense_2_7_accuracy: 0.8713 - dense_2_8_accuracy: 0.9321 - dense_2_9_accuracy: 0.9625 - dense_2_10_accuracy: 0.9811 - dense_2_11_accuracy: 0.9901 - dense_2_12_accuracy: 0.9951 - dense_2_13_accuracy: 0.9973 - dense_2_14_accuracy: 0.9986 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 86/100\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 10.2666 - dense_2_loss: 1.3591 - dense_2_1_loss: 1.6163 - dense_2_2_loss: 1.8214 - dense_2_3_loss: 1.6914 - dense_2_4_loss: 1.4094 - dense_2_5_loss: 0.9444 - dense_2_6_loss: 0.6024 - dense_2_7_loss: 0.3638 - dense_2_8_loss: 0.1952 - dense_2_9_loss: 0.1169 - dense_2_10_loss: 0.0599 - dense_2_11_loss: 0.0342 - dense_2_12_loss: 0.0171 - dense_2_13_loss: 0.0104 - dense_2_14_loss: 0.0070 - dense_2_15_loss: 0.0053 - dense_2_16_loss: 0.0043 - dense_2_17_loss: 0.0033 - dense_2_18_loss: 0.0027 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.7653 - dense_2_1_accuracy: 0.5615 - dense_2_2_accuracy: 0.4529 - dense_2_3_accuracy: 0.4638 - dense_2_4_accuracy: 0.5383 - dense_2_5_accuracy: 0.6653 - dense_2_6_accuracy: 0.7869 - dense_2_7_accuracy: 0.8707 - dense_2_8_accuracy: 0.9323 - dense_2_9_accuracy: 0.9609 - dense_2_10_accuracy: 0.9813 - dense_2_11_accuracy: 0.9893 - dense_2_12_accuracy: 0.9951 - dense_2_13_accuracy: 0.9973 - dense_2_14_accuracy: 0.9986 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9994 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 87/100\n",
      "49/49 [==============================] - 7s 133ms/step - loss: 10.1916 - dense_2_loss: 1.3376 - dense_2_1_loss: 1.5967 - dense_2_2_loss: 1.8079 - dense_2_3_loss: 1.6814 - dense_2_4_loss: 1.4045 - dense_2_5_loss: 0.9402 - dense_2_6_loss: 0.6002 - dense_2_7_loss: 0.3636 - dense_2_8_loss: 0.1948 - dense_2_9_loss: 0.1172 - dense_2_10_loss: 0.0611 - dense_2_11_loss: 0.0345 - dense_2_12_loss: 0.0173 - dense_2_13_loss: 0.0106 - dense_2_14_loss: 0.0073 - dense_2_15_loss: 0.0050 - dense_2_16_loss: 0.0038 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.7727 - dense_2_1_accuracy: 0.5718 - dense_2_2_accuracy: 0.4611 - dense_2_3_accuracy: 0.4662 - dense_2_4_accuracy: 0.5350 - dense_2_5_accuracy: 0.6713 - dense_2_6_accuracy: 0.7886 - dense_2_7_accuracy: 0.8713 - dense_2_8_accuracy: 0.9321 - dense_2_9_accuracy: 0.9625 - dense_2_10_accuracy: 0.9809 - dense_2_11_accuracy: 0.9895 - dense_2_12_accuracy: 0.9953 - dense_2_13_accuracy: 0.9969 - dense_2_14_accuracy: 0.9986 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 88/100\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 10.1271 - dense_2_loss: 1.3174 - dense_2_1_loss: 1.5772 - dense_2_2_loss: 1.7938 - dense_2_3_loss: 1.6745 - dense_2_4_loss: 1.4009 - dense_2_5_loss: 0.9385 - dense_2_6_loss: 0.6025 - dense_2_7_loss: 0.3631 - dense_2_8_loss: 0.1947 - dense_2_9_loss: 0.1172 - dense_2_10_loss: 0.0609 - dense_2_11_loss: 0.0348 - dense_2_12_loss: 0.0173 - dense_2_13_loss: 0.0103 - dense_2_14_loss: 0.0067 - dense_2_15_loss: 0.0051 - dense_2_16_loss: 0.0040 - dense_2_17_loss: 0.0032 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.7797 - dense_2_1_accuracy: 0.5813 - dense_2_2_accuracy: 0.4677 - dense_2_3_accuracy: 0.4710 - dense_2_4_accuracy: 0.5383 - dense_2_5_accuracy: 0.6700 - dense_2_6_accuracy: 0.7904 - dense_2_7_accuracy: 0.8711 - dense_2_8_accuracy: 0.9331 - dense_2_9_accuracy: 0.9611 - dense_2_10_accuracy: 0.9811 - dense_2_11_accuracy: 0.9889 - dense_2_12_accuracy: 0.9953 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9984 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9994 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 89/100\n",
      "49/49 [==============================] - 7s 136ms/step - loss: 10.0537 - dense_2_loss: 1.2993 - dense_2_1_loss: 1.5579 - dense_2_2_loss: 1.7803 - dense_2_3_loss: 1.6681 - dense_2_4_loss: 1.3936 - dense_2_5_loss: 0.9353 - dense_2_6_loss: 0.5983 - dense_2_7_loss: 0.3632 - dense_2_8_loss: 0.1945 - dense_2_9_loss: 0.1173 - dense_2_10_loss: 0.0600 - dense_2_11_loss: 0.0334 - dense_2_12_loss: 0.0171 - dense_2_13_loss: 0.0108 - dense_2_14_loss: 0.0075 - dense_2_15_loss: 0.0052 - dense_2_16_loss: 0.0038 - dense_2_17_loss: 0.0032 - dense_2_18_loss: 0.0027 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.7839 - dense_2_1_accuracy: 0.5906 - dense_2_2_accuracy: 0.4798 - dense_2_3_accuracy: 0.4730 - dense_2_4_accuracy: 0.5387 - dense_2_5_accuracy: 0.6706 - dense_2_6_accuracy: 0.7890 - dense_2_7_accuracy: 0.8705 - dense_2_8_accuracy: 0.9310 - dense_2_9_accuracy: 0.9611 - dense_2_10_accuracy: 0.9802 - dense_2_11_accuracy: 0.9899 - dense_2_12_accuracy: 0.9951 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9984 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 90/100\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 9.9681 - dense_2_loss: 1.2798 - dense_2_1_loss: 1.5384 - dense_2_2_loss: 1.7672 - dense_2_3_loss: 1.6588 - dense_2_4_loss: 1.3882 - dense_2_5_loss: 0.9264 - dense_2_6_loss: 0.5954 - dense_2_7_loss: 0.3611 - dense_2_8_loss: 0.1923 - dense_2_9_loss: 0.1157 - dense_2_10_loss: 0.0606 - dense_2_11_loss: 0.0334 - dense_2_12_loss: 0.0168 - dense_2_13_loss: 0.0103 - dense_2_14_loss: 0.0068 - dense_2_15_loss: 0.0051 - dense_2_16_loss: 0.0039 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.7911 - dense_2_1_accuracy: 0.6007 - dense_2_2_accuracy: 0.4856 - dense_2_3_accuracy: 0.4761 - dense_2_4_accuracy: 0.5457 - dense_2_5_accuracy: 0.6739 - dense_2_6_accuracy: 0.7894 - dense_2_7_accuracy: 0.8720 - dense_2_8_accuracy: 0.9310 - dense_2_9_accuracy: 0.9623 - dense_2_10_accuracy: 0.9804 - dense_2_11_accuracy: 0.9897 - dense_2_12_accuracy: 0.9951 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9986 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 91/100\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 9.9012 - dense_2_loss: 1.2605 - dense_2_1_loss: 1.5204 - dense_2_2_loss: 1.7539 - dense_2_3_loss: 1.6542 - dense_2_4_loss: 1.3822 - dense_2_5_loss: 0.9264 - dense_2_6_loss: 0.5929 - dense_2_7_loss: 0.3575 - dense_2_8_loss: 0.1924 - dense_2_9_loss: 0.1148 - dense_2_10_loss: 0.0606 - dense_2_11_loss: 0.0335 - dense_2_12_loss: 0.0170 - dense_2_13_loss: 0.0108 - dense_2_14_loss: 0.0073 - dense_2_15_loss: 0.0051 - dense_2_16_loss: 0.0039 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.7972 - dense_2_1_accuracy: 0.6101 - dense_2_2_accuracy: 0.4918 - dense_2_3_accuracy: 0.4772 - dense_2_4_accuracy: 0.5453 - dense_2_5_accuracy: 0.6713 - dense_2_6_accuracy: 0.7919 - dense_2_7_accuracy: 0.8724 - dense_2_8_accuracy: 0.9310 - dense_2_9_accuracy: 0.9627 - dense_2_10_accuracy: 0.9800 - dense_2_11_accuracy: 0.9901 - dense_2_12_accuracy: 0.9955 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9984 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 92/100\n",
      "49/49 [==============================] - 7s 134ms/step - loss: 9.8446 - dense_2_loss: 1.2419 - dense_2_1_loss: 1.5019 - dense_2_2_loss: 1.7435 - dense_2_3_loss: 1.6471 - dense_2_4_loss: 1.3802 - dense_2_5_loss: 0.9257 - dense_2_6_loss: 0.5913 - dense_2_7_loss: 0.3584 - dense_2_8_loss: 0.1926 - dense_2_9_loss: 0.1143 - dense_2_10_loss: 0.0607 - dense_2_11_loss: 0.0337 - dense_2_12_loss: 0.0176 - dense_2_13_loss: 0.0109 - dense_2_14_loss: 0.0079 - dense_2_15_loss: 0.0052 - dense_2_16_loss: 0.0036 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.8009 - dense_2_1_accuracy: 0.6179 - dense_2_2_accuracy: 0.4961 - dense_2_3_accuracy: 0.4792 - dense_2_4_accuracy: 0.5441 - dense_2_5_accuracy: 0.6717 - dense_2_6_accuracy: 0.7913 - dense_2_7_accuracy: 0.8722 - dense_2_8_accuracy: 0.9323 - dense_2_9_accuracy: 0.9627 - dense_2_10_accuracy: 0.9811 - dense_2_11_accuracy: 0.9895 - dense_2_12_accuracy: 0.9957 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9986 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 93/100\n",
      "49/49 [==============================] - 8s 155ms/step - loss: 9.7832 - dense_2_loss: 1.2260 - dense_2_1_loss: 1.4853 - dense_2_2_loss: 1.7293 - dense_2_3_loss: 1.6409 - dense_2_4_loss: 1.3757 - dense_2_5_loss: 0.9227 - dense_2_6_loss: 0.5921 - dense_2_7_loss: 0.3591 - dense_2_8_loss: 0.1918 - dense_2_9_loss: 0.1153 - dense_2_10_loss: 0.0595 - dense_2_11_loss: 0.0339 - dense_2_12_loss: 0.0171 - dense_2_13_loss: 0.0105 - dense_2_14_loss: 0.0072 - dense_2_15_loss: 0.0051 - dense_2_16_loss: 0.0038 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.8038 - dense_2_1_accuracy: 0.6243 - dense_2_2_accuracy: 0.5016 - dense_2_3_accuracy: 0.4802 - dense_2_4_accuracy: 0.5461 - dense_2_5_accuracy: 0.6750 - dense_2_6_accuracy: 0.7933 - dense_2_7_accuracy: 0.8711 - dense_2_8_accuracy: 0.9315 - dense_2_9_accuracy: 0.9629 - dense_2_10_accuracy: 0.9809 - dense_2_11_accuracy: 0.9895 - dense_2_12_accuracy: 0.9949 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9984 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 94/100\n",
      "49/49 [==============================] - 7s 146ms/step - loss: 9.7247 - dense_2_loss: 1.2080 - dense_2_1_loss: 1.4688 - dense_2_2_loss: 1.7186 - dense_2_3_loss: 1.6358 - dense_2_4_loss: 1.3737 - dense_2_5_loss: 0.9207 - dense_2_6_loss: 0.5893 - dense_2_7_loss: 0.3564 - dense_2_8_loss: 0.1923 - dense_2_9_loss: 0.1153 - dense_2_10_loss: 0.0594 - dense_2_11_loss: 0.0339 - dense_2_12_loss: 0.0174 - dense_2_13_loss: 0.0105 - dense_2_14_loss: 0.0074 - dense_2_15_loss: 0.0051 - dense_2_16_loss: 0.0041 - dense_2_17_loss: 0.0032 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.8086 - dense_2_1_accuracy: 0.6311 - dense_2_2_accuracy: 0.5064 - dense_2_3_accuracy: 0.4860 - dense_2_4_accuracy: 0.5465 - dense_2_5_accuracy: 0.6772 - dense_2_6_accuracy: 0.7913 - dense_2_7_accuracy: 0.8751 - dense_2_8_accuracy: 0.9327 - dense_2_9_accuracy: 0.9613 - dense_2_10_accuracy: 0.9807 - dense_2_11_accuracy: 0.9893 - dense_2_12_accuracy: 0.9951 - dense_2_13_accuracy: 0.9975 - dense_2_14_accuracy: 0.9984 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 95/100\n",
      "49/49 [==============================] - 7s 141ms/step - loss: 9.6503 - dense_2_loss: 1.1907 - dense_2_1_loss: 1.4517 - dense_2_2_loss: 1.7092 - dense_2_3_loss: 1.6277 - dense_2_4_loss: 1.3679 - dense_2_5_loss: 0.9148 - dense_2_6_loss: 0.5853 - dense_2_7_loss: 0.3545 - dense_2_8_loss: 0.1894 - dense_2_9_loss: 0.1140 - dense_2_10_loss: 0.0599 - dense_2_11_loss: 0.0338 - dense_2_12_loss: 0.0169 - dense_2_13_loss: 0.0103 - dense_2_14_loss: 0.0073 - dense_2_15_loss: 0.0051 - dense_2_16_loss: 0.0039 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.8137 - dense_2_1_accuracy: 0.6381 - dense_2_2_accuracy: 0.5119 - dense_2_3_accuracy: 0.4866 - dense_2_4_accuracy: 0.5492 - dense_2_5_accuracy: 0.6776 - dense_2_6_accuracy: 0.7915 - dense_2_7_accuracy: 0.8728 - dense_2_8_accuracy: 0.9315 - dense_2_9_accuracy: 0.9617 - dense_2_10_accuracy: 0.9813 - dense_2_11_accuracy: 0.9897 - dense_2_12_accuracy: 0.9955 - dense_2_13_accuracy: 0.9973 - dense_2_14_accuracy: 0.9986 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 96/100\n",
      "49/49 [==============================] - 7s 137ms/step - loss: 9.5952 - dense_2_loss: 1.1749 - dense_2_1_loss: 1.4370 - dense_2_2_loss: 1.6987 - dense_2_3_loss: 1.6220 - dense_2_4_loss: 1.3633 - dense_2_5_loss: 0.9119 - dense_2_6_loss: 0.5851 - dense_2_7_loss: 0.3544 - dense_2_8_loss: 0.1898 - dense_2_9_loss: 0.1136 - dense_2_10_loss: 0.0594 - dense_2_11_loss: 0.0340 - dense_2_12_loss: 0.0171 - dense_2_13_loss: 0.0104 - dense_2_14_loss: 0.0070 - dense_2_15_loss: 0.0050 - dense_2_16_loss: 0.0038 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.8178 - dense_2_1_accuracy: 0.6455 - dense_2_2_accuracy: 0.5179 - dense_2_3_accuracy: 0.4885 - dense_2_4_accuracy: 0.5496 - dense_2_5_accuracy: 0.6795 - dense_2_6_accuracy: 0.7915 - dense_2_7_accuracy: 0.8720 - dense_2_8_accuracy: 0.9323 - dense_2_9_accuracy: 0.9627 - dense_2_10_accuracy: 0.9809 - dense_2_11_accuracy: 0.9901 - dense_2_12_accuracy: 0.9953 - dense_2_13_accuracy: 0.9973 - dense_2_14_accuracy: 0.9984 - dense_2_15_accuracy: 0.9994 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 97/100\n",
      "49/49 [==============================] - 7s 136ms/step - loss: 9.5453 - dense_2_loss: 1.1598 - dense_2_1_loss: 1.4234 - dense_2_2_loss: 1.6906 - dense_2_3_loss: 1.6170 - dense_2_4_loss: 1.3610 - dense_2_5_loss: 0.9104 - dense_2_6_loss: 0.5836 - dense_2_7_loss: 0.3520 - dense_2_8_loss: 0.1895 - dense_2_9_loss: 0.1139 - dense_2_10_loss: 0.0596 - dense_2_11_loss: 0.0338 - dense_2_12_loss: 0.0165 - dense_2_13_loss: 0.0103 - dense_2_14_loss: 0.0067 - dense_2_15_loss: 0.0051 - dense_2_16_loss: 0.0038 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.8201 - dense_2_1_accuracy: 0.6507 - dense_2_2_accuracy: 0.5224 - dense_2_3_accuracy: 0.4922 - dense_2_4_accuracy: 0.5515 - dense_2_5_accuracy: 0.6807 - dense_2_6_accuracy: 0.7937 - dense_2_7_accuracy: 0.8757 - dense_2_8_accuracy: 0.9329 - dense_2_9_accuracy: 0.9623 - dense_2_10_accuracy: 0.9809 - dense_2_11_accuracy: 0.9903 - dense_2_12_accuracy: 0.9953 - dense_2_13_accuracy: 0.9973 - dense_2_14_accuracy: 0.9984 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998     \n",
      "Epoch 98/100\n",
      "49/49 [==============================] - 7s 136ms/step - loss: 9.5183 - dense_2_loss: 1.1440 - dense_2_1_loss: 1.4087 - dense_2_2_loss: 1.6808 - dense_2_3_loss: 1.6141 - dense_2_4_loss: 1.3593 - dense_2_5_loss: 0.9130 - dense_2_6_loss: 0.5829 - dense_2_7_loss: 0.3571 - dense_2_8_loss: 0.1938 - dense_2_9_loss: 0.1153 - dense_2_10_loss: 0.0626 - dense_2_11_loss: 0.0346 - dense_2_12_loss: 0.0175 - dense_2_13_loss: 0.0103 - dense_2_14_loss: 0.0072 - dense_2_15_loss: 0.0053 - dense_2_16_loss: 0.0041 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.8219 - dense_2_1_accuracy: 0.6571 - dense_2_2_accuracy: 0.5268 - dense_2_3_accuracy: 0.4955 - dense_2_4_accuracy: 0.5533 - dense_2_5_accuracy: 0.6818 - dense_2_6_accuracy: 0.7923 - dense_2_7_accuracy: 0.8738 - dense_2_8_accuracy: 0.9325 - dense_2_9_accuracy: 0.9615 - dense_2_10_accuracy: 0.9798 - dense_2_11_accuracy: 0.9905 - dense_2_12_accuracy: 0.9951 - dense_2_13_accuracy: 0.9975 - dense_2_14_accuracy: 0.9979 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9994 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 99/100\n",
      "49/49 [==============================] - 7s 150ms/step - loss: 9.4470 - dense_2_loss: 1.1307 - dense_2_1_loss: 1.3952 - dense_2_2_loss: 1.6696 - dense_2_3_loss: 1.6076 - dense_2_4_loss: 1.3539 - dense_2_5_loss: 0.9041 - dense_2_6_loss: 0.5807 - dense_2_7_loss: 0.3540 - dense_2_8_loss: 0.1907 - dense_2_9_loss: 0.1156 - dense_2_10_loss: 0.0605 - dense_2_11_loss: 0.0335 - dense_2_12_loss: 0.0170 - dense_2_13_loss: 0.0102 - dense_2_14_loss: 0.0071 - dense_2_15_loss: 0.0050 - dense_2_16_loss: 0.0037 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.8238 - dense_2_1_accuracy: 0.6639 - dense_2_2_accuracy: 0.5331 - dense_2_3_accuracy: 0.4981 - dense_2_4_accuracy: 0.5506 - dense_2_5_accuracy: 0.6820 - dense_2_6_accuracy: 0.7925 - dense_2_7_accuracy: 0.8734 - dense_2_8_accuracy: 0.9317 - dense_2_9_accuracy: 0.9619 - dense_2_10_accuracy: 0.9811 - dense_2_11_accuracy: 0.9897 - dense_2_12_accuracy: 0.9955 - dense_2_13_accuracy: 0.9975 - dense_2_14_accuracy: 0.9984 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n",
      "Epoch 100/100\n",
      "49/49 [==============================] - 7s 136ms/step - loss: 9.3945 - dense_2_loss: 1.1159 - dense_2_1_loss: 1.3827 - dense_2_2_loss: 1.6635 - dense_2_3_loss: 1.6028 - dense_2_4_loss: 1.3507 - dense_2_5_loss: 0.9032 - dense_2_6_loss: 0.5767 - dense_2_7_loss: 0.3505 - dense_2_8_loss: 0.1897 - dense_2_9_loss: 0.1132 - dense_2_10_loss: 0.0598 - dense_2_11_loss: 0.0340 - dense_2_12_loss: 0.0173 - dense_2_13_loss: 0.0105 - dense_2_14_loss: 0.0068 - dense_2_15_loss: 0.0052 - dense_2_16_loss: 0.0040 - dense_2_17_loss: 0.0031 - dense_2_18_loss: 0.0026 - dense_2_19_loss: 0.0022 - dense_2_accuracy: 0.8261 - dense_2_1_accuracy: 0.6665 - dense_2_2_accuracy: 0.5368 - dense_2_3_accuracy: 0.5019 - dense_2_4_accuracy: 0.5535 - dense_2_5_accuracy: 0.6803 - dense_2_6_accuracy: 0.7915 - dense_2_7_accuracy: 0.8732 - dense_2_8_accuracy: 0.9319 - dense_2_9_accuracy: 0.9627 - dense_2_10_accuracy: 0.9809 - dense_2_11_accuracy: 0.9903 - dense_2_12_accuracy: 0.9953 - dense_2_13_accuracy: 0.9971 - dense_2_14_accuracy: 0.9984 - dense_2_15_accuracy: 0.9992 - dense_2_16_accuracy: 0.9996 - dense_2_17_accuracy: 0.9998 - dense_2_18_accuracy: 0.9998 - dense_2_19_accuracy: 0.9998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2033188f760>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([Xoh, s0, c0], outputs, epochs=100, batch_size=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see the results on new examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 50ms/step\n",
      "source: فرنگی\n",
      "output: فرنگی \n",
      "\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "source: آذار\n",
      "output: آرار \n",
      "\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "source: کتب دینی\n",
      "output: گتیییی \n",
      "\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "source: نوسابه\n",
      "output: نوساهه \n",
      "\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "source: آذادگی\n",
      "output: ددادیی \n",
      "\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "source: آغار\n",
      "output: آرار \n",
      "\n"
     ]
    }
   ],
   "source": [
    "EXAMPLES = ['فرنگی', 'آذار', 'کتب دینی', 'نوسابه', 'آذادگی', 'آغار']\n",
    "s00 = np.zeros((1, n_s))\n",
    "c00 = np.zeros((1, n_s))\n",
    "for example in EXAMPLES:\n",
    "    source = string_to_int(example, T, vocab)\n",
    "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(vocab)), source))).swapaxes(0,1)\n",
    "    source = np.swapaxes(source, 0, 1)\n",
    "    source = np.expand_dims(source, axis=0)\n",
    "\n",
    "    prediction = model.predict([source, s00, c00])\n",
    "    prediction = np.argmax(prediction, axis = -1)\n",
    "    output = [inv_vocab[int(i)] for i in prediction if inv_vocab[int(i)] != '<pad>']\n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", ''.join(output),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b1c37460846ba095889c981f6bdf7dbe3db3422cb2efaf84f879792979df9daa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
